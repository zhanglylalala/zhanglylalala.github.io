<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://http://www.laughingtree.cn').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="LaughingTree">
<meta property="og:url" content="http://http//www.laughingtree.cn/page/3/index.html">
<meta property="og:site_name" content="LaughingTree">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://http//www.laughingtree.cn/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>LaughingTree</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LaughingTree</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">20</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">28</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/16/10-Poisson-Porcess/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/16/10-Poisson-Porcess/" class="post-title-link" itemprop="url">10. Poisson Porcess</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-16 23:41:53" itemprop="dateCreated datePublished" datetime="2020-03-16T23:41:53+08:00">2020-03-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-17 23:27:43" itemprop="dateModified" datetime="2020-03-17T23:27:43+08:00">2020-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h1><ol>
<li>The Poisson process is a continuous-time analog of the Bernoulli process. </li>
<li><script type="math/tex">\lambda</script> is the rate of one Poisson process. Namely the rate of arrivals per unit time. </li>
<li>We define <script type="math/tex">P(k,\tau)=P(there\ are\ exactly\ k\ arrivals\ during\ an\ interval\ of\ length\ \tau)</script></li>
<li>This probability is the same for all intervals of the same length <script type="math/tex">\tau</script>. Namely, arrivals are equally likely ar all times. </li>
<li>The number of arrivals during a particular interval is independent of the history of arrivals outside this interval. </li>
<li><script type="math/tex">P(0,\tau)=1-\lambda\tau+o(\tau)</script><br /><script type="math/tex">P(1,\tau)=\lambda\tau+o_1(\tau)</script><br /><script type="math/tex">P(k,\tau)=o_k(\tau)\ \ (k>1)</script>. <br /><script type="math/tex">\displaystyle\lim_{\tau\to0}\frac{o(\tau)}{\tau}=0</script>, <script type="math/tex">\displaystyle\lim_{\tau\to0}\frac{o_k(\tau)}{\tau}</script>. </li>
<li>The <script type="math/tex">o(\tau)</script> and <script type="math/tex">o_k(\tau)</script> are meant to be negligible in comparison to <script type="math/tex">\tau</script>, when the interaval length <script type="math/tex">\tau</script> is very small. </li>
</ol>
<h1 id="Number-of-Arrivals-in-an-Interval"><a href="#Number-of-Arrivals-in-an-Interval" class="headerlink" title="Number of Arrivals in an Interval"></a>Number of Arrivals in an Interval</h1><ol>
<li>First fix the time interval of length <script type="math/tex">\tau</script> and partition it into <script type="math/tex">\displaystyle\frac{\tau}{\delta}</script> periods of length <script type="math/tex">\delta</script>, where <script type="math/tex">\delta</script> is a very small number. </li>
<li>Because <script type="math/tex">P(k,\tau)=o_k(\tau)</script> when <script type="math/tex">k≥2</script>, the probability of more than two arrivals during any period can be neglected. </li>
<li>Each period has one arrival with probability approximately equal to <script type="math/tex">\lambda\delta</script> and is independent of each other. </li>
<li>Therefore, the process can be approximated by a Bernoulli process with parameter <script type="math/tex">\lambda\delta</script>. </li>
<li>Hence, the PMF is <script type="math/tex">P(k,\tau)=e^{-\lambda\tau}\displaystyle\frac{(\lambda\tau)^k}{k!}</script>, the expected number of arrivals is <script type="math/tex">E[N\tau]=np=\lambda\tau</script> and the variance is <script type="math/tex">var(N_\tau)=\lambda\tau</script>. <script type="math/tex">N_\tau</script> denotes the number of arrivals during a time interval of length <script type="math/tex">\tau</script>. </li>
<li>The probability law for the time <script type="math/tex">T</script> of the first arrival, assuming that the process starts at time zero: <br /><script type="math/tex">F_T(t)=P(T≤t)=1-P(T>t)=1-P(0,t)=1-e^{-\lambda t}</script>. </li>
<li>The PDF formula: <script type="math/tex">f_T(t)=\lambda e^{-\lambda t}</script> <script type="math/tex">(t≥0)</script>. </li>
<li>The probability of <script type="math/tex">k</script> arrivals during a set of times of total length <script type="math/tex">\tau</script> is always given by <script type="math/tex">P(k,\tau)</script>, even if that set is not an interval. </li>
</ol>
<h1 id="Independence-and-Memorylessness"><a href="#Independence-and-Memorylessness" class="headerlink" title="Independence and Memorylessness"></a>Independence and Memorylessness</h1><ol>
<li>For any given time <script type="math/tex">t>0</script>, the history of the process after time <script type="math/tex">t</script> is also a Poisson process, and is independent from the history of the process until <script type="math/tex">t</script>. </li>
<li>Let <script type="math/tex">t</script> be a given time and let <script type="math/tex">\overline{T}</script> be the time of the first arrival after time <script type="math/tex">t</script>. Then <script type="math/tex">\overline{T}-t</script> has an exponential distribution with parameter <script type="math/tex">\lambda</script>, and is independent of the history of the process until time <script type="math/tex">t</script>. </li>
</ol>
<h1 id="The-kth-Arrival-Time"><a href="#The-kth-Arrival-Time" class="headerlink" title="The kth Arrival Time"></a>The kth Arrival Time</h1><ol>
<li>The definition is the same as in the Bernoulli process. </li>
<li>$E[Y_k]=E[T_1]+…+E[T_k]=\displaystyle\frac{k}{\lambda}$</li>
<li>$var(Y_k)=var(T_1)+…+var(T_k)=\displaystyle\frac{k}{\lambda^2}$</li>
<li>The PDF of <script type="math/tex">Y_k</script> (Erlang PDF of order <script type="math/tex">k</script>) is <script type="math/tex">f_{Y_k}(y)=\displaystyle\frac{\lambda^ky^{k-1}e^{-\lambda y}}{(k-1)!}\ \ (y≥0)</script>. </li>
<li>For a small <script type="math/tex">\delta</script>, the product <script type="math/tex">\delta\cdot f_{Y_k}(y)</script> approximates the probability that the <script type="math/tex">k</script>th arrival occurs between times <script type="math/tex">y</script> and <script type="math/tex">y+\delta</script>. </li>
<li>When <script type="math/tex">\delta</script> is very small, the probability of more thant one arrival during the interval <script type="math/tex">[y,y+\delta]</script> is negligible. Thus the <script type="math/tex">k</script>th arrival occurs between <script type="math/tex">y</script> and <script type="math/tex">y+\delta</script> if and only if: <br />&emsp;There is an arrival during the interval <script type="math/tex">[y,y+\delta]</script><br />&emsp;There are exactly <script type="math/tex">k-1</script> arrivals before time <script type="math/tex">y</script>. <br /><script type="math/tex">\delta f_{Y_k}(y)\approx \lambda\delta \cdot P(k-1,y)=\lambda\delta\displaystyle\frac{\lambda^{k-1}y^{k-1}e^{-\lambda y}}{(k-1)!}</script><br />So <script type="math/tex">f_{Y_k}(y)=\displaystyle\frac{\lambda^ky^{k-1}e^{-\lambda y}}{(k-1)!}</script></li>
<li>CDF: <script type="math/tex">F_{Y_k}(y)=P(Y_k≤y)=\displaystyle\sum^\infty_{n=k}P(n,y)=1-\sum^{k-1}_{n=0}P(n,y)=1-\sum^{k-1}_{n=0}\frac{(\lambda y)^ne^{-\lambda y}}{n!}</script></li>
</ol>
<h1 id="Splitting-and-Merging"><a href="#Splitting-and-Merging" class="headerlink" title="Splitting and Merging"></a>Splitting and Merging</h1><ol>
<li>As in the Bernoulli process, start with one Poisson process with rate <script type="math/tex">\lambda</script> and split its arrivals at rate <script type="math/tex">q</script> into two independent new processes with new rates <script type="math/tex">\lambda q</script> and <script type="math/tex">\lambda(1-q)</script>. </li>
<li>Alternatively, start with two Poisson processes with rates <script type="math/tex">\lambda_1</script> and <script type="math/tex">\lambda_2</script> and merge them into one process with rate <script type="math/tex">\lambda_1+\lambda_2</script>. <br/><script type="math/tex">P(0\ arrivals\ in\ the\ merged\ process)\approx(1-\lambda_1\delta)(1-\lambda_2\delta)\approx1-(\lambda_1+\lambda_2)\delta</script><br /><script type="math/tex">P(1\ arrivals\ in\ the\ merged\ process)\approx\lambda_1\delta(1-\lambda_2\delta)+(1-\lambda_1\delta)\lambda_2\delta\approx(\lambda_1+\lambda_2)\delta</script></li>
<li>Any particular arrival of the merged process has probability <script type="math/tex">\displaystyle\frac{\lambda_1}{\lambda_1+\lambda_2}</script> of originating from the first process, and probability <script type="math/tex">\displaystyle\frac{\lambda_2}{\lambda_1+\lambda_2}</script> of originating from the second process. <br /><script type="math/tex">P(1\ arrival\ of\ first\ process|1\ arrival)=\displaystyle\frac{P(1\ arrival\ of\ first\ process)}{P(1\ arrival)}\approx\frac{\lambda_1\delta}{(\lambda_1+\lambda_2)\delta}=\frac{\lambda_1}{\lambda_1+\lambda+2}</script></li>
</ol>
<h1 id="Sums-of-Random-Variables"><a href="#Sums-of-Random-Variables" class="headerlink" title="Sums of Random Variables"></a>Sums of Random Variables</h1><ol>
<li>Let <script type="math/tex">N,X_1,X_2,…</script> be independent random variables. <script type="math/tex">N</script> takes nonnegative integer values. Let <script type="math/tex">Y=X_1+…+X_N</script>. <script type="math/tex">Y=0</script> when <script type="math/tex">N=0</script>. </li>
<li>If <script type="math/tex">X_i</script> is Bernoulli with parameter <script type="math/tex">p</script>, and <script type="math/tex">N</script> is binomial with parameters <script type="math/tex">m</script> and <script type="math/tex">q</script>, then <script type="math/tex">Y</script> is binomial with parameters <script type="math/tex">m</script> and <script type="math/tex">pq</script>. </li>
<li>If <script type="math/tex">X_i</script> is Bernoulli with parameter <script type="math/tex">p</script> and <script type="math/tex">N</script> is Poisson with parameter <script type="math/tex">\lambda</script>, then <script type="math/tex">Y</script> is Poisson with parameter <script type="math/tex">\lambda p</script>. </li>
<li>If <script type="math/tex">X_i</script> is geometric with parameter <script type="math/tex">p</script>, and <script type="math/tex">N</script> is geometric with parameter <script type="math/tex">q</script>, then <script type="math/tex">Y</script> is geometirc with parametric <script type="math/tex">pq</script>. </li>
<li>If <script type="math/tex">X_i</script> is exponential with parameter <script type="math/tex">\lambda</script>, and <script type="math/tex">N</script> is geometric with parameter <script type="math/tex">q</script>, then <script type="math/tex">Y</script> is exponential with parameter <script type="math/tex">\lambda q</script>. </li>
<li>If <script type="math/tex">N_t</script> denotes the number of arrivals of a Poisson process with parameter <script type="math/tex">\lambda</script> within an interval of length <script type="math/tex">t</script>, and <script type="math/tex">T</script> is an interval with length that is exponentially distributed with parameter <script type="math/tex">\upsilon</script> and is independent of the Poisson process, then <script type="math/tex">N_T+1</script> is geometrically distributed with parameter <script type="math/tex">\displaystyle\frac{\upsilon}{\lambda+\upsilon}</script>. </li>
<li>The sum of a large number of independent arrival processes (not necessarily Poisson) can be approximated by a Poisson process with arrival rate equal to the sum of the individual arrival rates. <br />The component processes must have a small rate relative to the total, so that none of them imposes its probabilistic character on the total arrival process. </li>
</ol>
<h1 id="Random-Incidence-Paradox"><a href="#Random-Incidence-Paradox" class="headerlink" title="Random Incidence Paradox"></a>Random Incidence Paradox</h1><ol>
<li><script type="math/tex">t^*</script> is a fixed time instance with at least one prior arrival. <script type="math/tex">L</script> is the inter arrival interval that contains <script type="math/tex">t^*</script>. </li>
<li>Let <script type="math/tex">L</script> be <script type="math/tex">[U,V]</script>. Split <script type="math/tex">L</script> into two parts <script type="math/tex">L=(t^*-U)+(V-t^*)</script>. </li>
<li><script type="math/tex">t^*-U</script> is determined by the past history of the process before <script type="math/tex">t^*</script> while <script type="math/tex">V-t^*</script> is determined by the future of the process after time <script type="math/tex">t^*</script>. </li>
<li>These two are independent and both are exponential with parameter <script type="math/tex">\lambda</script>. So <script type="math/tex">L</script> is the sum of two independent exponential random variables with parameter <script type="math/tex">\lambda</script>, i.e. Erlang of order two, with mean <script type="math/tex">\displaystyle\frac{2}{\lambda}</script>. </li>
<li>This is not right, the mean should be <script type="math/tex">\displaystyle\frac{1}{\lambda}</script>. The problem is that when we insert that time instance <script type="math/tex">t^*</script> into time line, it is more likely to fall into larger intervals rather than  smaller intervals. So the expectation of the interval is larger. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/16/09-Bernoulli-Process/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/16/09-Bernoulli-Process/" class="post-title-link" itemprop="url">09. Bernoulli Process</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-16 20:51:19" itemprop="dateCreated datePublished" datetime="2020-03-16T20:51:19+08:00">2020-03-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-17 08:48:46" itemprop="dateModified" datetime="2020-03-17T08:48:46+08:00">2020-03-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li>The Bernoulli process can be visualized as a sequence of independent coin tosses where the probability of heads in each toss is a fixed number $p$ in the range $(0,1)$. </li>
<li>In convention, each trial produces a $1$ (a success) with probability $p$, and a $0$ (a failure) with probability $1-p$, independent of what happens in other trials. </li>
</ol>
<h1 id="Independence-and-Memorylessness"><a href="#Independence-and-Memorylessness" class="headerlink" title="Independence and Memorylessness"></a>Independence and Memorylessness</h1><ol>
<li>Fresh-start property: The future trials are independent from the past ones. Starting from any given point in time, the future is also modeled by a Bernoulli process, which is independent from the past. </li>
<li>Memorylessness property: The time $T$ until the first success is a geometric random variable. After $n$ failures, since a fresh-start Bernoulli process is constituting, the remaining trials until first success $T-n$ is still a geometric random variable. $P(T-n=t|T&gt;n)=P(T=t)$</li>
</ol>
<h1 id="Interarrival-Times"><a href="#Interarrival-Times" class="headerlink" title="Interarrival Times"></a>Interarrival Times</h1><ol>
<li>$Y_k$ represents the time of $k$th success. $T_k$ stands for the $k$th interarrival time. </li>
<li>$T_1=Y_1$, $T_i=Y_i-Y_{i-1}(i&gt;1)$</li>
<li>$Y_k=\displaystyle\sum^k_{i=1}T_i$</li>
<li>$T_1$ is a geometric random variable with parameter $p$. After $T_1$, the future is a new Bernoulli process similar to the old one. </li>
<li>All $T_i$ are independent and all have the same geometric distribution. </li>
<li>In the problem of finding the first failure string, the string begins with the first failure and ends with the first success since then is not geometric. Because we look at it from the frist failure when we know it’s a failure. So we need to look from the experiment after the frist failure, since we only know whether it’s going to success or not. So the geometric begins after the fist failure occurred, ends when the frist success occurred. And the expectation of that is $E[T_1]+1$. </li>
</ol>
<h1 id="The-kth-Arrivial-Time"><a href="#The-kth-Arrivial-Time" class="headerlink" title="The  kth Arrivial Time"></a>The  kth Arrivial Time</h1><ol>
<li>$Y_k=T_1+…+T_k$, $T_1,…,T_k$ are independent and have geometric PMF. So $E[Y_k]=E[T_1+…+T_k]=E[T_1]+…E[T_k]=\displaystyle\frac{k}{p}$</li>
<li>$var(Y_k)=var(T_1+…+T_k)=var(T_1)+…+var(T_k)=\displaystyle\frac{k(1-p)}{p^2}$</li>
<li>The PMF of $Y_k$ (Pascal PMF of order $k$): $p_{Y_k}(t)={t-1\choose k-1}p^k(1-p)^{t-k}  t≥k$</li>
</ol>
<h1 id="Split-and-Merge"><a href="#Split-and-Merge" class="headerlink" title="Split and Merge"></a>Split and Merge</h1><h2 id="Split"><a href="#Split" class="headerlink" title="Split"></a>Split</h2><ol>
<li>Imaging tossing two coins, the first coin has a bias to the head of probability $p$ and the second coin has a bias to the head of probability $q$. </li>
<li>If the outcome of the first coin is head, then we toss the second coin. </li>
<li>We can split the result of first coin into two independent process. Both of the later process focus only on the second coin. </li>
<li>One process marks the head side on the second coin as success and tail on the first coin or tail on the second coin as failure. <br />The other process marks the tail on the second coin as success and tail on the first coin or head on the second coin as failure. </li>
<li>The first process has a proabability of success of $pq$ while the second process has a probability of success of $p(1-q)$. </li>
</ol>
<h2 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h2><ol>
<li>Other than split one process into two process, we can also merge two process into one process. </li>
<li>Tossing the same two coins as above at the same time, the result is success if one of the coins is head. </li>
<li>This final process has a probability of <script type="math/tex">1-P(none\ is\ head)=1-(1-p)(1-q)=p+q-pq</script></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/16/08-Iterated-Expectation-and-Sum-of-a-Random-Number-of-Random-Variables/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/16/08-Iterated-Expectation-and-Sum-of-a-Random-Number-of-Random-Variables/" class="post-title-link" itemprop="url">08. Iterated Expectation and Sum of a Random Number of Random Variables</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-16 08:51:43 / Modified: 16:12:20" itemprop="dateCreated datePublished" datetime="2020-03-16T08:51:43+08:00">2020-03-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Iterated-Expectation"><a href="#Iterated-Expectation" class="headerlink" title="Iterated Expectation"></a>Iterated Expectation</h1><ol>
<li>$E[X|Y]$ is a random variable that takes the value $E[X|Y=y]$ when $Y$ takes the value $y$. <br />Since $E[X|Y=y]$ is a function of $y$, $E[X|Y]$ is a function of $Y$. </li>
<li>The distribution of $E[X|Y]$ is determined by the distribution of $Y$ by change $y$ in $E[X|Y=y]$ into $Y$. </li>
<li>Iterated Expectation: Because $E[X|Y]$ is a random variable, naturally it has its own expectation <br />$E[E[X|Y]]=\left\{\begin{array}{}\displaystyle\sum_yE[X|Y=y]p_Y(y)&amp;Y discrete\\\int^\infty_{-\infty}E[E[X|Y=y]]f_Y(y)dy&amp;Y continuous \end{array}\right.=E[X]$</li>
<li>For any function $g$, given the value of $Y$, $g(Y)$ is a constant. So $E[Xg(Y)|Y]=g(Y)E[X|Y]$. </li>
<li>We denote $\hat{X}=E[X|Y]$ as an estimator of $X$ given $Y$. </li>
<li>The estimation error $\tilde{X}=\hat{X}-X$ satisfying $E[\tilde{X}|Y]=E[\hat{X}-X|Y]=E[\hat{X}|Y]-E[X|Y]=\hat{X}-\hat{X}=0$</li>
<li>The random variable $E[\tilde{X}|Y]$ is identically zero: $E[\tilde{X}|Y=y]=0$ for all values of $y$. <br />The law of iterated expectation also tells us $E[\tilde{X}]=E[E[\tilde{X}|Y]]=0$. <br />It indicates that the estimation error doesn’t have a systematic upward or downward bias. </li>
<li>$E[\hat{X}\tilde{X}]=E[E[\hat{X}\tilde{X}|Y]]$. Since $\hat{X}$ is complete determined by $Y$. So $E[E[\hat{X}\tilde{X}|Y]]=E[\hat{X}E[\tilde{X}|Y]]=E[\hat{X}0]=0$<br />It follows that $cov(\hat{X},\tilde{X})=E[\hat{X}\tilde{X}]-E[\hat{X}]E[\tilde{X}]=0-E[X]\cdot0=0$<br />So $\hat{X}$ and $\tilde{X}$ are uncorrelated. <br />So $var(X)=var(\tilde{X})+var(\hat{X})$. </li>
<li>The conditional variance: $var(X|Y)=E[(X-E{X|Y})^2|Y]=E[\tilde{X}^2|Y]$</li>
<li>This is the function of $Y$ whose value is the conditional variance of $X$ when $Y$ takes the value $y$. </li>
<li>Using $E[\tilde{X}]=0$, $var(\tilde{X})=E[\tilde{X}^2]=E[E[\tilde{X}^2|Y]]=E[var(X|Y)]$. <br />Since $var(X)=var(\tilde{X})+var{\hat{X}}$, we have the law of total variance: $var(X)=E[var(X|Y)]+var(E[X|Y])$</li>
</ol>
<h1 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h1><ol>
<li>The transform associated with a random variable $X$ is a function $M_X(s)$ of a scalar parameter $s$, defined by $M_X(s)=E[e^{sX}]$. </li>
<li>If $X$ is a discrete random variable, the transform can be written as $M(s)=\displaystyle\sum_xe^{sx}p_X(x)$. <br />If $X$ is a continuous random variable, $M(s)=\displaystyle\int^\infty_{-\infty}e^{sx}f_X(x)dx$. </li>
<li>If $Y=aX+b$, $M_Y(s)=E[e^{s(aX+b)}]=E[e^{saX}e^{sb}]=e^{sb}E[e^{saX}]=e^{sb}M_X(sa)$. </li>
<li>$\displaystyle\frac{d}{ds}M(s)=\int^\infty_{-\infty}xe^{sx}f_X(x)dx$ and $\displaystyle\left.\frac{d}{ds}M(s)\right|_{s=0}=\int^{\infty}_{-\infty}xf_X(x)dx=E[X]$<br />$\displaystyle\frac{d^n}{ds^n}M(s)=\int^{\infty}_{-\infty}x^ne^{sx}f_X(x)dx$ and $\displaystyle\left.\frac{d^n}{ds^n}M(s)\right|_{s=0}=\int^\infty_{-\infty}x^nf_X(x)dx=E[X^n]$</li>
<li>$M_X(0)=E[e^{0X}]=E[1]=1$</li>
<li>If $X$ takes only nonnegative integer values, then $\displaystyle\lim_{s\to-\infty}M_X(s)=P(X=0)$</li>
<li>If $Z=X+Y$ $M_Z(s)=E[e^{sZ}]=E[e^{s(X+Y)}]=E[e^{sX+sY}]=E[e^{sX}e^{sY}]$. If $X$ and $Y$ are independent, $M_Z(s)=E[e^{sX}]E[e^{sY}]=M_X(s)M_Y(s)$. </li>
<li>In discrete random variables: <br />&emsp;For $Bernoulli(p)$, $M_X(s)=1-p+pe^s$<br />&emsp;For $Binomial(n,p)$, $M_X(s)=(1-p+pe^s)^n$<br />&emsp;For $Geometric(p)$, $M_X(s)=\displaystyle\frac{pe^s}{1-(1-p)e^s}$<br />&emsp;For $Poisson(\lambda)$, $M_X(s)=e^{\lambda(e^s-1)}$<br />&emsp;For $Uniform(a,b)$, $M_X(s)=\displaystyle\frac{e^{sa}(e^{s(b-a+1)}-1)}{(b-a+1)(e^s-1)}$</li>
<li>In continuous random variables: <br />&emsp;For $Uniform(a,b)$, $M_X(s)=\displaystyle\frac{e^{sb}-e^{sa}}{s(b-a)}$<br />&emsp;For $Exponential(\lambda)$, $M_X(s)=\displaystyle\frac{\lambda}{\lambda-s}(s&lt;\lambda)$<br />&emsp;For $Normal(\mu,\sigma^2)$, $M_X(s)=e^{\frac{\sigma^2s^2}{2}+\mu s}$</li>
<li>$M_{X_1,…,X_n}(s_1,…,s_n)=E[e^{s_1X_1+…+s_nX_n}]$</li>
</ol>
<h1 id="Sum-of-a-Random-Number-of-Independent-Random-Variables"><a href="#Sum-of-a-Random-Number-of-Independent-Random-Variables" class="headerlink" title="Sum of a Random Number of Independent Random Variables"></a>Sum of a Random Number of Independent Random Variables</h1><ol>
<li>We consider the sum $Y=X_1+X_2+…+X_N$. <br />$N$ is a random variable that takes nonnegative integer values. <br />$X_1,X_2,…$ are identically independent distributed random variables. </li>
<li>If $N=0$, $Y=0$. </li>
<li>Fix a nonnegative integer $n$. <br />$E[Y|N=n]=E[X_1+X_2+…X_n]=nE[X]$. <br />So $E[Y|N]=NE[X]$. By the law of iterated expectation, $E[Y]=E[E[Y|N]]=E[NE[X]]=E[N]E[X]$</li>
<li>$\begin{equation}\begin{split}var(Y)&amp;=E[var(Y|N)]+var(E[Y|N])\\&amp;=E[Nvar(X)]+var(NE[X])\\&amp;=E[N]var(X)+(E[X])^2var(N)\end{split}\end{equation}$</li>
<li>$E[e^{sY}|N=n]=E[e^{sX_1}…e^{sX_n}]=E[e^{sX_1}]…E[e^{sX_n}]=(M_X(s))^n$. <br />$M_Y(s)=E[e^{sY}]=E[E[e^{sY}|N]]=E[(M_X(s))^N]=\displaystyle\sum^\infty_{n=0}(M_X(s))^np_N(n)=\sum^\infty_{n=0}e^{nlog M_X(s)}p_N(n)$<br />Compare with $M_N(s)=\displaystyle\sum^\infty_{n=0}e^{sn}p_N(n)$, $M_Y(s)=M_N(log M_X(s))$. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/14/07-Derived-Distributions-Convolution-Covariance-and-Correlation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/14/07-Derived-Distributions-Convolution-Covariance-and-Correlation/" class="post-title-link" itemprop="url">07. Derived Distributions, Convolution, Covariance and Correlation</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-14 23:03:14" itemprop="dateCreated datePublished" datetime="2020-03-14T23:03:14+08:00">2020-03-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-16 08:52:37" itemprop="dateModified" datetime="2020-03-16T08:52:37+08:00">2020-03-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Derived-Distributions"><a href="#Derived-Distributions" class="headerlink" title="Derived Distributions"></a>Derived Distributions</h1><ol>
<li>Calculate the PDF of $Y=g(X)$ of a continuous random variable $X$: <br />&emsp;First calculate the CDF $F_Y$ of $Y$ using $F_Y(y)=P(g(X)≤y)=\displaystyle\int_{\{x|g(x)≤y\}}f_X(x)dx$<br />&emsp;Differentiate to obtain the PDF of $Y$: $f_Y(y)=\displaystyle\frac{dF_Y}{dy}(y)$</li>
<li>If $Y=aX+b$, where $a$ and $b$ are scalars, with $a≠0$. Then $f_Y(y)=\displaystyle\frac{1}{|a|}f_X(\frac{y-b}{a})$<br />Graphical proof: The width will be $|a|$ times wider and the height will be $|a|$ times shorter. The points are shifted by $b$. If $a$ were negative, the graph will be flipped around the vertical axis. <br />Algebra proof: <br />&emsp;If $a&gt;0$<br />&emsp;&emsp;$F_Y(y)=P(Y≤y)=P(aX+b≤y)=P(X≤\displaystyle\frac{y-b}{a})=F_X(\frac{y-b}{a})$<br />&emsp;&emsp;$f_Y(y)=\displaystyle\frac{dF_Y}{dy}(y)=\frac{1}{a}f_X(\frac{y-b}{a})$<br />&emsp;If $a<0$<br />&emsp;&emsp;$F_Y(y)=P(Y≤y)=P(aX+b≤y)=P(X≥\displaystyle\frac{y-b}{a})=1-F_X(\frac{y-b}{a})$<br />&emsp;&emsp;$f_Y(y)=\displaystyle\frac{dF_Y}{dy}(y)=-\frac{1}{a}f_X(\frac{y-b}{a})$</li>
<li>$Y=g(X)$ is a monotonic function and its inverse is $X=h(Y)$. Assume that $h$ is differentiable. Then the PDF of $Y$ in the region where $f_Y(y)&gt;0$ is given by $f_Y(y)=f_X(h(y))\bigg|\displaystyle\frac{dh}{dy}(y)\bigg|$. <br />Proof: <br />&emsp;If $g$ is monotonically increasing<br />&emsp;&emsp;$F_Y(y)=P(g(X)≤y)=P(x≤h(y))=F_X(h(y))$<br />&emsp;&emsp;$f_Y(y)=\displaystyle\frac{dF_Y}{dy}(y)=f_X(h(y))\frac{dh}{dy}(y)$<br />&emsp;If $g$ is monotonically decreasing<br />&emsp;&emsp;$F_Y(y)=P(g(X)≤y)=P(X≥h(y))=1-F_X(h(y))$<br />&emsp;&emsp;$f_Y(y)=\displaystyle\frac{dF_Y}{dy}(y)=f_X(h(y))\bigg[-\frac{dh}{dy}(y)\bigg]$</li>
<li>This method also works for functions of multiple random variables. <br />&emsp;$F_Z(z)=P(g(X,Y)≤z)=\displaystyle\iint\limits_{\{(x,y)|(x,y)≤z\}}f_{X,Y}(x,y)dxdy$<br />&emsp;$f_Z(z)=\displaystyle\frac{dF_Z}{dz}(z)$</li>
<li>Two-sided exponential PDF (Laplace PDF): $f_Z(z)=\displaystyle\frac{\lambda}{2}e^{-\lambda|z|}$</li>
<li>For independent $X$ and $Y$, $Z=X+Y$. <br />&emsp;If $X$ and $Y$ are independent discrete random variables with PMFs $p_X,p_y$: <br />&emsp;&emsp;$\begin{equation}\begin{split}p_Z(z)&amp;=P(X+Y=z)\\&amp;=\displaystyle\sum_{\{(x,y)|x+y=z\}}P(X=x,Y=y)\\&amp;=\sum_xP(X=x,Y=z-x)\\&amp;=\sum_xp_X(x)p_Y(z-x) \end{split}\end{equation}$<br />&emsp;&emsp;The resulting PMF $p_Z$ is called the convolution of the PMFs of $X$ and $Y$. <br />&emsp;If $X$ and $Y$ are independent continuous random variables with PDFs $f_X$ and $f_Y$. <br />&emsp;&emsp;$\begin{equation}\begin{split}P(Z≤z|X=x)&amp;=P(X+Y≤z|X=x)\\&amp;=P(x+Y≤z|X=x)\\&amp;=P(x+Y≤z)\\&amp;=P(Y≤z-x) \end{split}\end{equation}$<br />&emsp;&emsp;Differentiating both sides with respect to $z$. $f_{Z|X}(z|x)=f_Y(z-x)$<br />&emsp;&emsp;So $f_{X,Z}(x,z)=f_X(x)f_{Z|X}(z|x)=f_X(x)f_Y(z-x)$<br />&emsp;&emsp;$f_Z(z)=\displaystyle\int^\infty_{-\infty}f_{X,Z}(x,z)dx=\int^\infty_{-\infty}f_X(x)f_Y(z-x)dx$<br />&emsp;&emsp;This is the convolution of the PDFs of $X$ and $Y$. </li>
<li>With the help of convolution, we know that the sum of two normal random variables with means $\mu_1,\mu_2$ and variance $\sigma_1^2,\sigma_2^2$ is another normal random variable with mean $\mu_1+\mu_2$ and variance $\sigma_1^2+\sigma^2_2$. </li>
<li>Graphical calculation of convolution: <br />&emsp;First plot $f_Y(z-t)$ as a function of $t$. <br />&emsp;Place the plots of $f_X(t)$ and $f_Y(z-t)$ on top of each other and form their product. <br />&emsp; The value of $f_Z(z)$ is the integral of the product of these two plots. </li>
</ol>
<h1 id="Covariance-and-Correlation"><a href="#Covariance-and-Correlation" class="headerlink" title="Covariance and Correlation"></a>Covariance and Correlation</h1><ol>
<li>The covariance of two random variables $X$ and $Y$, denoted by $cov(X,Y)$ is defined by $cov(X,Y)=E[(X-E[X])(Y-E[Y])]$. </li>
<li>When $cov(X,Y)=0$, we say that $X$ and $Y$ are uncorrelated. <br />Roughly speaking, a positive or negative covariance indicates that the value of $X-E[X]$ and $Y-E[Y]$ obtained in a single experiment tend to have the same or the opposite sign. </li>
<li>$cov(X,Y)=E[XY]-E[X]E[Y]$</li>
<li>$cov(X,X)=var(X)$</li>
<li>$cov(X,aY+b)=a\cdot cov(X,Y)$</li>
<li>$cov(X,Y+Z)=cov(X,Y)+cov(X,Z)$</li>
<li>If $X$ and $Y$ are independent, $E[XY]=E[X]E[Y]$, so $cov(X,Y)=0$. Thus, if $X$ and $Y$ are independent, they are also uncorrelated. But two uncorrelated random variables are not alway independent. </li>
<li>If $E[X|Y=y]=E[X]$, $E[XY]=\displaystyle\left\{\begin{array}{}\sum_yyp_Y(y)E[X|Y=y]=E[X]\sum_yyp_Y(y)=E[X]E[Y]&amp;X and Y are discrete\\\\\int^\infty_{-\infty}yf_y(y)E[X|Y=y]dy=E[X]\int^\infty_{-\infty}yf_Y(y)dy=E[X]E[Y]&amp;X and Y are continuous \end{array}\right.$<br />So $cov(X,Y)=0$. In this case $X$ and $Y$ are uncorrelated but not always independent. </li>
<li>The correlation coefficient of two random variables $X$ and $Y$ that have nonzero variances: $\rho(X,Y)=\displaystyle\frac{cov(X,Y)}{\sqrt{var(X)var(Y)}}$</li>
<li>The correlation coefficient is a normalized version of the covariance. It ranges from $-1$ to $1$. </li>
<li>If $\rho&gt;0$, then the value of $X-E[X]$ and $Y-E[Y]$ tend to have the same sign. if $\rho&lt;0$, then the value of $X-E[X]$ and $Y-E[Y]$ tend to have the opposite sign. </li>
<li>$\rho=\pm1$ means $X$ and $Y$ are strongly dependent, knowing one can restore another one. <br />$\rho=1$ if and only if there exists a positive constant $c$ such that $Y-E[Y]=c(X-E[X])$. <br />$\rho=-1$ if and only if there exists a negative constant $c$ such that $Y-E[Y]=c(X-E[X])$. </li>
<li>If $X_1,X_2,…,X_n$ are random variables with finite variance, we have $var\left(\displaystyle\sum^n_{i=1}X_i\right)=\sum^n_{i=1}var(X_i)+\sum_{\{(i,j)|i≠j\}}cov(X_i,X_j)$. <br />For $n=2$, $var(X_1+X_2)=var(X_1)+var(X_2)+2cov(X_1,X_2)$. <br />Proof: We denote $\tilde{X_i}=X_i-E[X_i]$. <br />&emsp;$\displaystyle\begin{equation}\begin{split}var\left(\sum^n_{i=1}X_i\right)&amp;=E\left[\left(\sum^n_{i=1}\tilde{X_i} \right)^2 \right]\\&amp;=E\left[\sum^n_{i=1}\sum^n_{j=1}\tilde{X_i}\tilde{X_j} \right]\\&amp;=\sum^n_{i=1}\sum^n_{j=1}E[\tilde{X_i}\tilde{X_j}]\\&amp;=\sum^n_{i=1}E[\tilde{X_i}^2]+\sum_{\{(i,j)|i≠j\}}E[\tilde{X_i}\tilde{X_j}]\\&amp;=\sum^n_{i=1}var(X_i)+\sum_{\{(i,j)|i≠j\}}cov(X_i,X_j) \end{split}\end{equation}$</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/14/06-Multiple-Continuous-Random-Variables-%20and-Conditional-PDFs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/14/06-Multiple-Continuous-Random-Variables-%20and-Conditional-PDFs/" class="post-title-link" itemprop="url">06. Multiple Continuous Random Variables and Conditional PDFs</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-14 19:35:04" itemprop="dateCreated datePublished" datetime="2020-03-14T19:35:04+08:00">2020-03-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-15 16:20:17" itemprop="dateModified" datetime="2020-03-15T16:20:17+08:00">2020-03-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Joint-PDFs"><a href="#Joint-PDFs" class="headerlink" title="Joint PDFs"></a>Joint PDFs</h1><ol>
<li>Two continuous random variables associated with the same experiment are jointly continuous and can be described in terms of a joint PDF $f_{X,Y}$. </li>
<li>$f_{X,Y}$ is a nonnegative function that satisfies $P((X,Y)\in B)=\displaystyle\iint\limits_{(x,y)\in B}f_{X,Y}(x,y)dxdy$, for every subset $B$ of the two-dimensional plane. </li>
<li>If $B=\{(x,y)|a≤x≤b,c≤y≤d\}$, $P(a≤X≤b,c≤Y≤d)=\displaystyle\int^d_c\int^b_af_{X,Y}(x,y)dxdy$<br />If $B$ is the entire two-dimensional plane, $P((X,Y)\in B)=\displaystyle\int^\infty_{-\infty}\int^\infty_{-\infty}f_{X,Y}(x,y)dxdy=1$</li>
<li>Like PDF of single random variable, the joint PDF can be viewed as the probability per unit area in the vicinity of a point. </li>
<li>$P(X\in A)=P(X\in A and Y\in (-\infty,\infty))=\displaystyle\int_A\int^\infty_{-\infty}f_{X,Y}(x,y)dydx$</li>
<li>The marginal PDF $f_X(x)=\displaystyle\int^\infty_{-\infty}f_{X,Y}(x,y)dy$. </li>
<li>The uniform model: <br />PDF: $f_{X,Y}(x,y)=\left\{\begin{array}{}\displaystyle\frac{1}{area of S}&amp;if (x,y)\in S\\0&amp;otherwise \end{array}\right.$<br /><br />Probability: $P((X,Y)\in A)=\displaystyle\iint\limits_{(x,y)\in A}f_{X,Y}(x,y)dxdy=\frac{1}{area of S}\iint\limits_{(x,y)\in A}dxdy=\frac{area of A}{area of S}$</li>
<li>We define the joint CDF of $X$ and $Y$ by $F_{X,Y}(x,y)=P(X≤x,Y≤y)=\displaystyle\int^x_{_\infty}\int^y_{-\infty}f_{X,Y}(s,t)dtds$. </li>
<li>$f_{X,Y}(x,y)=\displaystyle\frac{\partial^2F_{X,Y}}{\partial x\partial y}(x,y)$</li>
<li>For expectation: $E[g(X,Y)]=\displaystyle\int^\infty_{-\infty}\int^\infty_{-\infty}g(x,y)f_{X,Y}(x,y)dxdy$</li>
<li>$E[aX+bY+c]=aE[X]+bE[Y]+c$</li>
</ol>
<h1 id="Conditioning"><a href="#Conditioning" class="headerlink" title="Conditioning"></a>Conditioning</h1><ol>
<li>Given an event $A$ with $P(A)&gt;0$, the conditional PDF is $P(X\in B|A)=\displaystyle\int_Bf_{X|A}(x)dx$ for any subset $B$ of the real line. </li>
<li>If $P(x\in A)&gt;0$, $P(X\in B|X\in A)=\displaystyle\frac{P(X\in B,X\in A)}{P(X\in A)}=\frac{\int_{A\cap B}f_X(x)dx}{P(X\in A)}$. So $f_{X|\{X\in A\}}(x)=\left\{\begin{array}{}\displaystyle\frac{f_X(x)}{P(X\in A)}&amp;if x\in A\\0&amp;otherwise \end{array}\right.$</li>
<li>If two random variables $X$ and $Y$, event $C=\{(X,Y)\in A\}$, $f_{X,Y|C}(x,y)=\left\{\begin{array}{}\displaystyle\frac{f_{X,Y}(x,y)}{P(C)}&amp;if (x,y)\in A\\0&amp;otherwise \end{array}\right.$ and $f_{X|C}(x)=\displaystyle\int^\infty_{-\infty}f_{X,Y|C}(x)dy$</li>
<li>Total probability theorem: If disjoint sets $A_1,A_2,…,A_n$ form a partition of the sample space, $f_X(x)=\displaystyle\sum^n_{i=1}P(A_i)f_{X|A_i}(x)$. </li>
<li>If $X$ and $Y$ are continuous random variables with joint PDF $f_{X,Y}$. For any $y$ with $f_Y(y)&gt;0$, the conditional PDF of $X$ given that $Y=y$ is $f_{X|Y}(x|y)=\displaystyle\frac{f_{X,Y}(x,y)}{f_Y(y)}$. </li>
<li>Total probability theorem based on another continuous random variable: $f_X(x)=\displaystyle\int^\infty_{-\infty}f_Y(y)f_{X|Y}(x|y)dy$</li>
<li>It’s best to view $y$ as a fixed number and consider $f_{X|Y}(x|y)$ as a function of the single variable $x$. Viewed as a function of $x$, $f_{X|Y}(x|y)$ has the same shape as the joint PDF $f_{X,Y}$, because the denominator $f_Y$ does not depend on $x$. </li>
<li>$f_{X,Y|Z}(x,y|z)=\displaystyle\frac{f_{X,Y,Z}(x,y,z)}{f_Z(z)}$ and $f_{X|Y,Z}(x|y,z)=\displaystyle\frac{f_{X,Y,Z}(x,y,z)}{f_{Y,Z}(y,z)}$</li>
<li>$E[X|A]=\displaystyle\int^\infty_{-\infty}xf_{X|A}(x)dx$ and $E[X|Y=y]=\displaystyle\int^\infty_{-\infty}xf_{X|Y}(x|y)dx$</li>
<li>$E[g(X)|A]=\displaystyle\int^\infty_{-\infty}g(x)f_{X|A}(x)dx$ and $E[g(X)|Y=y]=\displaystyle\int^\infty_{-\infty}g(x)f_{X|Y}(x|y)dx$</li>
<li>If disjoint sets $A_1,A_2,…,A_n$ form a partition of sample space, $E[X]=\displaystyle\sum^n_{i=1}P(A_i)E[X|A_i]$. </li>
<li>$E[X]=\displaystyle\int^\infty_{-\infty}E[X|Y=y]f_Y(y)dy$</li>
</ol>
<h1 id="Continuous-Bayes-Rule"><a href="#Continuous-Bayes-Rule" class="headerlink" title="Continuous Bayes Rule"></a>Continuous Bayes Rule</h1><ol>
<li>Since $f_Xf_{Y|X}=f_{X,Y}=f_Yf_{X|Y}$, $f_{X|Y}(x|y)=\displaystyle\frac{f_X(x)f_{Y|X}(y|x)}{f_Y(y)}$. </li>
<li>We can also plug in the total probability theorem: $f_{X|Y}(x|y)=\displaystyle\frac{f_X(x)f_{Y|X}(y|x)}{\int^\infty_{-\infty}f_X(x)f_{Y|X}(y|x)}$</li>
<li>A discrete event $A$ and a continuous random variable $Y$<br />$\begin{equation}\begin{split}P(A|Y=y)&amp;\approx P(A|y≤Y≤y+\delta)\\&amp;=\displaystyle\frac{P(A)P(y≤Y≤y+\delta|A)}{P(y≤Y≤y+\delta)}\\&amp;\approx\frac{P(A)f_{Y|A}(y)\delta}{f_Y(y)\delta}\\&amp;=\frac{P(A)f_{Y|A}(y)}{f_Y(y)} \end{split}\end{equation}$<br />Substitute $f_Y(y)$, $P(A|Y=y)=\displaystyle\frac{P(A)f_{Y|A}(y)}{P(A)f_{Y|A}(y)+P(A^c)f_{Y|A^c}(y)}$<br />Let $A=\{N=n\}$ and $p_N$ be the PMF of $N$. $P(A|Y=y)=P(N=n|Y=y)=\displaystyle\frac{p_N(n)f_{Y|N}(y|n)}{f_Y(y)}=\frac{p_N(n)f_{Y|N}(y|n)}{\sum_ip_N(i)f_{Y|N}(y|i)}$</li>
<li>$A$ is still a discrete event and $Y$ is a continuous random variable<br />$f_{Y|A}(y)=\displaystyle\frac{f_Y(y)P(A|Y=y)}{P(A)}$<br />Also plug in the total probability theorem: $f_{Y|A}(y)=\displaystyle\frac{f_Y(y)P(A|Y=y)}{\int^\infty_{-\infty}f_Y(t)P(A|Y=t)dt}$</li>
</ol>
<h1 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h1><ol>
<li>Two continuous random variables $X$ and $Y$ are independent if their joint PDF is the product of the marginal PDFs: $f_{X,Y}(x,y)=f_X(x)f_Y(y)$, for all $x,y$. </li>
<li>Also $f_{X|Y}(x|y)=f_X(x)$, for all $y$ with $f_Y(y)&gt;0$ and all $x$. </li>
<li>$P(X\in A and Y\in B)=P(X\in A)P(Y\in B)$. </li>
<li>$F_{X,y}(x,y)=F_X(x)F_Y(y)$, for all $x, y$. </li>
<li>$E[g(X)h(Y)]=E[g(X)]E[h(Y)]$</li>
<li>$var(X+Y)=var(X)+var(y)$. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/14/05-Continuous-Random-Variables/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/14/05-Continuous-Random-Variables/" class="post-title-link" itemprop="url">05. Continuous Random Variables</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-14 15:46:33 / Modified: 18:13:24" itemprop="dateCreated datePublished" datetime="2020-03-14T15:46:33+08:00">2020-03-14</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Continuous-Random-Variables"><a href="#Continuous-Random-Variables" class="headerlink" title="Continuous Random Variables"></a>Continuous Random Variables</h1><ol>
<li>A random variable $X$ is called continuous if there is a nonnegative function $f_X$, called the probability density function (PDF) of $X$, such that $P(X\in B)=\displaystyle\int_Bf_X(x)dx$ for every subset $B$  of the real line. </li>
<li>In particular, the probability that the value of $X$ falls within an interval is $P(a≤X≤b)=\displaystyle\int^b_af_X(x)dx$. </li>
<li>The probability can be interpreted as the area under the graph of the PDF. </li>
<li>For any single value $a$, we have $P(X=a)=\displaystyle\int^a_af_X(x)dx=0$. <br />So including or excluding the nedpoints of an interval has no effect on its probability. $P(a≤X≤b)=P(a≤X&lt;b)=P(a&lt;X≤b)=P(a&lt;X&lt;b)$</li>
<li>$P(a≤X≤b)=P(X≤b)-P(X≤a)$</li>
<li>$\displaystyle\int^\infty_{-\infty}f_X(x)dx=1$</li>
<li>$P([x,x+\delta])=\displaystyle\int^{x+\delta}_xf_X(t)dt\approx f_X(x)\cdot\delta$. So we can view f_X(X) as the probability mass per unit length near $x$. Even though a PDF is used to calculate event probabilities, $f_X(x)$ is not the probability of any particular event. </li>
<li>$f_X(x)$ is not restricted to be less than or equal to one. </li>
<li>If a random variable $X$ takes values in an interval $[a,b]$ and any two subintervals of the same length have the same probability, we refer to this type of random variable as uniform or uniformly distributed. <br />$f_X(x)=\left\{\begin{array}{}\displaystyle\frac{1}{b-a}&amp;if a≤x≤b\\0&amp;otherwise \end{array}\right.$</li>
<li>The expected value of a continuous random variable $X$ is defined by $E[X]=\displaystyle\int^\infty_{-\infty}xf_X(x)dx$. </li>
<li>If $X$ is a continuous random variable, $Y=g(X)$ is also a random variable but may be a decrete one. But $E[g(X)]=\displaystyle\int^\infty_{-\infty}g(x)f_X(x)dx$ is true for any kind of $Y$, descrete or continuous. </li>
<li>The variance is still $var(X)=E[(X-E[X])^2]=\displaystyle\int_{-\infty}^\infty(x-E[X])^2f_X(x)dx$. <br />And $var(X)=E[(X-E[X])^2]$ still stands. </li>
<li>For uniform random variable: <br />$E[X]=\displaystyle\int^\infty_{-\infty}xf_X(x)dx=\int^b_ax\frac{1}{b-a}dx=\frac{a+b}{2}$<br />$E[X^2]=\displaystyle\int^\infty_{-\infty}x^2f_X(x)dx=\int^b_a\frac{x^2}{b-a}dx=\frac{b^3-a^3}{3(b-a)}=\frac{a^2+ab+b^2}{3}$<br />$var(X)=E[X^2]-(E[X])^2=\displaystyle\frac{a^2+ab+b^2}{3}-\frac{(a+b)^2}{4}=\frac{(b-a)^2}{12}$</li>
<li>Exponential random variable: <br />PDF: $f_X(x)=\left\{\begin{array}{}\lambda e^{-\lambda x}&amp;if x≥0\\0&amp;otherwise \end{array}\right.$<br />$P(X≥a)=\displaystyle\int^\infty_a\lambda e^{-\lambda x}dx=e^{-\lambda a}$<br />$\begin{equation}\begin{split}E[X]&amp;=\displaystyle\int^\infty_0x\lambda e^{-\lambda x}dx\\&amp;=(-xe^{-\lambda x})\Big|^\infty_0+\int^\infty_0e^{-\lambda x}dx\\&amp;=\frac{1}{\lambda} \end{split}\end{equation}$<br />$\begin{equation}\begin{split}E[X^2]&amp;=\displaystyle\int^\infty_0x^2\lambda e^{-\lambda x}dx\\&amp;=(-x^2e^{-\lambda x})\Big|^\infty_0+\int^\infty_02xe^{-\lambda x}dx\\&amp;=0+\frac{2}{\lambda}E[X]\\&amp;=\frac{2}{\lambda^2} \end{split}\end{equation}$<br />$var(X)=E[X^2]-(E[X])^2=\displaystyle\frac{1}{\lambda^2}$</li>
</ol>
<h1 id="Cumulative-Distribution-Functions"><a href="#Cumulative-Distribution-Functions" class="headerlink" title="Cumulative Distribution Functions"></a>Cumulative Distribution Functions</h1><ol>
<li>The cumulative distribution function (CDF) of a random variable $X$ is denoted by $F_X$ and provides probability $P(X≤x)$. <br />$F_X(x)=P(X≤x)=\left\{\begin{array}{}\displaystyle\sum_{k≤x}p_X(k)&amp;if X is discrete\\\int^x_{-\infty}f_X(t)dt&amp;if X is continuous \end{array}\right.$</li>
<li>CDF accumulates probability up to the value $x$. </li>
<li>The graph of CDF of a descrete random variable has a staircase form at the values of probability mass. At the points where a jump occurs, the value of $F_X$ is the larger of the two corresponding value, i.e. $F_X$ is continuous from the right. </li>
<li>$F_X$ is monotonically nondecreasing: if $x≤y$, then $F_X(x)≤F_X(y)$. </li>
<li>$\displaystyle\lim_{x\to-\infty}F_X=0$ and $\displaystyle\lim_{x\to\infty}F_X=1$</li>
<li>If $X$ is discrete, then $F_X(x)$ is a piecewise constant function of $x$. If $X$ is continuous, then $F_X(x)$ is a continuous function of $x$. </li>
<li>If $X$ is descrete, $p_X(k)=P(X≤k)-P(X≤k-1)=F_X(k)-F_X(k-1)$. If $X$ is continuous, $f_X(x)=\displaystyle\frac{dF_X}{dx}(x)$. </li>
<li>If $X_1,X_2,…,X_n$ are independent, $F_X(x)=F_{X_1}(x)…F_{X_n}(x)$</li>
<li>For geometric random variable, $F_{geo}(n)=\displaystyle\sum^n_{k=1}p(1-p)^{k-1}=1-(1-p)^2$<br />For exponential random variable, $F_{exp}(x)=\displaystyle\int^x_0\lambda e^{-\lambda t}dt=1-e^{-\lambda x}$ for $x&gt;0$ and $F_X(x)=0$, for $x≤0$. <br />$F_{geo}=F_{exp}$ when $x=n\delta$ ($\delta=-\displaystyle\frac{-ln(1-p)}{\lambda}$), with $n=1,2,…$. </li>
</ol>
<h1 id="Normal-Random-Variables"><a href="#Normal-Random-Variables" class="headerlink" title="Normal Random Variables"></a>Normal Random Variables</h1><ol>
<li>A continuous random variable $X$ is said to be normal or Gaussian if it has a PDF of the form $f_X(x)=\displaystyle\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$, where $\mu$ and $\sigma$ are two scalar parameters characterizing the PDF with $\sigma&gt;0$. </li>
<li>The PDF is symmetric around $\mu$. So its mean $E[X]$ has to be $\mu$. </li>
<li><br />$\begin{equation}\begin{split}var(X)&amp;=\displaystyle\frac{1}{\sqrt{2\pi}\sigma}\int^\infty_{-\infty}(x-\mu)^2e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx\\&amp;=\frac{\sigma^2}{\sqrt{2\pi}}\int^\infty_{-\infty}y^2e^{-\frac{y^2}{2}}dy                         y=\frac{x-\mu}{\sigma}\\&amp;=\frac{\sigma^2}{\sqrt{2\pi}}\Bigg[(-ye^{-\frac{y^2}{2}})\bigg|^\infty_{-\infty}+\int^\infty_{-\infty}e^{-\frac{y^2}{2}}dy\Bigg]\\&amp;=\sigma^2 \end{split}\end{equation}$</li>
<li>The standard normal random variable is a normal random variable with zero mean and unit variance. CDF: $\Phi(y)=P(Y≤y)=\displaystyle\frac{1}{\sqrt{2\pi}}\int^y_{-\infty}e^{-\frac{t^2}{2}}dt$</li>
<li>In standard normal random variable, for $y&lt;0$, $\Phi(y)=P(Y≤y)=P(Y≥-y)=1-P(Y≤-y)=1-\Phi(-y)$</li>
<li>For any $X$, we can standardize by $Y=\displaystyle\frac{X-\mu}{\sigma}$. And in reverse, any $X=\sigma Y+\mu$. So $P(X≤x)=P(\displaystyle\frac{X-\mu}{\sigma}≤\frac{x-\mu}{\sigma})=P(Y≤\frac{x-\mu}{\sigma})=\Phi(\frac{x-\mu}{\sigma})$</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/12/04-Discrete-Random-Variables-Probability-Mass-Functions-Expectations/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/12/04-Discrete-Random-Variables-Probability-Mass-Functions-Expectations/" class="post-title-link" itemprop="url">04. Discrete Random Variables; Probability Mass Functions; Expectations</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-12 18:14:27" itemprop="dateCreated datePublished" datetime="2020-03-12T18:14:27+08:00">2020-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-14 13:36:46" itemprop="dateModified" datetime="2020-03-14T13:36:46+08:00">2020-03-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Random-Variables"><a href="#Random-Variables" class="headerlink" title="Random Variables"></a>Random Variables</h1><ol>
<li>Sometimes we can use a random variable to associate the (numerical) value with each outcome. </li>
<li>A random variable is a real valued function of the experimental outcome. </li>
<li>A function of a random variable defines another random variable. </li>
<li>A random variable can be conditioned on an event or on another random variable. </li>
<li>A random variable can be independent of an event or from another random variable. </li>
<li>A random variable is called discrete if its range is either finite or countably infinite. </li>
<li>If $Y=g(X)$ is a function of a random variable $X$, then $Y$ is also a random variable. </li>
</ol>
<h1 id="Probability-Mass-Function"><a href="#Probability-Mass-Function" class="headerlink" title="Probability Mass Function"></a>Probability Mass Function</h1><ol>
<li>Probability mass function (PMF) of a discrete random variable $X$ can capture the probabilities of the values that it can take. PMF of $X$ is denoted $p_X$. </li>
<li>If $x$ is any possible value of $X$, the probability mass of $x$ denoted $p_X(x)$ is the probability of the event $\{X=x\}$ consisting of all outcomes that give rise to a value of $X$ equal to $x$: $p_X(x)=P(\{X=x\})$ or $p_X(x)=P(X=x)$. </li>
<li>Convention: upper case characters denote random variables and lower case characters denote real numbers such as the numerical values of a random variable. </li>
<li>The events $\{X=x\}$ are disjoint and form a partition of the sample space. So: $\displaystyle\sum_xp_X(x)=1$ and $P(X\in S)=\displaystyle\sum_{x\in S}pX(x)$. </li>
<li>$p_X(x)$ equals the sum of the probabilities of all outcomes that give rise to event $\{X=x\}$. </li>
<li>If $Y=g(X)$, $p_Y(y)=\displaystyle\sum_{\{x|g(x)=y\}}p_X(x)$. </li>
</ol>
<h1 id="Special-Random-Variable"><a href="#Special-Random-Variable" class="headerlink" title="Special Random Variable"></a>Special Random Variable</h1><ol>
<li>The Bernoulli random variable takes the two values $1$ and $0$. And its PMF is $p_X(x)=\left\{\begin{array}{}p&amp;if x=1\\1-p&amp;if x=0 \end{array}\right.$</li>
<li>The Binomial random variable: In $n$ times of Bernoulli experiment, $x=1$ has happened $k$ times. Its PMF is $p_X(k)=P(X=k)=\big(^n_k\big)p^k(1-p)^{n-k}$. </li>
<li>The geometric random variable: In $n$ times of Bernoulli experiment, $x=1$ haven’t occurred until $k$th time. Its PMF is $p_X(k)=(1-p)^{k-1}p$. </li>
<li>The Poisson random variable has PMF of $p_X(k)=e^{-\lambda}\displaystyle\frac{\lambda^k}{k!}$. This is a binomial random variable with very small $p$ and very large $n$. $e^{-\lambda}\displaystyle\frac{\lambda^k}{k!}\approx\big(^n_k\big)p^k(1-p)^{n-k}$. $\lambda=np$</li>
</ol>
<h1 id="Expectation-and-Variance"><a href="#Expectation-and-Variance" class="headerlink" title="Expectation and Variance"></a>Expectation and Variance</h1><ol>
<li>The expectation of $X$ is a weighted average of the possible values of $X$. $E[X]=\displaystyle\sum_xxp_X(x)$. </li>
<li>The expectationid well-defined if $\displaystyle\sum_x|x|p_X(x)&lt;\infty$. In this case, it’s known that the infinite sum converges to a finite value that is independent of the order in which the various terms are summed. </li>
<li>The expectation is the center of gravity of the PMF. The sum of the torques from the weights to its left is equal to the sum of the torques from the weights to its right. $\displaystyle\sum_x(x-c)p_X(x)=0$. </li>
<li>The $n$th moment as $E[X^n]=\displaystyle\sum_xx^np_X(x)$. </li>
<li>The variance of $X$ is $var(X)=E[(X-E[X])^2]$. </li>
<li>The variance provides a measure of dispersion of $X$ around its mean. </li>
<li>Another measure of dispersion is the standard deviation of $X$, $\sigma_X=\sqrt{var(X)}$. </li>
<li>The PMF of $(X-E[X])^2$ is not alway the same PMF of $X$. </li>
<li>Expected value rule for functions of random variables: $E[g(X)]=\displaystyle\sum_xg(x)p_X(x)$. <br />So the variance: $var(X)=E[(X-E[x])^2]=\displaystyle\sum_x(x-E[x])^2p_X(x)$. <br />The $n$th moment: $E[X^n]=\displaystyle\sum_xx^np_X(x)$. </li>
<li>Variance is zero if and only if all $x=E[X]$. </li>
<li>In general, $E[g(X)]≠g(E[X])$</li>
</ol>
<h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><ol>
<li>The expectation of a constant is itself. $E[c]=c$. </li>
<li>Define $Y=aX+b$, $E[Y]=\displaystyle\sum_x(ax+b)p_X(x)=a\sum_xxp_X(x)+b\sum_xp_X(x)=aE[x]+b$<br />$\begin{equation}\begin{split}var(Y)&amp;=\displaystyle\sum_x(ax+b-E[aX+b])^2p_X(x)\\&amp;=\sum_x(ax+b-aE[x]-b)^2p_X(x)\\&amp;=a^2\sum_x(x-E[x])^2p_X(x)\\&amp;=a^2var(X) \end{split}\end{equation}$</li>
<li><br />$\begin{equation}\begin{split}var(X)&amp;=\sum_x(x-E[X])^2p_X(x)\\&amp;=\sum_x(x^2-2xE[X]+(E[X])^2)p_X(x)\\&amp;=\sum_xx^2p_X(x)-2E[x]\sum_xxp_X(x)+(E[X])^2\sum_xp_X(x)\\&amp;=E[X^2]-2(E[X])^2+(E[X])^2\\&amp;=E[X^2]-(E[X])^2 \end{split}\end{equation}$</li>
<li>For discrete uniform over $[a,b]$: <br />$p_X(k)=\left\{\begin{array}{}\displaystyle\frac{1}{b-a+1}&amp;if k=a,a+1,…b\\0&amp;otherwise \end{array}\right.$<br />$E[X]=\displaystyle\frac{a+b}{2}$, $var(X)=\displaystyle\frac{(b-a)(b-a+2)}{12}$</li>
<li>For Bernoulli random variable: <br />$E[X]=1\cdot p+0\cdot(1-p)=p$<br />$E[X^2]=1^2\cdot p+0^2\cdot(1-p)=p$<br />$var(X)=E[X^2]-(E[X])^2=p-p^2=p(1-p)$</li>
<li>For Poisson random variable: <br />$\begin{equation}\begin{split}E[X]&amp;=\displaystyle\sum_{k=0}^\infty ke^{-\lambda}\frac{\lambda^k}{k!}\\&amp;=\sum^\infty_{k=1}ke^{-\lambda}\frac{\lambda^k}{k!}\\&amp;=\lambda\sum^\infty_{k=1}e^{-\lambda}\frac{\lambda^{k-1}}{(k-1)!}\\&amp;=\lambda\sum^\infty_{m=0}e^{-\lambda}\frac{\lambda^m}{m!}\\&amp;=\lambda \end{split}\end{equation}$</li>
</ol>
<h1 id="Other-PMF-Models"><a href="#Other-PMF-Models" class="headerlink" title="Other PMF Models"></a>Other PMF Models</h1><h2 id="Joint-PMF"><a href="#Joint-PMF" class="headerlink" title="Joint PMF"></a>Joint PMF</h2><ol>
<li>The probability of the values that $X$ and $Y$ can take are captured by the joint PMF of $X$ and $Y$, denoted $p_{X,Y}$. $p_{X,Y}(x,y)=P(X=x,Y=y)$. </li>
<li>$P((X,Y)\in A)=\displaystyle\sum_{(x,y)\in A}p_{X,Y}(x,y)$</li>
<li>Sometimes, we refer to $p_X$ and $p_Y$ as the marginal PMFs. And we can calculate the marginla PMFs by using the tabular method, $p_X(x)=\displaystyle\sum_{y}p_{X,Y}(x,y)$</li>
<li>If $Z=g(X,Y)$, $p_Z(z)=\displaystyle\sum_{\{(x,y)|g(x,y=z)\}}p_{X,Y}(x,y)$</li>
<li>The expected value rule: $E[g(X,Y)]=\displaystyle\sum_x\sum_yg(x,y)p_{X,Y}(x,y)$</li>
<li>$E[aX+bY+c]=aE[X]+bE[Y]+c$</li>
<li>For Binomial random variable: <br />$E[X]=\displaystyle\sum_{i=1}^nE[X_i]=\sum^n_{i=1}p=np$</li>
</ol>
<h2 id="Conditional-PMF"><a href="#Conditional-PMF" class="headerlink" title="Conditional PMF"></a>Conditional PMF</h2><ol>
<li>$p_{X|A}(x)=P(X=x|A)=\displaystyle\frac{P(\{X=x\}\cap A)}{P(A)}=\frac{P(\{X=x\}\cap A)}{\sum_xP(\{X=x\}\cap A)}$</li>
<li>$E[X|A]=\displaystyle\sum_xxp_{X|A}(x)$<br />$E[X|Y=y]=\displaystyle\sum_xxp_{X|Y}(x|y)$</li>
<li>$p_{X|Y}(x|y)=P(X=x|Y=y)=\displaystyle\frac{p_{X,Y}(x,y)}{p_Y(y)}$</li>
<li>$p_{X,Y}(x,y)=p_{X|Y}(x|y)\cdot p_Y(y)=p_{Y|X}(y|x)\cdot p_X(x)$</li>
<li>Partition of sample space into disjoint events $A_1,A_2,…,A_n$. <br />$p_X(x)=\displaystyle\sum_{i=1}^nP(A_i)p_{X|A_i}(x)$<br />$p_{X|B}(x)=\displaystyle\sum^n_{i=1}P(A_i|B)p_{X|A_i\cap B}(x)$<br /> Total expectation theorem: $E[X]=\displaystyle\sum^n_{i=1}P(A_i)E[X|A_i]$<br />$E[X|B]=\displaystyle\sum^n_{i=1}P(A_i|B)E[X|A_i\cap B]$</li>
<li>$E[g(X)|A]=\displaystyle\sum_xg(x)p_{X|A}(x)$</li>
<li>$E[X]=\displaystyle\sum_yp_Y(y)E[X|Y=y]$</li>
<li>Geometric random variable: <br />$E[X]=P(X=1)E[X|X=1]+P(X&gt;1)E[X|X&gt;1]$<br />$E[X|X&gt;1]=E[X-1|X-1&gt;0]+1=E[X]+1$<br />So $E[X]=p\cdot 1+(1-p)(E(x)+1)$. <br />$E[X]=\displaystyle\frac{1}{p}$<br />Similarly, we can obtain $E[X^2|X&gt;1]=E[(1+X)^2|X+1&gt;1]=E[(1+X)^2]=1+2E[X]+E[X^2]$<br />So that $E[X^2]=p\cdot 1+(1-p)(1+2E[X]+E[X^2])$ from which we obtain $E[X^2]=\displaystyle\frac{2}{p^2}-\frac{1}{p}$. <br />$var(X)=E[X^2]-(E[X])^2=\displaystyle\frac{1-p}{p^2}$</li>
</ol>
<h1 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h1><ol>
<li>The random variable $X$ is independent of the event $A$ if $P(X=x and A)=P(X=x)P(A)=p_X(x)P(A)$ for all $x$. </li>
<li>$P(X=x and A)=p_{X|A}(x)P(A)$</li>
<li>$p_{X|A}(x)=p_X(x)$ for all $x$</li>
<li>Two random variables $X$ and $Y$ are independent if $p_{X,Y}(x,y)=p_X(x)p_Y(y)$, for all $x,y$. </li>
<li>$X$ and $Y$ are said to be conditionally independent, given a positive probability event $A$, if $P(X=x,Y=y|A)=P(X=x|A)P(Y=y|A)$ or $p_{X,Y|A}(x,y)=p_{X|A}(x)p_{Y|A}(y)$, for all $x$ and $y$. </li>
<li>If $X $ and $Y$ are independent random variables, then <br />$g(X)$ and $h(Y)$ are also independent. <br />$E[XY]=E[X]E[Y]$ and $E[g(X)h(Y)]=E[g(X)]E[h(Y)]$<br />$var(X+Y)=var(X)+var(Y)$</li>
<li>Three  random variables $X,Y,Z$ are said to be independent if $p_{X,Y,Z}(x,y,z)=p_X(x)p_Y(y)p_Z(z)$ for all $x,y,z$</li>
<li>If $X,Y,Z$ are independent, then <br />$f(X),g(Y),h(Z)$ are also independent and $g(X,Y)$ and $h(Z)$ are independent. <br />$var(\displaystyle\sum^n_{i=1}X_i)=\sum^n_{i=1}var(X_i)$</li>
<li>For Binomial random variable: <br />For each toss, we let $X_i$ be the random variable which is equal to 1 if the $i$th toss comes up a head. Then $X=\displaystyle\sum^n_{i=1}X_i$<br />$var(X)=\displaystyle\sum^n_{i=1}v ar(X_i)=np(1-p)$</li>
<li>For Poisson random variable: <br />$E[Y]=\lambda$<br />$\begin{equation}\begin{split}E[Y^2]&amp;=\displaystyle\sum^\infty_{k=1}k^2e^{-\lambda}\frac{\lambda^k}{k!}\\&amp;=\lambda\sum^\infty_{k=1}k\frac{e^{-\lambda}\lambda^{k-1}}{(k-1)!}\\&amp;=\lambda\sum^\infty_{m=0}(m+1)\frac{e^{-\lambda}\lambda^m}{m!}\\&amp;=\lambda(E[Y]+1)\\&amp;=\lambda(\lambda+1) \end{split}\end{equation}$<br />$var(Y)=E[Y^2]-(E[Y])^2=\lambda(\lambda+1)-\lambda^2=\lambda$</li>
<li>For Bernoulli random variable: <br />Let $X=1$ is event occurred. Then $E[X]=p,var(X)=p(1-p)$<br />Sample mean $S_n=\displaystyle\frac{X_1+X_2+…+X_n}{n}$<br />$E[S_n]=\displaystyle\sum^n_{i=1}\frac{1}{n}E[X_i]=p$<br />$var(S_n)=\displaystyle\sum^n_{i=1}\frac{1}{n^2}var(X_i)=\frac{p(1-p)}{n}$<br />Not only for Bernoulli experiments, $var(S_n)=\displaystyle\frac{var(X)}{n}$ as long as $X_i$ are independent, with common mean $E[X]$ and variance $var(X)$. <br />The sample mean $S_n$ can be viewed as a good estimate of the probability of $X=1$. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/12/03-Independence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/12/03-Independence/" class="post-title-link" itemprop="url">03. Independence</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-12 12:35:13" itemprop="dateCreated datePublished" datetime="2020-03-12T12:35:13+08:00">2020-03-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-13 21:00:17" itemprop="dateModified" datetime="2020-03-13T21:00:17+08:00">2020-03-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Independence"><a href="#Independence" class="headerlink" title="Independence"></a>Independence</h1><ol>
<li>$A$ is independent of $B$ if $P(A|B)=P(A)$. The occurrence of $B$ provides no such information and does not alter the probability that $A$ has occurred. </li>
<li>$P(A|B)=\displaystyle\frac{P(A\cap B)}{P(B)}$. So if $A$ is independent of $B$, $P(A\cap B)=P(A|B)P(B)=P(A)P(B)$. </li>
<li>We take $P(A\cap B)=P(A)P(B)$ as the definition of independent, because it can be used even when $P(B)=0$. </li>
<li>For $A$ and $B$, the definition is a symmetric property. If $A$ is independent of $B$, then $B$ is independent of $A$, and we can unambiguously say that $A$ and $B$ are independent events. </li>
<li>Disjoint sets definitely not independent. </li>
<li>If $A$ and $B$ are independent, the same holds true for $A$ and $B^c$. </li>
</ol>
<h1 id="Conditional-Independence"><a href="#Conditional-Independence" class="headerlink" title="Conditional Independence"></a>Conditional Independence</h1><ol>
<li>Given an event $C$, the events $A$ and $B$ are called conditionally independent if $P(A\cap B|C)=P(A|C)P(B|C)$. </li>
<li>$\begin{equation}\begin{split}P(A\cap B|C)&amp;=\displaystyle\frac{P(A\cap B\cap C)}{P(C)}\\&amp;=\frac{P(C)\frac{P(B\cap C)}{P(C)}\frac{P(A\cap B\cap C)}{P(B\cap C)}}{P(C)}\\&amp;=\frac{P(C)P(B|C)P(A|B\cap C)}{P(C)}\\&amp;=P(B|C)P(A|B\cap C)\end{split}\end{equation}$</li>
<li>Also $P(A\cap B|C)=P(A|C)P(B|C)$. So we see the conditional independence is the same as the condition $P(A|B\cap C)=P(A|C)$</li>
<li>This relation states that is $C$ is known to have occurred, the additional knowledge that $B$ also occurred does not change the probability of $A$. </li>
<li>Independence of two events $A$ and $B$ with respect to the unconditional probability law does not imply conditional independence and vice versa. </li>
</ol>
<h1 id="Independence-of-a-Collection-of-Events"><a href="#Independence-of-a-Collection-of-Events" class="headerlink" title="Independence of a Collection of Events"></a>Independence of a Collection of Events</h1><ol>
<li>Events $A_1,…,A_n$ are independent if $\displaystyle P(\bigcap_{i\in S}A_i)=\prod_{i\in S}P(A_i)$, for every subset $S$ of $\{1,2,…,n\}$</li>
<li>Distinguish from pairwise independence which is defined as any two events are independent. </li>
<li>We can substitute any set with its complement with the independence true. </li>
</ol>
<h1 id="Independent-Trials-and-Binomial-Probabilities"><a href="#Independent-Trials-and-Binomial-Probabilities" class="headerlink" title="Independent Trials and Binomial Probabilities"></a>Independent Trials and Binomial Probabilities</h1><ol>
<li>If an experiment involves a sequence of independent but identical stages, we say that we have a sequence of independent trials. </li>
<li>In the special case where there are only two possible results at each stage, we say that we have a sequence of independent Bernoulli trials. </li>
<li>Two possible results $A$ and $B$, and the probability of $A$ is $p$. In $n$ experiments, the probability that $A$ occurs $k$ times is $p(k)=\big(^n_k\big)p^k(1-p)^{n-k}$. <br />$\big(^n_k\big)=\displaystyle\frac{n!}{k!\cdot(n-k)!}$</li>
<li>The sum of probabilities of each possible outcomes should be $1$. <br />Binomial formula: $\displaystyle\sum^n_{k=0}\big(^n_k\big)p^k(1-p)^{n-k}=1$<br />When $p=\displaystyle\frac{1}{2}$, $\displaystyle\sum^n_{k=0}\big(^n_k\big)=2^2$</li>
<li>A similar coefficient is the multinomial coefficient: <br />$\begin{equation}\begin{split}\big(^{     n}_{n_1n_2…n_r}\big)&amp;=\big(^n_{n_1}\big)\big(^{n-n_1}_{  n_2}\big)\big(^{n-n_1-n_2}_{    n_3})…\big(^{n-n_1-…-n_{r-1}}_{          n_r})\\&amp;=\displaystyle\frac{n!}{n_1!(n-n_1)!}\frac{(n-n_1)!}{n_2!(n-n_1-n_2!)}…\frac{(n-n_1-…-n_{r-1})!}{n_r!(n-n_1-…-n_r)!}\\&amp;=\frac{n!}{n_1!n_2!…n_r!}\end{split}\end{equation}$</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/10/02-Conditioning-and-Bayes-rule/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/10/02-Conditioning-and-Bayes-rule/" class="post-title-link" itemprop="url">02. Conditioning and Bayes' rule</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-10 09:11:31" itemprop="dateCreated datePublished" datetime="2020-03-10T09:11:31+08:00">2020-03-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-12 12:33:40" itemprop="dateModified" datetime="2020-03-12T12:33:40+08:00">2020-03-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Conditional-Probability"><a href="#Conditional-Probability" class="headerlink" title="Conditional Probability"></a>Conditional Probability</h1><ol>
<li>$P(A|B)$ means the conditional probability of $A$ given $B$. </li>
<li>$P(A|B)=\displaystyle\frac{P(A\cap B)}{P(B)}$, if $P(B)&gt;0$. If $P(B)=0$, the probability $P(A|B)$ is undefined. </li>
<li>Three probability laws: <br />&emsp;$P(A|B)&gt;0$<br />&emsp;$P(\Omega|B)=1$<br />&emsp;$P(A_1\cup A_2|B)=P(A_1|B)+P(A_2|B)$ if $A_1$ and $A_2$ are two disjoint sets. </li>
<li>For any two sets $A$ and $C$, $P(A\cup C|B)≤P(A|B)+P(C|B)$</li>
<li>$P(B|B)=\displaystyle\frac{P(B)}{P(B)}=1$</li>
<li>We might discard all possible outcomes outside $B$ and treat the conditional probabilities as a probability law defined on the new universe $B$. </li>
<li>Often $P(A\cap B)$ is not easy to calculate directly. But we can calculate $P(A|B)$ first and then use $P(A\cap B)=P(B)P(A|B)$ to get the answer. </li>
<li>Calculate various probabilities in conjunction with a tree-based sequential description of an experiment: <br />&emsp;First set up the tree so that an event of interest is associated with a leaf. <br />&emsp;Every node represents the probability of an event which is the conjunction of all passed events from root to it. <br />&emsp;Every path is the conditional probability of the next event given the conjunction of all the afore-passed events. </li>
<li>Multiplication rule: <br /><script type="math/tex">\begin{equation}\begin{split}P(\bigcap^n_{i=1}A_i)&=\displaystyle P(A_1)\frac{P(A_1\cap A_2)}{P(A_1)}\frac{P(A_1\cap A_2\cap A_3)}{P(A_1\cap A_2)}…\frac{P(\bigcap^n_{i=1}A_i)}{P(\bigcap^{n-1}_{i=1}A_i)} \\&=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)…P(A_n|\bigcap^{n-1}_{i=1}A_i)\end{split}\end{equation}</script></li>
</ol>
<h1 id="Total-Probability-Theorem-and-Bayes’-Rule"><a href="#Total-Probability-Theorem-and-Bayes’-Rule" class="headerlink" title="Total Probability Theorem and Bayes’ Rule"></a>Total Probability Theorem and Bayes’ Rule</h1><ol>
<li>A set of sets is a partition of the sample space if and only if each possible outcomes is included in exactly one of the sets. </li>
<li>Total probability theorem: Let $A_1,…,A_2$ be disjoint events that form a partion of the sample space and assume that $P(A_i)&gt;0$, for all $i$. <br />For any event $B$, <br /><script type="math/tex">\begin{equation}\begin{split}P(B)&=P(\Omega\cap B)=P((A_1\cup A_2\cup…\cup A_n)\cap B)\\&=P((A_1\cap B)\cup(A_2\cap B)\cup…\cup(A_n\cap B))\\&=P(A_1\cap B)+P(A_2\cap B)+…+P(A_n\cap B)\\&=P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+…+P(A_n)P(B|A_n) \end{split}\end{equation}</script></li>
<li>We are partition the sample space into a number of scenarios. Then the probability that $B$ occurs is a weighted average of its conditional probability under each scenarios, where each scenario is weighted according to its unconditional probability. </li>
<li>Bayes’ rule: The condition of Bayes’ rule is the same as total probability theorem<br />Since $P(A_i\cap B)=P(B)P(A_i|B)=P(A_i)P(B|A_i)$<br /><script type="math/tex">\begin{equation}\begin{split}P(A_i|B)&=\displaystyle\frac{P(A_i)P(B|A_i)}{P(B)}\ \ \ \ \ \ \ \ Plug\ in\ total\ probability\ theorem\\&=\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+…+P(A_n)P(B|A_n)} \end{split}\end{equation}</script></li>
<li>Bayes’ rule is often used for inference. We observe the effect, and we wish to infer the cause.<br />The events $A_i$’s are associated with the causes and the event $B$ represents the effect. <br />The probability $P(B|A_i)$ that the effect will be observed when the cause is represent amounts to a probabilistic model of the cause-effect relation. <br />The probability $P(A_i|B)$ is the probability that the cause $A_i$ is present when the effect has been observed. <br />We refer $P(A_i|B)$ as the posterior probability of event $A_i$ given the information. $P(A_i)$ is called the prior probability. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/03/04/01-Probability-Models-and-Axioms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/04/01-Probability-Models-and-Axioms/" class="post-title-link" itemprop="url">01. Probability Models and Axioms</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-03-04 11:48:19" itemprop="dateCreated datePublished" datetime="2020-03-04T11:48:19+08:00">2020-03-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-13 10:03:17" itemprop="dateModified" datetime="2020-03-13T10:03:17+08:00">2020-03-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/" itemprop="url" rel="index">
                    <span itemprop="name">Mathematics</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Mathematics/Probability-MIT-6-041/" itemprop="url" rel="index">
                    <span itemprop="name">Probability (MIT 6.041)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Sets"><a href="#Sets" class="headerlink" title="Sets"></a>Sets</h1><ol>
<li>A <code>set</code> is a collection of objects which are the <code>elements of the set</code>. </li>
<li>If <script type="math/tex">S</script> is a set and <script type="math/tex">x</script> is an element of <script type="math/tex">S</script>, we write <script type="math/tex">x\in S</script>. If <script type="math/tex">x</script> is not an element of <script type="math/tex">S</script>, we write <script type="math/tex">x\notin S</script>. </li>
<li><code>Empty set</code>: a set has no elements, denoted by <script type="math/tex">\emptyset</script>. </li>
<li>If <script type="math/tex">S</script> contains a finite number of elements, we write it as a list of the elements in braces: <script type="math/tex">S=\{x_1,x_2,…,x_n\}</script><br />If <script type="math/tex">S</script> contains contains infinitely many elements which can be enumerated in a list, we write <script type="math/tex">S=\{x_1,x_2,…\}</script>. <script type="math/tex">S</script> is <code>countably infinite</code>. <br />If all <script type="math/tex">x</script>‘s in <script type="math/tex">S</script> have a certain property <script type="math/tex">P</script>, we write <script type="math/tex">S=\{x|x\ satisfies\ P\}</script>. <script type="math/tex">|</script> is to be read as “such that”. In this case, <script type="math/tex">x</script> take a continuous range of values, and cannot be written down in a list. So <script type="math/tex">S</script> is called uncountable. </li>
<li>If every element of <script type="math/tex">S</script> is also an element of <script type="math/tex">T</script>, <script type="math/tex">S</script> is a <code>subset</code> of <script type="math/tex">T</script>, <script type="math/tex">S\subset T</script>. </li>
<li>If <script type="math/tex">S\subset T</script> and <script type="math/tex">T\subset S</script>, two sets are <code>equal</code>, <script type="math/tex">S=T</script>. </li>
<li><code>Universal set</code> contains all objects that could conceivably be of interest in a particular context. It’s usually denoted by <script type="math/tex">\Omega</script>. </li>
<li>Sets <script type="math/tex">S</script> are subsets of <script type="math/tex">\Omega</script>. </li>
<li>The <code>complement of a set</code> <script type="math/tex">S</script>, <code>with respect to the universe</code> <script type="math/tex">\Omega</script>, is <script type="math/tex">\{x\in\Omega|x\notin S\}</script>. It’s denoted by <script type="math/tex">S^c</script>. </li>
<li><script type="math/tex">\Omega^c=\emptyset</script>. </li>
<li><code>Union</code> of <script type="math/tex">S</script> and <script type="math/tex">T</script>: <script type="math/tex">S\cup T=\{x|x\in S\ or\ x\in T\}</script></li>
<li><code>Intersection</code> of <script type="math/tex">S</script> and <script type="math/tex">T</script>: <script type="math/tex">S\cap T=\{x|x\in S\ and\ x\in T\}</script></li>
<li>Two sets are <code>disjoint</code> if <script type="math/tex">S\cap T=\emptyset</script>. Several sets are disjoint if no two of them have common element. </li>
<li>A collection of sets is a <code>partition</code> of a set <script type="math/tex">S</script> if the sets are disjoint and their union is <script type="math/tex">S</script>. </li>
<li>If <script type="math/tex">x</script> and <script type="math/tex">y</script> are two objects, we use <script type="math/tex">(x,y)</script> to denote the <code>ordered pair</code> of <script type="math/tex">x</script> and <script type="math/tex">y</script>. </li>
<li>The set of scalars (real number) is denoted by <script type="math/tex">R</script>. The <script type="math/tex">n</script>-dimensional space is denoted by <script type="math/tex">R^n</script>. </li>
<li><code>Venn diagrams</code> are used to visualize sets. </li>
<li>Algebra of sets: <br />&emsp;<script type="math/tex">\begin{equation}\begin{split}S\cup T&=T\cup S\\ S\cap(T\cup T)&=(S\cap T)\cup(S\cap U)\\ (S^c)^c&=S\\ S\cup\Omega&=\Omega\\ (\bigcup\limits_nS_n)^c&=\bigcap\limits_nS^c_n \end{split}\end{equation}</script> &emsp; <script type="math/tex">\begin{equation}\begin{split}S\cup(T\cup U)&=(S\cup T)\cup U\\S\cup(T\cap U)&=(S\cup T)\cap(S\cup U)\\S\cap S^c&=\emptyset\\S\cap\Omega&=S\\ (\bigcap\limits_nS_n)^c&=\bigcup\limits_nS^c_n \end{split}\end{equation}</script></li>
</ol>
<h1 id="Probabilistic-Models"><a href="#Probabilistic-Models" class="headerlink" title="Probabilistic Models"></a>Probabilistic Models</h1><h2 id="Sample-Space"><a href="#Sample-Space" class="headerlink" title="Sample Space"></a>Sample Space</h2><ol>
<li>Every probabilistic model invelves an underlying process, called the <code>experiment</code>, that will produce exactly one out of several possible outcomes. </li>
<li><code>Sample space</code> <script type="math/tex">\Omega</script> contains all possible outcomes of an experiment. </li>
<li><code>Event</code> is any collection of possible outcomes, including the entire sample sapce and the empty set. </li>
<li><code>Probabilistic model</code> must contain the sample space and the probability law. </li>
<li>In our formulation of a probabilistic model, there is only one experiment. </li>
<li>Three requirements of sample space: <br />&emsp;Sample space may consist of a finite or an infinite possible of outcomes. <br />&emsp;Different elements of the sample space should be distinct and mutually exclusive. <br />&emsp;The sample space chosen for a probabilistic model must be collectively exhaustive. <br />&emsp;The sample space should have enough detail to distinguish between all outcomes of interest to the modeler. </li>
<li>$0$ Probability doesn’t means impossible which only stands for extremely unlikely. $1$ Probability doesn’t means certain which only stands for extremely likely. $0$ probability events may happen while $1$ probability events may not occur. </li>
</ol>
<h2 id="Probability-Law"><a href="#Probability-Law" class="headerlink" title="Probability Law"></a>Probability Law</h2><ol>
<li>Probability law specifies the likelihood of any event. </li>
<li>The probability law assigns to every event $A$ a number $P(A)$, called probability of $A$. </li>
<li>Probability axioms: <br />&emsp;Nonnegativity: $P(A)≥0$, for every event $A$. <br />&emsp;Additivity: If $A$ and $B$ are two disjoint events, then $P(A\cup B)=P(A)+P(B)$. <br />&emsp;Normalization: The probability of the entire sample space is 1, $P(\Omega)=1$. </li>
<li>$P(\emptyset)=0$, for $1=P(\Omega)=P(\Omega\cup\emptyset)=P(\Omega)+P(\emptyset)=1+P(\emptyset)$. </li>
<li>If $A\subset B$, then $P(A)≤P(B)$. </li>
<li>$P(A\cup B)=P(A)+P(B)-P(A\cap B)≤P(A)+P(B)$</li>
<li>$P(A\cup B\cup C)=P(A)+P(A^c\cap B)+P(A^c\cap B^c\cap C)$</li>
</ol>
<h3 id="Discrete-Models"><a href="#Discrete-Models" class="headerlink" title="Discrete Models"></a>Discrete Models</h3><ol>
<li>Discrete probability law: If the sample space consists of a finite number of possible outcomes, $P(\{s_1,s_2,…,s_n\})=\displaystyle\sum^n_{i=1}P(s_i)$. Here $s_i$ is a single element. </li>
<li>Discrete Uniform Probability Law: If the sample space consists of $n$ possible outcomes which are equally likely (i.e. all single-element events have the same probability), the the probability of any event $A$ is given by $P(A)=\displaystyle\frac{number of elements of A}{n}$. </li>
</ol>
<h3 id="Continuous-Models"><a href="#Continuous-Models" class="headerlink" title="Continuous Models"></a>Continuous Models</h3><ol>
<li>The sample space of these models usually has infinite number of elements. </li>
<li>In continuous models, the probability of each single elements is $0$, but the probability of a event consisting several elements may be positive. </li>
<li>Normally these models are solved by graphs. $P(A)=\displaystyle\frac{integral of A}{integral of sample space}$. If the experiment is linear, the probability is $P(A)=\displaystyle\frac{\int_Adt}{\int_Sdt}$. If the experiment is 2-dimension, the probability is $P(A)=\displaystyle\frac{\iint_Adt}{\iint_Sdt}$. </li>
</ol>
<h1 id="Paradoxes-in-Probability"><a href="#Paradoxes-in-Probability" class="headerlink" title="Paradoxes in Probability"></a>Paradoxes in Probability</h1><p><strong><code>In probability, different mathods may give different answers, but all of the answers are correct.</code></strong></p>
<p>The difference between answers is most affected by the sample space they choose. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiyunZhang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">165</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liyun Zhang</span>
</div>
<div class="BbeiAn-info">
       浙ICP备 -
    <a target="_blank" href="http://www.miitbeian.gov.cn/" style="color:#000000"  rel="nofollow">19047088号-1</a> <!--a标签中增加nofollow属性，避免爬虫出站。-->|
    <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001835" style="color:#000000;text-decoration:none;padding-left:30px;background:url(https://s1.ax1x.com/2018/09/29/ilmwIH.png) no-repeat left center" rel="nofollow">浙公网安备 33011802001835号</a>      <!--这里将图标作为了背景，以使得能和后面的文字在同一行-->

</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
