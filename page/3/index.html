<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://http://www.laughingtree.cn').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":-1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="LaughingTree">
<meta property="og:url" content="http://http//www.laughingtree.cn/page/3/index.html">
<meta property="og:site_name" content="LaughingTree">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://http//www.laughingtree.cn/page/3/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>LaughingTree</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LaughingTree</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">30</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/05/10/03-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/10/03-%E8%AF%8D%E6%B3%95%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">03. 词法分析</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-10 14:59:42" itemprop="dateCreated datePublished" datetime="2020-05-10T14:59:42+08:00">2020-05-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:49:40" itemprop="dateModified" datetime="2020-08-07T19:49:40+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-HDU-Stanford/" itemprop="url" rel="index">
                    <span itemprop="name">编译原理 (HDU / Stanford)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="词法分析程序设计"><a href="#词法分析程序设计" class="headerlink" title="词法分析程序设计"></a>词法分析程序设计</h1><h2 id="主要功能"><a href="#主要功能" class="headerlink" title="主要功能"></a>主要功能</h2><ol>
<li>读入源程序字符序列</li>
<li>对源程序预处理</li>
<li>识别源程序中的单词，创建符号表并在相应的符号表中登录信息</li>
<li>把单词和行号关联，以便编译器将错误信息与源程序位置联系起来</li>
<li>输出单词序列</li>
<li>组织：<br />&emsp;作为独立的一遍：源程序完整文件转换为单词符号序列文件<br />&emsp;与语法分析器合在一起作为一遍：词法分析是语法分析的子程序，语法分析调用词法分析获得单词符号二元式</li>
</ol>
<h2 id="单词"><a href="#单词" class="headerlink" title="单词"></a>单词</h2><ol>
<li>单词：具有独立含义的最小语法单位</li>
<li>分类：标识符 (变量名)、关键字、常数、运算符、分界符、字符串</li>
<li>词法分析程序根据词类划分源码，输出二元式$&lt;类别,单词属性值&gt;$ (称为单词符号)。一类一码可以把类别省略，但本质还是二元组</li>
<li>模式：一个单词的可能形式，定义类别为某一个类的单词的形式</li>
<li>词素：源程序的一个字符序列，是单词符号的实例。单词属性值不一定是词素。比如源码中的变量名就是一个词素，但是在二元组中，单词属性值可能是$1$</li>
</ol>
<h2 id="正规表达式"><a href="#正规表达式" class="headerlink" title="正规表达式"></a>正规表达式</h2><ol>
<li>正规表达式是表示字符串格式的模式，可以用来描述单词的结构</li>
<li>正规集：正规式$(r)$所匹配的所有字符串的集合，实际上是一个正规语言，记为$L(r)$</li>
<li>正则表达式定义：<br />&emsp;$\epsilon$是正规式，其匹配语言是$L(\epsilon)=\{\epsilon\}$<br />&emsp;如果$a\in\Sigma$，则$a$是一个正规式，其匹配语言是$L(a)=\{a\}$<br />&emsp;如果$r$和$s$都是正规式，则有：<br />&emsp;&emsp;$(r)|(s)$也是正规式，表示语言$L(r)\displaystyle\cup L(s)$<br />&emsp;&emsp;$(r)(s)$也是正规式，表示语言$L(r)\cdot L(s)$<br />&emsp;&emsp;$(r)^*$也是正规式，表示语言$(L(r))^*$<br />&emsp;&emsp;$(r)$也是正规式，表示语言$L(r)$</li>
<li>运算符号优先级：$*=+&gt;\cdot&gt;|$</li>
<li>或运算具有交换律和结合律，连接运算具有结合律和对或运算的分配律</li>
<li>$\epsilon$是连接运算的单位元</li>
</ol>
<h1 id="有限状态自动机"><a href="#有限状态自动机" class="headerlink" title="有限状态自动机"></a>有限状态自动机</h1><ol>
<li>状态：区分事物的标识</li>
<li>有限状态自动机是离散状态系统</li>
<li>定义：具有离散输入与离散输出的一种数学模型，用于识别输入的符号串是否属于某个语言的合法句子</li>
<li>输入为符号串，输出为是或者否</li>
<li>分类<br />&emsp;确定性有限自动机 (DFA)：输入符号定，下一动作定<br />&emsp;非确定性有限自动机 (NFA)：输入符号定，下一动作不定</li>
<li>组成：字母表 (输入符号)、状态 (开始状态、接受或终结状态)、转移函数</li>
<li>开始状态用一个箭头空指向该状态，也可以再加一个“start”</li>
<li>接受状态，也称为终结状态，表示该串是可识别串，用两个圆圈表示</li>
<li>转移函数有两个参数，当前状态和输入符号。输出下一状态</li>
<li>从开始状态到终结状态之间的输入符号按顺序组成的串就是可识别的串</li>
</ol>
<h2 id="确定性有限自动机"><a href="#确定性有限自动机" class="headerlink" title="确定性有限自动机"></a>确定性有限自动机</h2><ol>
<li>DFS表示为一个五元组$M=(Q,\Sigma,q_0,F,\delta)$<br />&emsp;$Q$表示有限状态集<br />&emsp;$\Sigma$表示字符表<br />&emsp;$q_0$表示初始状态，只能有一个$q_0\in Q$<br />&emsp;$F$表示接受或终止状态集，可以有多个元素$F\subseteq Q$<br />&emsp;$\delta$表示转移函数，$Q\times\Sigma\to Q$</li>
<li>表示方法：状态转换图和状态转换表</li>
<li>状态转换表中开始状态加$\to$，终结状态加$*$</li>
</ol>
<h2 id="非确定性有限自动机"><a href="#非确定性有限自动机" class="headerlink" title="非确定性有限自动机"></a>非确定性有限自动机</h2><ol>
<li>NFA也是由五元组表示，但是转移函数的值域不同。</li>
<li>NFA的转移函数也是一个当前状态+一个输入符号的定义域，但是值域不是一个状态，而是状态的集合。所以对$\delta$有$Q\times(\Sigma\cup\{\epsilon\})\to 2^Q$</li>
<li>在非确定性有限自动机中，输入符号可以是$\epsilon$，表示两个状态识别的串是同一个串</li>
<li>NFA的不确定性体现在两个方面<br />&emsp;第一种情况是识别一个符号的时候有两条路径可走<br />&emsp;第二种是$\epsilon$边的存在，使得同一个子串可以由两个不同的状态</li>
<li>DFA是NFA的一个特例，但两种自动机识别语言的能力是一样的，即如果一个语言可以被DFA识别，则一定存在一个NFA可以识别该语言，反之亦然</li>
<li>正规表达式直接转化成DFA比较难，一般是先转化为NFA，再把NFA转化为DFA</li>
</ol>
<h2 id="NFA到DFA的转换"><a href="#NFA到DFA的转换" class="headerlink" title="NFA到DFA的转换"></a>NFA到DFA的转换</h2><ol>
<li>将NFA中一个状态输入同一个符号的所有下一状态作为DFA中的一个集合，以消除第一种不确定性</li>
<li>$\epsilon$闭包：$t$是一个状态，则$\epsilon-closure(t)$是从状态$t$只经过$\epsilon$转换可以到达的状态集；$T$是一个状态集，$\epsilon-closure(T)$是从$T$中任一状态只经过$\epsilon$边就可以达到的状态集<br />$t$本身以及$T$中的所有状态都属于他们的$\epsilon$闭包<br />利用$\epsilon$闭包可以消除第二种不确定性</li>
<li>定义$move(t,a)$表示从状态$t$经过$a$边能到达的状态集；$move(T,a)$表示从状态集$T$中的状态经非空的$a$转换可达到的状态集</li>
<li>子集构造法<br />&emsp;DFA的开始状态是$\epsilon-closure(s_0)$，这就是第一个稳定的状态<br />&emsp;其余状态由$\epsilon-closure(move(T,a))$生成<br />&emsp;DFA的终态是包含原来终态的状态</li>
<li>子集构造算法流程<br />&emsp;先获得开始状态$\epsilon-closure(s_0)$<br />&emsp;对每一个状态，先找到它在每一个输入下的稳定次态，即，枚举$\epsilon-colosure(move(T,a))$，对每一个可能的$a$<br />&emsp;每次找到一个新的状态，就加入DFA的状态集中，并在未来也对这个新状态进行枚举</li>
</ol>
<h2 id="从正规式到NFA的转换"><a href="#从正规式到NFA的转换" class="headerlink" title="从正规式到NFA的转换"></a>从正规式到NFA的转换</h2><ol>
<li>直接从正规式的定义出发来定义正规式对应的NFA</li>
<li>对$r=\epsilon$<br /><img src="/img/03.词法分析01.png" width="30%"></li>
<li>对$r=a$<br /><img src="/img/03.词法分析02.png" width="30%"></li>
<li>对$r=s|t$<br /><img src="/img/03.词法分析03.png" width="30%"></li>
<li>对$r=st$，$s$和$t$有一个公用节点，表示$s$的接受状态和$t$的开始状态<br /><img src="/img/03.词法分析04.png" width="30%"></li>
<li>对$r=(s)^*$<br /><img src="/img/03.词法分析05.png" width="30%"></li>
<li>$r=(s)$的NFA和$r=s$的一样</li>
<li>得到的NFA的性质<br />&emsp;$N(r)$的状态数最多是$r$中符号和算符总数的两倍<br />&emsp;$N(r)$只有一个开始状态和一个接受状态，接受状态没有向外的转换<br />&emsp;$N(r)$的每个状态有一个不是$\epsilon$边的指向其他状态的边，或者最多只有两条$\epsilon$边</li>
</ol>
<h2 id="DFA的化简"><a href="#DFA的化简" class="headerlink" title="DFA的化简"></a>DFA的化简</h2><ol>
<li>任何DFA或NFA都存在唯一一个最简DFA与之等价，且任何DFA或NFA都可以转换为该最简DFA</li>
<li>等价状态<br />&emsp;一致性：两个状态必须同为接受状态或非接受状态<br />&emsp;蔓延性：对所有输入符号，两个状态转移到等价状态</li>
<li>寻找等价状态：隐含表法 (求同法)<br />&emsp;横少尾，纵少头的阶梯表<br />&emsp;可以直接确定等价的打钩，直接确定不等价的画叉<br />&emsp;取决于次态的写上次态，第一轮比完后再一一比较次态，直至全部判断完毕<br />&emsp;如果遇到相互依赖，循环不定的情况，默认他们都等价</li>
<li>寻找等价状态：求异法<br />&emsp;先将状态划分为接受状态和非接受状态，再逐步划分精细化，最后得到不可在细分的划分<br />&emsp;考察每一个划分，要求其中的每一个元素在所有输入的情况下的次态组成的集合是该划分的子集<br />&emsp;如果有元素存在某一个输入对应的次态不是原划分的元素，就把该元素从原划分中剥离出去，直到没有元素应该被剥离</li>
<li>在找到了等价状态之后，重新命名各个等效状态集，并把转移函数的输入中的状态改为原输入状态所属的等效状态，把输出状态改为原输出状态所属的等效状态</li>
</ol>
<h2 id="有限自动机到正规文法"><a href="#有限自动机到正规文法" class="headerlink" title="有限自动机到正规文法"></a>有限自动机到正规文法</h2><ol>
<li>任意一个FA识别的语言都能用正规文法来生成</li>
<li>每一个状态代表一个非终结符号</li>
<li>如果状态$i$是一个接受状态，则设规则$A_i\to\epsilon$</li>
<li>如果状态$i$是一个开始状态，则$A_i$是开始符号</li>
<li>其他情况，如下图，则设规则$A_i\to aA_j$<br /><img src="/img/03.词法分析06.png" width="30%"></li>
</ol>
<h1 id="词法分析器的自动构造"><a href="#词法分析器的自动构造" class="headerlink" title="词法分析器的自动构造"></a>词法分析器的自动构造</h1><ol>
<li>先用正规式表示出单词的结构，再由正规式生成NFA，最后再化简得到最简DFA</li>
<li>给每个正规式构造相应的NFA，将所有的NFA用或运算并联起来</li>
<li>当存在多个匹配的时候，可以选择最长匹配，或规定优先顺序</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/05/07/02-%E5%BD%A2%E5%BC%8F%E6%96%87%E6%B3%95%E4%B8%8E%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/07/02-%E5%BD%A2%E5%BC%8F%E6%96%87%E6%B3%95%E4%B8%8E%E5%BD%A2%E5%BC%8F%E8%AF%AD%E8%A8%80/" class="post-title-link" itemprop="url">02. 形式文法与形式语言</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-07 22:17:22" itemprop="dateCreated datePublished" datetime="2020-05-07T22:17:22+08:00">2020-05-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:49:59" itemprop="dateModified" datetime="2020-08-07T19:49:59+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-HDU-Stanford/" itemprop="url" rel="index">
                    <span itemprop="name">编译原理 (HDU / Stanford)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="符号串和语言"><a href="#符号串和语言" class="headerlink" title="符号串和语言"></a>符号串和语言</h1><h2 id="符号串"><a href="#符号串" class="headerlink" title="符号串"></a>符号串</h2><ol>
<li>符号：可以相互区分的记号或元素</li>
<li>字母表$\Sigma$：符号的有限集合</li>
<li>符号串：字母表上的符号的有穷序列</li>
<li>空符号串$\epsilon$是不包含任何符号的符号串，是任何$\Sigma$上的符号串，相当于符号串这个群的单位元</li>
<li>连接：符号串$\alpha$、$\beta$的连接，是把$\beta$的符号写在$\alpha$的符号之后得到的符号串$\alpha\beta$。$\epsilon\alpha=\alpha\epsilon=\alpha$。不满足交换律，是偏序关系</li>
<li>幂：符号串$\alpha$自身连接$n$次得到的符号串，$\alpha^n$。$\alpha^0=\epsilon$，$\alpha^1=\alpha$</li>
<li>符号串的长度表示符号串中包含符号的个数。$|\epsilon|=0$</li>
<li>子串：符号串中的连续子序列</li>
<li>前缀：符号串中从第一位开始的子序列<br />后缀：符号串中以最后一位结尾的子序列<br />真前/后缀：与原串不同的前/后缀</li>
<li>符号串集合<br />并集：<script type="math/tex">A+B=\{w|w\in A\ or\ w\in B\}</script><br />连接：<script type="math/tex">A\cdot B=\{xy|x\in A\ and\ y\in B\}</script><br />幂次：$A^0=\{\epsilon\}$，$A^n=A^{n-1}A$<br />$Kleene$闭包 (星闭包)：$\Sigma^* $表示由字母表中的符号组成的符号串集合。<br />正闭包：$\Sigma^+=\Sigma^*-\{\epsilon\}$，即长度大于$1$的符号串的集合</li>
</ol>
<h2 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h2><ol>
<li>定义：某个字母表上的符号串的集合</li>
<li>任何一个语言都是$\Sigma^*$的一个子集</li>
<li>描述方法<br />&emsp;有穷：直接列出<br />&emsp;无穷：给出生成方式 (文法)或识别方式 (自动机)</li>
</ol>
<h1 id="文法"><a href="#文法" class="headerlink" title="文法"></a>文法</h1><p>主要用到的是上下文无关文法和正规文法 (正则文法)</p>
<h2 id="文法-1"><a href="#文法-1" class="headerlink" title="文法"></a>文法</h2><ol>
<li>文法$G$是一个四元组$G=(V_T,V_N,S,P)$<br />&emsp;$V_T$是终结符号集，即最终出现在句子中，在字母表中的单词符号<br />&emsp;$V_N$是非终结符号集，即推导过程中出现，但不在字母表中，不会在最后的句子中出现的符号<br />&emsp;$S$是开始符号，$S\in V_N$，表示最大的语法单位<br />&emsp;$P$是产生式规则集合。$P=\{\alpha\to\beta|\alpha至少包含一个非终结符号\}$</li>
<li>从文法生成串：从开始符号$S$开始，利用规则$P$重写所有非终结符号，直到每一个符号都是终结符号为止</li>
<li>$P\to\alpha_1,P\to\alpha_2,…,P\to\alpha_n$缩写为$P\to\alpha_1|\alpha_2|…|\alpha_n$</li>
<li>只有四个元素都完全一样的两个文法才是相同的文法，否则即使产生的语言一样也不相同</li>
<li>直接推导：用一条规则把左部的一个非终结符号直接替换为一个终结符号或另一个非终结符号<br />推导：$0$步或多步直接推导，$\alpha\Rightarrow…\Rightarrow\beta$等价于$\alpha\Rightarrow^*\beta$<br />用$\Rightarrow^+$表示至少$1$步的推导</li>
<li>最左推导：每一步的推导都施加在句型的最左边的非终结符号上，记为$\Rightarrow_{lm}$，生成的句型称为左句型<br />最右推导：施加在最右边的非终结符号上，记为$\Rightarrow_{rm}$，生成的句型称为右句型。也称为规范推导</li>
<li>归约：用产生式的左部替代右部的过程，是识别串的方式，是推导的逆过程。如果$A\to\gamma$是产生式规则，记为$\alpha\gamma\beta\Leftarrow\alpha A\beta$<br />最左归约：每一步归约都施加在句型最左边的可归约串上，规范归约，是最右推导的逆过程 (因此最右推导是规范推导)<br />最右归约：施加在最右边的可归约串上，是最左推导的逆过程</li>
<li>文法$G[S]:…$中，$G$是文法的名字，$S$是开始符号，后面跟的是产生式规则集，产生式规则集中左部都是非终结符号，其他的都是终结符号</li>
<li>乔姆斯基文法的四种类型 ($G=(V_T,V_N,S,P),\alpha\to\beta\in P$)<br />&emsp;0型，短语结构文法 (PSG)：$|\alpha|≠0$，识别系统是图灵机<br />&emsp;1型，上下文有关文法 (CSG)：$|\alpha|≤|\beta|$，识别系统是线性有界自动机<br />&emsp;2型，上下文无关文法 (CFG)：$|\alpha|=1,\alpha\in V_N$，识别系统是下推自动机<br />&emsp;3型，正规文法 (RG)：右部最多一个非终结符号且在所有可选项中非终结符号都在终结符号的同一侧，在左边的是左线性文法，在右边的是右线性文法。识别系统是有限自动机</li>
</ol>
<h2 id="语言-1"><a href="#语言-1" class="headerlink" title="语言"></a>语言</h2><ol>
<li>句型：若存在$S\Rightarrow^*\alpha$，则$\alpha$是一个句型。必须从开始符号出发推导<br />句子：$\alpha$是一个句型，且$\alpha$只包含终结符号</li>
<li>文法产生的语言$L(G)=\{\alpha|\alpha是G的句子\}$。即语言是文法$G$推导出的所有句子组成的集合。$L(G)=\{\alpha|S\Rightarrow^*\alpha且\alpha\in V_T^*\}$</li>
<li>文法产生的语言只有一个，但是一个语言可以由多种文法产生。如果两个文法生成同一个语言，则这两个文法等价</li>
<li>产生式集合、终结符号、非终结符号有限，但是在有递归的情况下可以导出无穷多个句子</li>
</ol>
<h2 id="分析树"><a href="#分析树" class="headerlink" title="分析树"></a>分析树</h2><ol>
<li>一个节点一个符号，用于展示推导过程</li>
<li>根结点是开始符号</li>
<li>叶子节点按中序遍历中的相对顺序从左到右构成这一棵推导树的结果句型</li>
<li>内部结点都是非终结符号。但是非终结符号也可能出现在叶子节点，这棵推导树的结果是句型不是句子</li>
<li>每一个结点及其所有子节点对应一条产生式规则，其中的父节点是左部，子节点从左到右排列构成右部</li>
<li>分析树忽略了推导过程中非终结符号的替换顺序，所以一棵树可以对应多个推导。但一棵树只能对应一个最左推导 (上到下，左到右)，以及一个最右推导 (上到下，右到左)</li>
<li>二义：一个文法的句子有不止一棵的分析树<br />又最左(右)推导与分析树一一对应，故句子二义说明有不止一个最左(右)推导</li>
<li>如果一个文法包含二义性句子，则这个文法是二义性的<br />产生某上下文无关语言的每一个文法都是二义的，则语言是先天二义的</li>
<li>文法的无二义性是不可判定的，因为不可能穷举文法的所有句子。只能判断有二义性</li>
<li>如果产生式集中一个非终结符号既存在左递归，又存在右递归，那该文法就是二义的</li>
<li>程序设计语言中二义文法的解决方法：<br />&emsp;改写为等价无二义文法<br />&emsp;设定优先顺序，限制可选分析树</li>
</ol>
<h2 id="句型分析"><a href="#句型分析" class="headerlink" title="句型分析"></a>句型分析</h2><ol>
<li>短语：$\alpha\beta\delta$是文法$G[S]$的一个句型，若存在推导$S\Rightarrow^*\alpha A\delta$且$A\Rightarrow^+\beta$，则称$\beta$是句型$\alpha\beta\delta$相对于非终结符号$A$的短语。<br />即，句型中一个可以独立由某个非终结符号生成的子串就是句型相对于非终结符号的短语<br />短语也是分析树中以非终结符号为子树根节点的最右叶子节点序列</li>
<li>直接短语：一步推导就能得到的短语，在分析树中，子树只有两层，短语是非终结符号的子节点</li>
<li>句柄：最左边的直接短语</li>
<li>素短语：至少包含一个终结符，不含更小带终结符的短语的短语</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/05/06/GTD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/06/GTD/" class="post-title-link" itemprop="url">GTD</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-06 23:37:37" itemprop="dateCreated datePublished" datetime="2020-05-06T23:37:37+08:00">2020-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:45:24" itemprop="dateModified" datetime="2020-08-07T19:45:24+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9C%8B%E8%BF%87%E7%9A%84%E4%B9%A6/" itemprop="url" rel="index">
                    <span itemprop="name">看过的书</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/img/GTD.png" alt=""></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/05/06/%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/06/%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E5%AD%A6%E4%B9%A0/" class="post-title-link" itemprop="url">如何高效学习</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-05-06 23:34:56" itemprop="dateCreated datePublished" datetime="2020-05-06T23:34:56+08:00">2020-05-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:44:15" itemprop="dateModified" datetime="2020-08-07T19:44:15+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%9C%8B%E8%BF%87%E7%9A%84%E4%B9%A6/" itemprop="url" rel="index">
                    <span itemprop="name">看过的书</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><img src="/img/如何高效学习.png"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/04/26/01-%E7%BC%96%E8%AF%91%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/26/01-%E7%BC%96%E8%AF%91%E6%A6%82%E8%BF%B0/" class="post-title-link" itemprop="url">01. 编译概述</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-26 21:56:45" itemprop="dateCreated datePublished" datetime="2020-04-26T21:56:45+08:00">2020-04-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:49:52" itemprop="dateModified" datetime="2020-08-07T19:49:52+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86-HDU-Stanford/" itemprop="url" rel="index">
                    <span itemprop="name">编译原理 (HDU / Stanford)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="语言处理系统"><a href="#语言处理系统" class="headerlink" title="语言处理系统"></a>语言处理系统</h1><ol>
<li>编译程序就是为机器翻译程序语言 (描述计算的符号)的软件系统。一个编译程序就是一种翻译系统，把源语言翻译成目标语言的等价程序</li>
<li><strong>源程序</strong>先经过<code>预处理</code>变为<strong>修改后的源程序</strong><br /><strong>修改后的源程序</strong>经过<code>编译器</code>变为<strong>汇编代码</strong><br /><strong>汇编代码</strong>经过<code>汇编器</code>变为<strong>可重定位的机器代码</strong><br /><strong>可重定位的机器代码</strong>最后经过<code>连接器</code>变为<strong>目标机器代码</strong></li>
<li>源程序经过编译器编译，若没有错误，就得到目标程序；若有错误就得到错误信息。输入进入目标程序再得到输出</li>
<li>编译程序将源程序全部翻译成机器语言程序后，再执行机器语言程序。一次编译可以多次执行<br />解释程序将源程序的一条语句翻译成机器语言程序，并立即执行，再接着翻译。解释程序可交互，易于调试，可移植性好，编译一次执行一次，但是效率低</li>
<li>Java先经Java编译器编译得到一个字节码，再在JVM上解释</li>
</ol>
<h1 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h1><ol>
<li>编译步骤<br />&emsp;词法分析：输入源程序，输出单词符号序列 (二元组序列)<br />&emsp;语法分析：输入单词符号序列，输出语法分析树<br />&emsp;语义分析：输入语法分析树，输出带语义的树<br />&emsp;中间代码生成：输入带语义的树，输出中间代码<br />&emsp;代码优化：输入原始中间代码，输出优化后的中间代码<br />&emsp;目标代码生成：输入优化后的中间代码，输出目标程序代码</li>
<li>词法分析把字符序列切分为有意义的单词。词法分析器的主要功能就是识别单词<br />识别关键字、标识符、数字等，并对每一个单词给出其类型，生成单词符号，表示为$&lt;单词词类, 单词值&gt;$。如果一个词类里只有一个单词，则可以省略单词词类<br />空格只是作为分隔符，忽略。过滤注释部分代码<br />每一个单词词类都有一个符号表，单词值指向符号表中的某一项</li>
<li>语法分析分析表达式、语句等。语法分析器的任务就是分析单词符号序列是如何构成语法单位的<br />语法树体现了单词符号序列的语法结，节点是单词符号或语法单元。<br />语法树的节点通常是操作符，而其子节点则是操作数</li>
<li>语义分析分析表达式或语句的操作内容。收集语义信息，类型检查和类型转换。验证语法结构合法的程序是否存在语义错误</li>
<li>中间代码称为三地址代码，每一条指令只执行一个操作。中间代码是一种面向抽象机器的代码结构，易于产生且易于翻译成目标语言</li>
<li>前端主要是分析工作，包括：词法分析、语法分析、语义分析、中间代码产生，以及部分代码优化工作，相关的错误处理和符号表的建立。前端依赖于源程序，并在很大程度上独立于目标机器</li>
<li>后端主要是综合工作，包括：代码优化、代码生成和相关错误处理。依赖于目标机器，因为涉及到指令选择</li>
<li>前端可以移植在不同的系统中，后端可以在不同语言中重用。若有$m$种语言，要在$n$个不同的平台中运行，需要的编译器就从原始的$O(nm)$个编译器变为$O(n+m)$个模块</li>
<li>把所有东西读一次并处理就是一遍，一次编译可能不止一遍。生成一次中间代码或目标代码必定是一遍</li>
<li>构造编译程序的工具：编译程序-编译程序、编译程序产生器、翻译程序书写系统<br />扫描器生成器：产生词法分析器<br />语法分析生成器<br />语法制导翻译引擎</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/04/26/03-%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA%E5%92%8C%E6%A3%80%E6%9F%A5%E7%BA%A0%E9%94%99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/26/03-%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA%E5%92%8C%E6%A3%80%E6%9F%A5%E7%BA%A0%E9%94%99/" class="post-title-link" itemprop="url">03. 数据表示和检查纠错</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-26 15:50:45" itemprop="dateCreated datePublished" datetime="2020-04-26T15:50:45+08:00">2020-04-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:52:28" itemprop="dateModified" datetime="2020-08-07T19:52:28+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6/" itemprop="url" rel="index">
                    <span itemprop="name">计算机组成原理 (清华大学)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据表示"><a href="#数据表示" class="headerlink" title="数据表示"></a>数据表示</h1><p>1. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/04/25/02-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/25/02-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%9A%84%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">02. 计算机的指令系统</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-25 16:11:16" itemprop="dateCreated datePublished" datetime="2020-04-25T16:11:16+08:00">2020-04-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:52:15" itemprop="dateModified" datetime="2020-08-07T19:52:15+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6/" itemprop="url" rel="index">
                    <span itemprop="name">计算机组成原理 (清华大学)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="计算机程序"><a href="#计算机程序" class="headerlink" title="计算机程序"></a>计算机程序</h1><ol>
<li>计算机程序是程序员和计算机硬件之间交互的语言</li>
<li>分类：高级语言程序、汇编语言程序、机器语言程序</li>
<li>高级语言 (算法语言)为了让程序员着重注意在解决实际问题所用的算法，更贴近自然语言，让程序设计更为高效。且高级语言通用性更好，可移植性强</li>
<li>计算机硬件是由逻辑门组成的，而逻辑门是由信号驱动的，信号以高低电平区分。所以计算机硬件只能理解$01$二进制串</li>
<li>机器语言是计算机硬件能直接识别和运行的指令的集合，是二进制码组成的指令，用机器语言设计程序基本不可行，可读性几乎为零</li>
<li>汇编语言用英文单词或其缩写替代二进制的指令代码，更容易记忆和理解。存储单元由汇编程序分配而非程序员，达到基本可用标准</li>
<li>高级语言设计出来的程序，需要经过编译程序或解释程序，才能在计算机的硬件系统上予以执行。汇编程序要经过汇编器翻译成机器语言后方可运行</li>
</ol>
<h1 id="计算机指令"><a href="#计算机指令" class="headerlink" title="计算机指令"></a>计算机指令</h1><p>冯诺依曼计算机<br />&emsp;存储程序计算机：程序由指令构成，程序功能通过指令序列描述，指令在内存中顺序存放<br />&emsp;顺序执行指令：用PC指示当前被执行的指令，从存储器中读出指令执行，PC指向下一条指令(程序指定的下一条指令，若没指定，默认为物理地址上的下一条)</p>
<h2 id="指令是什么"><a href="#指令是什么" class="headerlink" title="指令是什么"></a>指令是什么</h2><h3 id="指令"><a href="#指令" class="headerlink" title="指令"></a>指令</h3><ol>
<li>指令用于程序设计人员告知计算机执行一个最基本运算、处理功能。程序由一个序列的计算机指令组成，程序的最小单元是指令</li>
<li>指令是指挥计算机硬件运行的命令，是计算机硬件执行程序的最小单位</li>
<li>指令是由多个二进制位组成的位串，是计算机硬件可以直接识别和执行的信息体</li>
<li>指令中应指明指令所完成的操作，并明确操作对象</li>
</ol>
<h3 id="指令系统"><a href="#指令系统" class="headerlink" title="指令系统"></a>指令系统</h3><ol>
<li>指令系统：一台计算机提供的全部指令</li>
<li>指令系统层处在硬件系统和软件系统之间，是硬、软件间的接口部分<br />硬件系统：实现指令，解决指令间衔接关系。包括数字逻辑层和微体系结构层<br />软件系统：按一定规则组织起来的指令，完成一定的功能。包括操作系统层、汇编语言层、高级语言层</li>
<li>指令系统的优劣势一个计算机系统是否成功的关键因素</li>
</ol>
<h2 id="指令的分类"><a href="#指令的分类" class="headerlink" title="指令的分类"></a>指令的分类</h2><ol>
<li>数据运算指令：算数运算、逻辑运算</li>
<li>数据传输指令：内存到寄存器、寄存器到寄存器</li>
<li>输入输出指令：与输入输出端口的数据传输，这是一种特殊的数据传输指令</li>
<li>控制指令：无条件跳转、条件跳转、子程序的支持 (调用和返回)</li>
<li>其他指令：停机、开/关中断、空操作、特权指令、设置条件码</li>
</ol>
<h2 id="指令格式"><a href="#指令格式" class="headerlink" title="指令格式"></a>指令格式</h2><ol>
<li>指令格式指的是操作码、操作数地址的二进制分配方案</li>
<li>操作码：指令的操作功能，每条指令都有一个确定的操作码</li>
<li>操作数地址：操作数存放的地址，或操作数本身</li>
<li>指令字：完整的一条指令的二进制表示</li>
<li>指令字长：指令中二进制的代码的位数<br />机器字长：计算机能够直接处理的二进制数据的位数<br />指令字长为一个字节的倍数</li>
<li>根据指令字长是否固定，指令还可以分为定长指令字结构和变长指令字结构 (现在少用)<br />根据操作码长度是否固定，分为定长操作码和扩展操作码</li>
</ol>
<h2 id="寻址方式"><a href="#寻址方式" class="headerlink" title="寻址方式"></a>寻址方式</h2><ol>
<li>寻址方式 (编址方式)：确定本条指令的操作数地址和下一条要执行的指令的地址的方法</li>
<li>寻址方式的数目和功能依不同的计算机系统而定。不同的寻址方式实现的复杂程度和运行性能各不同</li>
<li>通常指令中每一个操作数都有一个地址字段表示其来源或去向的地址</li>
<li>形式地址：在指令中给出的操作数或指令的地址<br />实际地址：依据形式地址及一定的规则得到的一个数值</li>
<li>操作数地址字段可能要指出：<br />&emsp;运算器中累加器的编号或专用寄存器名称 (编号)<br />&emsp;输入/输出指令中用到的I/O设备的入出端口地址<br />&emsp;内存储器的一个存储单元 (或一个I/O设备)的地址</li>
</ol>
<h2 id="MIPS指令系统"><a href="#MIPS指令系统" class="headerlink" title="MIPS指令系统"></a>MIPS指令系统</h2><ol>
<li>MIPS：Microprocessor without interlocked piped stages 无内部互锁流水级的微处理器</li>
<li>为RISC芯片设计的指令系统</li>
<li>MIPS32所有的指令都是32位字长，有3种指令格式：寄存器型，立即数型，和转移型<br />MIPS64是面向54位处理器的指令系统</li>
<li>操作数寻址：基址加16位偏移的访存寻址、立即数寻址和寄存器寻址</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/04/24/01-%E6%A6%82%E8%BF%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/24/01-%E6%A6%82%E8%BF%B0/" class="post-title-link" itemprop="url">01. 概述</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-24 22:35:12" itemprop="dateCreated datePublished" datetime="2020-04-24T22:35:12+08:00">2020-04-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:52:18" itemprop="dateModified" datetime="2020-08-07T19:52:18+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-%E6%B8%85%E5%8D%8E%E5%A4%A7%E5%AD%A6/" itemprop="url" rel="index">
                    <span itemprop="name">计算机组成原理 (清华大学)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li>计算机是什么：<br />&emsp;一种高速运行的电子设备<br />&emsp;用于进行数据的算术或逻辑运算<br />&emsp;可接受输入信息<br />&emsp;根据用户要求对信息进行加工<br />&emsp;输出结果</li>
<li>冯诺依曼机<br />&emsp;存储程序<br />&emsp;用二进制编码数据<br />&emsp;体系结构：控制器、运算器、存储器、输入、输出设备<br />&emsp;以运算器 (logic unit，LU)为核心</li>
<li>现代计算机<br />&emsp;功能部件：CPU (数据通道、控制器)、存储器、输入输出设备<br />&emsp;VLSI<br />&emsp;体系结构：流水技术 (PIPELINE)、层次存储系统、并行</li>
<li>计算机运行机制<br />&emsp;数据通道：完成算术和逻辑运算，通常包括其中的寄存器<br />&emsp;控制器：CPU的组成成分，它根据程序指令来指挥数据通道、存储器和输入输出设备运行，共同完成程序功能<br />&emsp;存储器：存放运行时程序及其所需要的数据的场所<br />&emsp;输入设备：信息进入计算机的设备<br />&emsp;输出设备：讲计算机结果展示给用户的设备</li>
<li>计算机的层次结构<br />&emsp;可运行高级语言的计算机 = 可运行汇编语言的计算机 + 编译器<br />&emsp;可运行汇编语言的计算机 = 可运行机器语言的计算机 + 汇编器<br />&emsp;可运行机器语言的计算机可以生成控制信号，由硬件实现</li>
<li>计算机系统<br />&emsp;硬件：中央处理器、存储器、外围设备<br />&emsp;软件：为了使用计算机而编写的各种系统的和用户的程序</li>
<li>评价计算机性能的指标<br />&emsp;吞吐率：单位时间内完成的任务数量，即完成的指令条数<br />&emsp;响应时间：完成任务的时间<br />&emsp;衡量性能的指标<br />&emsp;&emsp;MIPS (吞吐率 Million Instructions per Second，注意与指令系统区分)<br />&emsp;&emsp;CPI (每条指令完成的平均周期数，相对于周期长度)<br />&emsp;&emsp;CPU时间 (绝对时间)、CPU时钟 (绝对时间)<br />&emsp;综合测试程序</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/04/09/07-CNN-Architectures/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/09/07-CNN-Architectures/" class="post-title-link" itemprop="url">07. CNN Architectures</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-09 19:47:07" itemprop="dateCreated datePublished" datetime="2020-04-09T19:47:07+08:00">2020-04-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:47:19" itemprop="dateModified" datetime="2020-08-07T19:47:19+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Stanford-CS231n/" itemprop="url" rel="index">
                    <span itemprop="name">计算机视觉 (Stanford CS231n)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="General"><a href="#General" class="headerlink" title="General"></a>General</h1><ol>
<li>When we say how many layers a network has, we mean how many layers in that network has weight. Namely, the sum of fully-connected layers and convolution layers. </li>
<li>The rule of naming a layer: <br />&emsp;If it is a fully-connected layer, its name will begin with “FC”. Behind “FC” is a number that stands for the number of neurons in this layer. Namely, the dimension of the output vector. <br />&emsp;If it is a convolution layer, its name begins with the size of the filter. In the middle is “conv”. At the end is the number of filters. <br />&emsp;Pooling layers write “Pool”. </li>
</ol>
<h1 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h1><ol>
<li>Published at 2012, AlexNet has many features: <br />&emsp;It is the first network using ReLU. <br />&emsp;It uses norm layers (not common now). <br />&emsp;It has heavy data augmentation. <br />&emsp;It uses the dropout with the probability of $0.5$. <br />&emsp;Its batch size is $128$, SGD momentum is $0.9$. <br />&emsp;The learning rate is $1e-2$, reduced by $10$ manually when validation accuracy plateaus. <br />&emsp;$L2$ weight decay is $5e-4$. <br />&emsp;$7$ models ensemble to create about $3%$ better accuracy. </li>
<li>Architecture: <br /><img src="/img/07.CNNArchitectures01.png" width="10%"></li>
<li>AlexNet was trained on <script type="math/tex">GTX\ 580</script> GPU with only $3$ GB of memory. However, the output of its first convolution layer is $55\times55\times96$. So the network has to spread across $2$ GPUs, half the neurons (feature maps) on each GPU. </li>
<li>$CONV1$, $CONV2$, $CONV4$, $CONV5$ in AlexNet only have connections with feature maps on the same GPU. $CONV3$, $FC6$, $FC7$, $FC8$ have connections with all feature maps in the preceding layer; They communicate across GPUs. </li>
<li>ZFNect is AlexNet, except that $CONV1$ changed to $7\times7$ with stride $2$, and $CONV3, 4, 5$ uses $512, 1024, 512$ filters. </li>
</ol>
<h1 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h1><ol>
<li>The main feature of VGG is small filters and deeper networks. </li>
<li>VGG has many architectures with $16\sim19$ layers. The structure of VGG can be divided into blocks. So in VGG, we can name the convolution layers like $conv-x-y$, where $x$ is the number of blocks and $y$ is its position in that block. </li>
<li>All three fully-connected layers of VGG are in the last as AlexNet. </li>
<li>VGG only uses small filters of $3\times3,CONV$ stride $1$, pad $1$, and <script type="math/tex">2\times2,MAX\ POOL</script> stride $2$. Stack of three $3\times3 conv$ (stride $1$) layers has the same effective receptive field as one $7\times7 conv$ layer. However, a stack of three $3\times3$ convolution layer is deeper, more nonlinearities and fewer parameters than one $7\times7$ layer and <br />The size of the receptive field is the size of pixels that affect one pixel of the last layer. In this case, $3\times3$ pixels affect one pixel of the first convolution layer. Since the stride is $1$, we pad the receptive field of last layer with $1$, which makes the receptive field of the second convolution layer $5\times5$. Finally, the receptive field of the last layer is $7\times7$. </li>
<li>Most memory is spent on early convolution layers while most parameters are in late fully-connected layers. </li>
</ol>
<h1 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h1><ol>
<li>GoogLeNet replaced fully-connected layer with average pooling layer. Nevertheless, in order to finetune hyperparameters, it still added one fully-connected layer at the end. </li>
<li>GoogLeNet first used the inception module. The naive version of inception module is as followed. <br /><img src="/img/07.CNNArchitectures02.png" width="30%"><br />It applied parallel filter operations on the input from the previous layer. With padding to make the output of four layers in the same size, the output of this inception module merely stacks them up. <br />However, its computational complexity is too high, not to mention that the pooling layer preserves feature depth, which means total depth after concatenation can only grow at every layer. </li>
<li>Hence, we use the bottleneck to reduce feature depth as followed. <br /><img src="/img/07.CNNArchitectures03.png" width="30%"><br />It uses less $1\times1$ filters then the depth of the input to reduce the complexity of calculation. </li>
<li>GoogLeNet stacks inception modules with dimension reduce on top of each other. <br /><img src="/img/07.CNNArchitectures04.png" width='30%'><br />At the beginning is the stem network and at the end is the classifier output. <br />Nevertheless, it added some auxiliary classification outputs to inject additional gradient at lower layers. </li>
</ol>
<h1 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h1><ol>
<li>ResNet is so deep that it has $152$ layers. </li>
<li>The problem is that when continue stacking deeper layers on a “plain” convolutional neural network, the performance will become even worse. </li>
<li>The team assumed that the problem is an <em>optimization</em> problem; deeper models are harder to optimize. </li>
<li>Their solution is to use network layers to fit a residual mapping instead of directly trying to fit a desired underlying mapping. Namely, learn a function $H(x)$ to be the output to the next layer. <br /><img src="/img/07.CNNArchitectures05.png" width="10%"></li>
<li>They also assumed that learning a $H(x) = F(x)+x$ is much easier than learning a completely unknown $H(x)$. <br /><img src="/img/07.CNNArchitectures06.png" width="20%"></li>
<li>ResNet stacks residual blocks together, and every residual block has two $3\times3$ convolutional layers. <br />It also periodically double the number of filters and downsample spatially using stride $2$. <br />The ResNet begins with an additional convolutional layer and ends with a fully-connected layer. <br /></li>
<li>For deeper networks, they also use the bottleneck layer to improve efficiency as GoogLeNet. </li>
<li>To train a ResNet in practice: <br />&emsp;Add a BN layer after each convolutional layer. <br />&emsp;Use $Xavier/2$ initialization. <br />&emsp;Some hyperparameters: <br />&emsp;&emsp;Momentum: $0.9$<br />&emsp;&emsp;Learning rate: $0.1$, divided by $10$ when validation error plateaus. <br />&emsp;&emsp;Mini-batch size: 256<br />&emsp;&emsp;Weight decay: $1e-5$<br />&emsp;No dropout used. </li>
</ol>
<h1 id="Complexity"><a href="#Complexity" class="headerlink" title="Complexity"></a>Complexity</h1><ol>
<li>AlexNet: small compute, still memory heavy, lower accuracy</li>
<li>VGG: highest memory, most operations</li>
<li>GoogLeNet: most efficient</li>
<li>ResNet: Moderate efficiency depending on model, highest accuracy</li>
<li>ResNet, with inception, has the highest accuracy. </li>
</ol>
<h1 id="Other-Architectures"><a href="#Other-Architectures" class="headerlink" title="Other Architectures"></a>Other Architectures</h1><ol>
<li>NiN (Network in Network): <br />&emsp;Multiple convolutional layers with “micro-network” within each convolutional layer to compute more abstract features for local patches. <br />&emsp;The micro-network uses multilayer perceptron, like fully-connected layer. <br />&emsp;This is the precursor to GoogLeNet and ResNet bottleneck layers. </li>
<li>Identity Mappings in Deep Residual Networks: Creates a more direct path for propagating information throughout the network (moves activation to residual mapping pathway)</li>
<li>Wide residual network: <br />&emsp;Residuals are the critical factor, not depth. <br />&emsp;Use wider residual blocks ($F\times k$ filters instead of $F$ filters in each layer)<br />&emsp;$50$-layer wide ResNet outperforms $152$-layer original ResNet. <br />&emsp;Increasing width instead of depth is more computationally efficient. The computation can be easily parallelized. </li>
<li>ResNeXt: increase the width of theresidual block through multiple parallel pathways. <br /><img src="/img/07.CNNArchitectures07.png" width="30%"></li>
<li>Stochastic depth: <br />&emsp;Motivation: reduce vanishing gradients and training time through short networks during training. <br />&emsp;Randomly drop a subset of layers during each training pass. <br />&emsp;Bypass with identity function. <br />&emsp;Use the full network at test time. </li>
<li>FractalNet: <br />&emsp;The key is transitioning efficiently from shallow to deep and residual representation is not necessary. <br />&emsp;It has both shallow and deep paths to output. <br />&emsp;It is trained with dropout, and test with full network. <br />&emsp;<img src="/img/07.CNNArchitectures08.png" width="50%"></li>
<li>Densely Connected Convolutional Networks: <br />&emsp;In dense block, each layer is connected to every other layer in a feedforward fashion. <br />&emsp;<img src="/img/07.CNNArchitectures09.png" width="30%"><br />&emsp;It alleviates vanishing gradients, strengths feature propagation, encourages feature reuse. <br />&emsp;<img src="/img/07.CNNArchitectures10.png" width="15%"></li>
<li>SqueezeNet: <br />&emsp;It has AlexNet-level accuracy with $50$ times fewer parameters and less than $0.5$ Mb model size. <br />&emsp;Fire modules consisting of a ‘squeeze’ layer with $1\times1$ filters feeding an ‘expand’ layer with $1\times1$ and $3\times3$ filters. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2020/04/08/06-Deep-Learning-Software/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/08/06-Deep-Learning-Software/" class="post-title-link" itemprop="url">06. Deep Learning Software</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-04-08 14:23:27" itemprop="dateCreated datePublished" datetime="2020-04-08T14:23:27+08:00">2020-04-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-07 19:48:02" itemprop="dateModified" datetime="2020-08-07T19:48:02+08:00">2020-08-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89-Stanford-CS231n/" itemprop="url" rel="index">
                    <span itemprop="name">计算机视觉 (Stanford CS231n)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CPU-and-GPU"><a href="#CPU-and-GPU" class="headerlink" title="CPU and GPU"></a>CPU and GPU</h1><ol>
<li>GPU has way more cores than CPU. Most CUP has no more than $10$ cores, while the GPU can contain thousands of cores. </li>
<li>Nevertheless, the clock speed of each core of CPU is a lot faster than the GPU. </li>
<li>Furthermore, the CPU does not have its own memory. CPU has to share the memory with the system. However, GPU can have a large amount of memory of its own. </li>
<li>So GPU is better when doing the tedious parallel tasks, like the multiplication of matrices. CPU is better at sequential tasks. </li>
<li>When it comes to machine learning, the GPU can be about $70$ times faster than CPU, and CUDA can be $3$ times faster than none-CUDAs. </li>
<li>When training, to synchronize the speed of GPU and reading data, we have three solutions: <br />&emsp;Read all data into RAM. <br />&emsp;Use SSD instead of HDD. <br />&emsp;Use multiple CPU threads to prefetch data. </li>
</ol>
<h1 id="Deep-Learning-Software"><a href="#Deep-Learning-Software" class="headerlink" title="Deep Learning Software"></a>Deep Learning Software</h1><ol>
<li>The networks build with NumPy can only run on the CPU, and it is hard to compute the gradients. </li>
<li>The point of deep learning frameworks: <br />&emsp;Quickly build big computational graphs. <br />&emsp;Efficiently compute gradients in computational graphs. <br />&emsp;Run it all efficiently on GPU. </li>
</ol>
<h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><ol>
<li>The main structure of TensorFlow is to define a computational graph first without doing any calculation. Then run the graph over and over. </li>
<li>If we want to run the code on GPU, define the graph under the tf.device. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.device(<span class="string">'/gpu:0'</span>):</span><br></pre></td></tr></table></figure>
Alternately, if we want to run on CUP, change the “gpu” to “cpu”. </li>
<li>A placeholder is an array ran on CPU while a Variable is another kind of array ran on GPU. Usually, we declare the input and output as placeholders and weights as Variables. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = tf.placeholder(tf.float32, shape = (N, D))</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.random_normal(D, H))</span><br></pre></td></tr></table></figure></li>
<li>Then we define the the process of forward pass with some functions. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.maximum(matrix, number) <span class="comment">#a matrix of which each entry is the maximum of corresponding entry and the number</span></span><br><span class="line">tf.matmul(x, w) <span class="comment"># a matrix represents the product of the matrices</span></span><br><span class="line">tf.reduce_mean(matrix) <span class="comment"># a number represents the mean of all entries of that matrix</span></span><br><span class="line">tf.reduce_sum(matrix, axis) <span class="comment"># a vector whose each entry is the sum of that matrix along that axis.</span></span><br></pre></td></tr></table></figure></li>
<li>The loss can be defined automatically. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.losses.mean_squared_error(y_pred, y)</span><br></pre></td></tr></table></figure></li>
<li>The gradients can be calculated automatically. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.gradients(loss, [variables]) <span class="comment"># multiple gradients of loss with respect to each variables</span></span><br></pre></td></tr></table></figure></li>
<li>If we defined the weights as Variables, then we need to update weights here. Otherwise, we do it in the session. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_w = w.assign(w - learning_rate * grad_w)</span><br></pre></td></tr></table></figure></li>
<li>There is no computation until here, only building the graph. </li>
<li>With the graph done, we enter the session so we can actually run the graph. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br></pre></td></tr></table></figure></li>
<li>In the session, we can initialize the placeholders in value. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">values = &#123;placeholder_name: np.random.randn(N, D), …&#125;</span><br></pre></td></tr></table></figure></li>
<li>If we defined the weights as Variables, we need to run graph once to initialize the weights. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure></li>
<li>After all those things, we can now enter the for-loop to run many times to train. In each iteration, we run the graph once. The loss in the parameter is defined above in the graph. <br />&emsp;If weights are defined as Variables, we need to group the gradients before entering the session and put the group in the parameter. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before session</span></span><br><span class="line">updates = tf.group(new_w, …)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in for-loop</span></span><br><span class="line">loss_val, _ = sess.run([loss, updates], feed_dict = value)</span><br></pre></td></tr></table></figure>
&emsp;If weights are defined as Placeholder, we need to update the weights here. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_val, grad_w_val, … = sess.run([loss, grad_w, …], feed_dict = values)</span><br><span class="line">values[w] -= learning_rate * grad_w_val</span><br></pre></td></tr></table></figure></li>
<li>If we are using optimizer, <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before session</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(<span class="number">1e-5</span>)</span><br><span class="line">updates = optimizer.minimize(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in for-loop</span></span><br><span class="line">loss_val, _ = sess.run([loss, updates], feed_dict = values)</span><br></pre></td></tr></table></figure></li>
<li>We can also use the predefined layers, which automatically set up weight and bias for us. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># before session</span></span><br><span class="line">init = tf.contrib.layers.xavier_initializer() <span class="comment"># use Xavier initializer</span></span><br><span class="line">h = tf.layers.dense(inputs = x, </span><br><span class="line">                    units = H, </span><br><span class="line">                    activation = tf.nn.relu, </span><br><span class="line">                    kernel_initializer = init)</span><br><span class="line">y_pred = tf.layers.dense(inputs = h,</span><br><span class="line">                         units = D,</span><br><span class="line">                         kernel_initializer = init)</span><br></pre></td></tr></table></figure></li>
<li>Furthermore, we can use Keras. <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define model</span></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line">model.add(keras.layers.core.Dense(input_dim = D, output_dim = H))</span><br><span class="line">model.add(keras.layers.core.Activation(<span class="string">'relu'</span>))</span><br><span class="line">model.add(keras.layers.core.Dense(input_dim = H, output_dim = D))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define optimizer</span></span><br><span class="line">optimizer = keras.optimizers.SGD(lr = <span class="number">1e0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Build the model, specify loss function</span></span><br><span class="line">model.compile(loss = <span class="string">'mean_squared_error'</span>, optimizer = optimizer)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train model</span></span><br><span class="line">history = model.fit(x, y, nb_epoch = <span class="number">50</span>, batch_size = N, verbose = <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h2><ol>
<li>Three levels of abstraction: <br />&emsp;Tensor: Imperative array, but runs on GPU. This is almost a Numpy array. <br />&emsp;Variable: Node in a computational graph; stores data and gradient. This is the Tensor, Variable, Placeholder on TensorFlow. <br />&emsp;Module: A neural network layer; may store state or learnable weights. </li>
<li><p>To run on GPU, cast tensors to a CUDA datatype. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dtype = torch.cuda.FloatTensor</span><br></pre></td></tr></table></figure>
</li>
<li><p>A Pytorch Variable is a node in a computational graph. All Variables have two essential properties. Data is a Tensor while grad is a Variable of gradients with the same shape as data. Naturally, grad.data is a Tensor of gradients. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = torch.autograd.Variable(torch.randn(N, D_in), requires_grad = <span class="literal">False</span> / <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Pytorch Tensors and Variables have the same API. Variables remember how they were created for backpropagation. </p>
</li>
<li><p>After defined all matrices, we enter the for-loop to train the network directly. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h = x.mm(w1) <span class="comment"># matrices multiplication</span></span><br><span class="line">h_relu = h.clamp(h, min = <span class="number">0</span>) <span class="comment"># squash h into [min, max]</span></span><br><span class="line">loss = (y_pred - y).pow(<span class="number">2</span>).sum() <span class="comment"># the sum of the square of (y_pred - y)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>Compute the gradient of the loss with respect to weights. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> w.grad:</span><br><span class="line">  w.grad.data.zero_() <span class="comment"># zero out grad first</span></span><br><span class="line">loss.backward()</span><br></pre></td></tr></table></figure>
</li>
<li><p>Update the weights. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w.data -= learning_rate * w.grad.data</span><br></pre></td></tr></table></figure>
</li>
<li><p>We can define our own autograd functions by writing forward and backward for Tensors. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyLayer</span><span class="params">(torch.autograd.Function)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    self.save_for_backward(x)</span><br><span class="line">    …</span><br><span class="line">    <span class="keyword">return</span> resultMatrix</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(self, grad_y)</span>:</span></span><br><span class="line">    x, = self.saved_tensors</span><br><span class="line">    grad_input = grad_y.clone()</span><br><span class="line">    …</span><br><span class="line">    <span class="keyword">return</span> gradMatrix</span><br><span class="line">  </span><br><span class="line"> <span class="comment"># in forward process in the for-loop</span></span><br><span class="line">myLayer = MyLayer()</span><br><span class="line">y_pred = … <span class="comment"># some operations involve myLayer</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>The predefined layers and loss functions are stored in <code>nn</code>. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Define model</span></span><br><span class="line">model = torch.nn.Sequential(</span><br><span class="line">  torch.nn.Linear(D_in, H), </span><br><span class="line">  torch.nn.ReLU(), </span><br><span class="line">  torch.nn.Linear(H, D_out))</span><br><span class="line">loss_fn = torch.nn.MSELoss(size_average = <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train model</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">500</span>):</span><br><span class="line">  <span class="comment"># Feed data</span></span><br><span class="line">  y_pred = model(x)</span><br><span class="line">  loss = loss_fn(y_pred, y)</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># Backward</span></span><br><span class="line">  model.zero_grad()</span><br><span class="line">  loss.backward()</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    param.data -= learning_rate * param.grad.data</span><br></pre></td></tr></table></figure>
</li>
<li><p>There also exist predefined optimizers. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Before training</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters, lr = learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># In training</span></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>
</li>
<li><p>We can also define our own Models. Models can contain weight as variables or other Modules. We only need to implement the initialization and forward function. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModule</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, D_in, H, D_out)</span>:</span></span><br><span class="line">    super(MyModule, self).__init__()</span><br><span class="line">    self.linear1 = torch.nn.Linear(D_in, H)</span><br><span class="line">    self.linear2 = torch.nn.Linear(H, D_out)</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    …</span><br><span class="line">    <span class="keyword">return</span> resultMatrix</span><br><span class="line">  </span><br><span class="line"><span class="comment"># Define model, then all is the same</span></span><br><span class="line">model = MyModule(D_in, H, D_out)</span><br></pre></td></tr></table></figure>
</li>
<li><p>A DataLoader wraps a Dataset and provides mini-batching, shuffling, multithreading. When custom data is needed, write a Dataset class. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> TensorDataset, DataLoader</span><br><span class="line">loader = DataLoader(TensorDataset(x, y), batch_size = <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">  <span class="keyword">for</span> x_batch, y_batch <span class="keyword">in</span> loader:</span><br><span class="line">    y_pred = model(x_batch)</span><br><span class="line">    loss = loss_fn(y_pred, y_batch)</span><br></pre></td></tr></table></figure>
</li>
<li><p>PyTorch has some pre-trained models which can be used immediately. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">alexnet = torchvision.models.alexnet(pretrained = <span class="literal">True</span>)</span><br><span class="line">vgg16 = torchvision.models.vgg<span class="number">.16</span>(pretrained = <span class="literal">True</span>)</span><br><span class="line">resnet101 = torchvision.models.resnet101(pretrained = <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h2><ol>
<li>Tensorflow is static graphs, while PyTorch is dynamic graphs. </li>
<li>A static graph is built at the beginning, and the graph is never changed while a dynamic graph is new each iteration. </li>
<li>With static graphs, the framework can optimize the graph before it runs. Furthermore, when one graph is built, we can serialize the static graph and run it without the code that built that graph. We can even change other language to run it. </li>
<li>However, to build dynamic graphs  has less code to write. Moreover, conditional and loops can be easily written with dynamic graphs. </li>
<li>Dynamic graphs are usually used on recurrent networks, recursive networks and modular networks. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/2/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><a class="page-number" href="/page/2/">2</a><span class="page-number current">3</span><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" href="/page/4/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiyunZhang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">181</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">30</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liyun Zhang</span>
</div>
<div class="BbeiAn-info">
       浙ICP备 -
    <a target="_blank" href="http://www.miitbeian.gov.cn/" style="color:#000000"  rel="nofollow">19047088号-1</a> <!--a标签中增加nofollow属性，避免爬虫出站。-->|
    <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001835" style="color:#000000;text-decoration:none;padding-left:30px;background:url(https://s1.ax1x.com/2018/09/29/ilmwIH.png) no-repeat left center" rel="nofollow">浙公网安备 33011802001835号</a>      <!--这里将图标作为了背景，以使得能和后面的文字在同一行-->

</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
