<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://http://www.laughingtree.cn').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="LaughingTree">
<meta property="og:url" content="http://http//www.laughingtree.cn/page/2/index.html">
<meta property="og:site_name" content="LaughingTree">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://http//www.laughingtree.cn/page/2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>LaughingTree</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LaughingTree</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">29</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">33</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/06/19/08-Snooping-based-Cache-Coherence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/19/08-Snooping-based-Cache-Coherence/" class="post-title-link" itemprop="url">08. Snooping-based Cache Coherence</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-19 19:30:41" itemprop="dateCreated datePublished" datetime="2022-06-19T19:30:41+08:00">2022-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-20 16:46:00" itemprop="dateModified" datetime="2022-06-20T16:46:00+08:00">2022-06-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="The-cache-coherence-problem"><a href="#The-cache-coherence-problem" class="headerlink" title="The cache coherence problem"></a>The cache coherence problem</h1><ol>
<li><p>This problem happens in a shared memory multi-processor system. Reading a value at address X should return the last value written to address X by any processor. </p>
</li>
<li><p>This is a problem created by replicating the data stored at address X in local caches (a hardware implementation detail), and cannot be fixed by adding locks. </p>
</li>
<li><p>Memory coherence problem exists because there is both global storage (main memory) and per-processor local storage (processor caches) implementing the abstraction of a single shared address space.</p>
</li>
<li><p>In a cache hierarchy,<br>L1 and L2 caches are private per core while L3 cahce is shared by cores in the same chip.<br>L3 cache is split into sectors or banks. Each bank is physically associated with a core, but managed hardware-wise as a single coherent unit.<br>L2 cache and L3 cache communicate through a ring interconnect where most inter-processor actions happens. </p>
</li>
</ol>
<h2 id="Uniprocessor-case"><a href="#Uniprocessor-case" class="headerlink" title="Uniprocessor case"></a>Uniprocessor case</h2><ol>
<li><p>On a uniprocessor, providing coherence is fairly simple, since writes typically come from one client: the processor. Load operation must examine all pending stores in store buffer and select the last sequence. </p>
</li>
<li><p>There is one exception on a uniprocessor, which is device I/O via direct memory access (DMA). </p>
</li>
<li><p>One solution to DMA is that CPU writes to shared buffers using uncached stores.<br>Another way is supported by OS which will mark virtual memory pages containing shared buffers as not-cachable, and explicitly flush pages from cache when I/O completes. </p>
</li>
<li><p>In practice, DMA transfers are infrequent compared to CPU loads and stores (so these heavyweight software solutions are acceptable)</p>
</li>
</ol>
<h2 id="Coherence-definition"><a href="#Coherence-definition" class="headerlink" title="Coherence definition"></a>Coherence definition</h2><ol>
<li><p>Obeys program order as expected of a uniprocessor system: A read by processor P to address X that follows a write by P to address X, should return the value of the write by P (assuming no other processor wrote to X in between)</p>
</li>
<li><p>Write propagation: A read by processor P1 to address X that follows a write by processor P2 to X returns the written value, if the read and write are “sufficiently separated” in time (assuming no other write to X occurs in between)</p>
</li>
<li><p>Write serialization: Writes to the same address are serialized: two writes to address X by any two processors are observed in the same order by all processors.</p>
</li>
<li><p>Write propagation means that notification of a write must eventually get to the other processors. Note that precisely when information about the write is propagated is not specified in the definition of coherence.</p>
</li>
</ol>
<h1 id="Implementing-choherence"><a href="#Implementing-choherence" class="headerlink" title="Implementing choherence"></a>Implementing choherence</h1><ol>
<li><p>Software-based solution: OS uses page-fault mechanism to propagate writes. It can be used to implement memory coherence over clusters of workstations</p>
</li>
<li><p>Hardware-based solutions: “snooping”-based coherence implementations and directory-based coherence implementations</p>
</li>
<li><p>Most modern multi-core CPUs implement cache coherence<br>Discrete GPUs do not implement cache coherence. Overhead of coherence deemed not worth it for graphics and scientific computing applications (NVIDIA GPUs provide single shared L2 + atomic memory operations)<br>But the latest Intel Integrated GPUs do implement cache coherence</p>
</li>
</ol>
<h2 id="Shared-caches"><a href="#Shared-caches" class="headerlink" title="Shared caches"></a>Shared caches</h2><ol>
<li><p>One single cache shared by all processors eliminates problem of replicating state in multiple caches and makes coherence easy. </p>
</li>
<li><p>This has obvious scalability problems since the point of a cache is to be local and fast. It also causes interference and contention due to many clients. </p>
</li>
<li><p>Facilitates fine-grained sharing (overlapping working sets). Loads/stores by one processor might pre-fetch lines for another processor</p>
</li>
</ol>
<h2 id="Snooping-cache-coherence-schemes"><a href="#Snooping-cache-coherence-schemes" class="headerlink" title="Snooping cache-coherence schemes"></a>Snooping cache-coherence schemes</h2><ol>
<li><p>Main idea: all coherence-related activity is broadcast to all processors</p>
</li>
<li><p>Cache controllers monitor (“they snoop”) memory operations, and react accordingly to maintain memory coherence</p>
</li>
<li><p>Cache controller must respond to actions from both ends:<br>It must respond the Load/Store requests from its local processor<br>It also must respond coherence-related activity broadcast over the chip’s interconnect. </p>
</li>
<li><p>The interconnect is between memory and caches possessed by each processor. There is not only memory-cache information, but also cache-cache information, which will limit the scality of the system. </p>
</li>
</ol>
<h3 id="Write-through-caches"><a href="#Write-through-caches" class="headerlink" title="Write-through caches"></a>Write-through caches</h3><ol>
<li><p>For the invalidation-based protocol, when one processor write into an address, cache controller broadcasts<br>invalidation message for other caches to mark that line to invalidation.<br>the next read from other processors will trigger cache miss</p>
</li>
<li><p>For the update-based protocol: other caches will update their local copies as the information is sent. </p>
</li>
<li><p>States: Valid (V) or Invalid (I)<br>A local processor read (PrRd) always ends at valid. If the operation starts from an invalid state, a message will be sent (BusRd). If it starts from a valid state, no message will be sent.<br>A local processor write (PrWr) always ends at the same state as before the operation (assumes write no-allocate policy), and always sends a message (BusWr).<br>When a write message from other processor is received (BusWr), It always ends at invalid state.  </p>
<p><img src="/img/08-Snooping-based-Cache-Coherence-1.jpeg" width="40%"></p>
</li>
<li><p>Requirements of the interconnect:<br>All write transactions visible to all cache controllers.<br>All write transactions visible to all cache controllers in the same order. </p>
</li>
<li><p>Simplifying assumptions here:<br>Interconnect and memory transactions are atomic<br>Processor waits until previous memory operations is complete before issuing next memory operation<br>Invalidation applied immediately as part of receiving invalidation broadcast</p>
</li>
</ol>
<h1 id="Write-back-caches-Invalidation-based"><a href="#Write-back-caches-Invalidation-based" class="headerlink" title="Write-back caches (Invalidation-based)"></a>Write-back caches (Invalidation-based)</h1><ol>
<li><p>Dirty state of cache line now indicates exclusive ownership</p>
</li>
<li><p>Exclusive: cache is only cache with a valid copy of line (it can safely be written to)<br>Owner: cache is responsible for supplying the line to other processors when they attempt to load it from memory (otherwise a load from another processor will get stale data from memory)</p>
</li>
<li><p>A line in the “exclusive” state can be modified without notifying<br>the other caches<br>Processor can only write to lines in the exclusive state. So they need a way to tell other caches that they want exclusive access to the line. They will do this by sending all the other caches messages<br>When cache controller snoops a request for exclusive access to line it contains, it must invalidate the line in its own cache</p>
</li>
</ol>
<h2 id="MSI-write-back-invalidation-protocol"><a href="#MSI-write-back-invalidation-protocol" class="headerlink" title="MSI write-back invalidation protocol"></a>MSI write-back invalidation protocol</h2><ol>
<li><p>Three cache line states:<br>Invalid (I): same as meaning of invalid in uniprocessor cache<br>Shared (S): line valid in one or more caches<br>Modified (M): line valid in exactly one cache (a.k.a. “dirty” or “exclusive” state)</p>
</li>
<li><p>The local processors have the same operations as write-through case.<br>The coherence-related bus transactions from remote caches have three kinds:<br>BusRd: obtain copy of line with no intent to modify<br>BusRdX: obtain copy of line with intent to modify<br>flush: write dirty line out to memory</p>
<p><img src="/img/08-Snooping-based-Cache-Coherence-2.png" width="50%"></p>
</li>
<li><p>When try to write an invalid line without reading it, the content of the current modified state line will be sent to the new writer. </p>
</li>
<li><p>Write propagation is achieved via combination of invalidation on BusRdX, and flush from M-state on subsequent BusRd/BusRdX from another processors</p>
</li>
<li><p>Write serialization<br>Writes that appear on interconnect are ordered by the order they appear on interconnect (BusRdX)<br>Reads that appear on interconnect are ordered by order they appear on interconnect (BusRd)<br>Writes that don’t appear on the interconnect (PrWr to line already in M state):</p>
<ul>
<li>Sequence of writes to line comes between two interconnect transactions for the line</li>
<li>All writes in sequence performed by same processor, P (that processor certainly observes them in correct sequential order)</li>
<li>All other processors observe notification of these writes only after a interconnect transaction for the line. </li>
<li>So all processors see writes in the same order. </li>
</ul>
</li>
</ol>
<h2 id="MESI-invalidation-protocol"><a href="#MESI-invalidation-protocol" class="headerlink" title="MESI invalidation protocol"></a>MESI invalidation protocol</h2><ol>
<li><p>MSI requires two interconnect transactions for the common case of reading an address, then writing to it</p>
<ul>
<li>Transaction 1: BusRd to move from I to S state</li>
<li>Transaction 2: BusRdX to move from S to M state</li>
</ul>
</li>
<li><p>Solution: add additional state E (“exclusive clean”) to mark the line that has not been modified, but only this cache has a copy of the line<br>This state decouples exclusivity from line ownership (line not dirty, so copy in memory is valid copy of data)<br>Upgrade from E to M does not require an interconnect transaction </p>
</li>
<li><p><img src="/img/08-Snooping-based-Cache-Coherence-3.jpeg" width="50%"></p>
</li>
</ol>
<h2 id="5-stage-invalidation-based-protocol"><a href="#5-stage-invalidation-based-protocol" class="headerlink" title="5-stage invalidation-based protocol"></a>5-stage invalidation-based protocol</h2><ol>
<li>Who should supply data on a cache miss when line is in the E or S state of another cache?<br>Can get cache line data from memory or can get data from another cache? If source is another cache, which one should provide it?</li>
<li>Cache-to-cache transfers add complexity, but commonly used to reduce both latency of data access and reduce memory bandwidth required by application</li>
<li>MESIF: Like MESI, but one cache holds shared line in F state rather than S (F=”forward”). Cache with line in F state services miss<br>Simplifies decision of which cache should service miss (basic MESI: all caches respond)<br>Used by Intel processors</li>
<li>MOESI: Transition from M to O (O=”owned, but not exclusive”) and do not flush to memory (In MESI protocol, transition from M to S requires flush to memory).<br>Other processors maintain shared line in S state, one processor maintains line in O state. Data in memory is stale, so cache with line in O state must service cache misses.<br>Used in AMD Opteron</li>
</ol>
<h1 id="Invalidation-based-vs-Update-based"><a href="#Invalidation-based-vs-Update-based" class="headerlink" title="Invalidation-based vs. Update-based"></a>Invalidation-based vs. Update-based</h1><ol>
<li><p>Invalidation-based protocol: To write to a line, cache must obtain exclusive access to it. All other caches must invalidate their copies</p>
</li>
<li><p>Update-based protocol: Can write to shared copy by broadcasting update to all other copies</p>
</li>
<li><p>Intuitively, update would seem preferable if other processors<br>sharing data continue to access it after a write occurs</p>
<p>But updates are overhead if data just sits in caches (and is never read by another processor again) or application performs many writes before the next read</p>
</li>
<li><p>Update can reduce cache miss rate since all shared copies remain valid.<br>Update can suffer from high traffic due to multiple writes before the next read by another processor</p>
</li>
</ol>
<h1 id="Snoop-for-a-cache-hierarchy"><a href="#Snoop-for-a-cache-hierarchy" class="headerlink" title="Snoop for a cache hierarchy"></a>Snoop for a cache hierarchy</h1><ol>
<li><p>Challenge: changes made to data at L1 cache may not be visible to L2 cache controller than snoops the interconnect. </p>
</li>
<li><p>All lines in closer to processor cache are also in farther from processor cache. Thus, all transactions relevant to L1 are also relevant to L2, so it is sufficient for only the L2 to snoop the interconnect. </p>
</li>
<li><p>If line is in owned state (M in MSI/MESI) in L1, it must also be in owned state in L2. Allows L2 to determine if a bus transaction is requesting a modified cache line in L1 without requiring information from L1</p>
</li>
<li><p>When line X is invalidated in L2 cache due to BusRdX from another cache. Must also invalidate line X in L1<br>Solution: Each L2 line contains an additional state bit indicating if line also exists in L1. This bit tells the L2 invalidations of the cache line due to coherence traffic need to be propagated to L1. </p>
</li>
<li><p>When L1 write is hit, the corresponding line in L2 cache is in modified state in the coherence protocol, but L2 data is stale.<br>When coherence protocol requires X to be flushed from L2, L2 cache must request the data from L1.<br>Add another bit for “modified-but-stale” (flushing a “modified-but-stale” L2 line requires getting the real data from L1 first.)</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/06/17/02-Convolution/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/17/02-Convolution/" class="post-title-link" itemprop="url">02. Convolution</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-17 13:25:48" itemprop="dateCreated datePublished" datetime="2022-06-17T13:25:48+08:00">2022-06-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-19 18:02:45" itemprop="dateModified" datetime="2022-06-19T18:02:45+08:00">2022-06-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F-MIT-6-007/" itemprop="url" rel="index">
                    <span itemprop="name">信号与系统 (MIT 6.007)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li><p>We try to decompose input signal into a linear combination of basic signals that will provide analytical convenience.<br>Normally the analytical convenience means that the output can be easily generated. </p>
</li>
<li><p>Decompose a signal into a linear combination of delayed impulses. And that leads to a representation for linear time-invairant systems, which is referred to as convolution.<br>Another kind is a decomposition of inputs into complex exponentials. That leads to a representation of signals and systems through Fourier analysis. </p>
</li>
<li><p>For a sequence $x[n]$, we can decompose $x[n]=\displaystyle\sum^{+\infty}_{k=-\infty}x[k]\delta[n-k] $. Namely, we can represent $x[n]$ as linear combination of delayed impulse signals $\delta[n-k]$ with coefficients been $x[k]$.<br>In a linear system, we denote $h_k[n] $ as the output when the input is $\delta[n-k]$,  we can have the output when the input is $x[n]$ as $y[n]=\displaystyle\sum^{+\infty}_{k=-\infty}x[k]h_k[n] $.<br>In addition, if the system is time-invariant, $h_k[n]=\delta(n-k)=\delta(n-k-0)=h_0[n-k] $. Define $h_0[n]=h[n]$ to get $y[n]=\displaystyle\sum^{+\infty}_{k=-\infty}x[k]h[n-k] $ in the form of convolution sum denoted by $y[n]=x[n]\ast h[n] $. </p>
</li>
<li><p>In continuous case, we want to represent $x(t)$ with successive rectangles and see each rectangle as an impulse. As the width of each rectangle goes narrower and narrower, the representation becomes more accurate.<br>For a rectangle in $(t_0,t_0+\Delta)$, the height of the rectangle depends on $x(t_0)$. We can represent the the impulse as $x(t_0)\delta_\Delta(t-t_0)\Delta$ where $\delta_\Delta(t)$ is defined as before.<br>Now, $x(t)$ is the sum of all impulses, $x(t)\cong\displaystyle\sum^{+\infty}_{k=-\infty}x(k\Delta)\delta_\Delta(t-k\Delta)\Delta $.  $x(t)=\displaystyle\lim_{\Delta\to0}\sum^{+\infty}_{k=-\infty}x(k\Delta)\delta_\Delta(t-k\Delta)\Delta=\int^{+\infty}_{-\infty}x(\tau)\delta(t-\tau)d\tau $. </p>
</li>
<li><p>In $x(t)=\displaystyle\lim_{\Delta\to0}\sum^{+\infty}_{k=-\infty}x(k\Delta)\delta_\Delta(t-k\Delta)\Delta $, we can see it as the linear combination of $\delta_\Delta(t-k\Delta)$ with coefficient been $x(k\Delta)\Delta $.<br>In a linear system, if the output of $\delta_\Delta(t-k\Delta)$ is $h_{k\Delta}(t)$, we can know that the output $x(t)$ is $y(t)=\displaystyle\lim_{\Delta\to0}\sum^{+\infty}_{k=-\infty}x(k\Delta)h_{k\Delta}(t)\Delta=\int^{+\infty}_{-\infty}x(\tau)h_\tau(t)d\tau $.<br>In addition, if the system is time-invariant, $h_\tau(t)=h_0(t-\tau)$, and $y(t)=\displaystyle\int^{+\infty}_{-\infty}x(\tau)h(t-\tau)d\tau $ in the form of convolution integral denoted by $y(t)=x(t)\ast h(t)$. </p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/06/12/01-Signals-and-Systems/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/12/01-Signals-and-Systems/" class="post-title-link" itemprop="url">01. Signals and Systems</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-12 16:53:56" itemprop="dateCreated datePublished" datetime="2022-06-12T16:53:56+08:00">2022-06-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-20 22:54:46" itemprop="dateModified" datetime="2022-06-20T22:54:46+08:00">2022-06-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F-MIT-6-007/" itemprop="url" rel="index">
                    <span itemprop="name">信号与系统 (MIT 6.007)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Sinusoidal-signals"><a href="#Sinusoidal-signals" class="headerlink" title="Sinusoidal signals"></a>Sinusoidal signals</h1><h2 id="Continuous-time"><a href="#Continuous-time" class="headerlink" title="Continuous-time"></a>Continuous-time</h2><ol>
<li><p>$x(t)=A cos(\omega_0t+\phi)$. Here $A$ stands for amplitude, $\omega_0$ stands for frequency, and $\phi$ stands for phase. </p>
</li>
<li><p>A sinusoidal signal has period $T_0=\frac{2\pi}{\omega_0}$. </p>
</li>
<li><p>Time shift corresponds to a phase change, while phase change, likewise, corresponds to time shift.<br>For a time shift $\Delta t$, we can always have a phase change $\Delta\phi$, so that $x(t+\Delta t) = Acos(\omega_0(t+\Delta t)+\phi)=Acos(\omega_0t+\omega_0\Delta t+\phi)\Rightarrow\Delta\phi=\omega_0\Delta t$, and $\Delta t=\frac{\Delta\phi}{\omega_0}$</p>
</li>
<li><p>When $\phi=0$, $x(t)$ is a cosine signal, which is even. Namely, $x(t)=x(-t)$,<br>When $\phi=-\frac{\pi}{2}$, $x(t)=Acos(\omega_0t-\frac{\pi}{2})=Asin(\omega_0t)=Acos(\omega(t-\frac{T_0}{4})$ is a sinusoid signal, which is odd. Namely $x(t)=-x(-t)$. </p>
</li>
</ol>
<h2 id="Discrete-time"><a href="#Discrete-time" class="headerlink" title="Discrete-time"></a>Discrete-time</h2><ol>
<li><p>$x[n]=A cos(\Omega_0n+\phi)$. Here $A$ stands for amplitude, $\Omega_0$ stands for frequency, and $\phi$ stands for phase. $n$ only takes the integer values. </p>
</li>
<li><p>A time shift corresponds to a phase change. $x[t+\Delta t]=Acos(\Omega_0(t+\Delta t))=Acos(\Omega_0t+\Omega_0\Delta t)$. Hence $\Delta\phi=\Omega_0\Delta t$<br>However, not any phase change can be interpreted as a simple time shift in the sequence. A constraint in discrete-time is that $t+\Delta t$ must be an integer, but $\Delta t=\frac{\Delta\phi}{\Omega_0}$ cannot guarantee that. </p>
</li>
<li><p>If we want $x[n]$ to be periodic, $x[n]=x[n+N]\Rightarrow Acos(\Omega_0n+\phi)=Acos(\Omega_0n+\Omega_0N+\phi)\Rightarrow \Omega_0N=2\pi m$ with $m$ being an integer.<br>So when there exists an $m$ to make $\frac{2\pi m}{\Omega_0}$ an integer, $x[n]$ is periodic, and  the period $N_0=\frac{2\pi m}{\Omega_0}$.  Namely $\frac{2\pi}{\Omega_0}$ must be rational. </p>
</li>
<li><p>$Acos((\Omega_0+\Delta\Omega)n+\phi)=Acos((\Omega_0n+\Delta\Omega n+\phi))$. So for $\Delta\Omega=2\pi k$, $x[n]=x’[n]$. Thus, in discrete-time case, this class of signals is identical signals for values of $\Omega_0$ separated by $2\pi$.<br>In continuous-time signal, all signals with different $\Omega_0$ have distinct values. </p>
</li>
</ol>
<h1 id="Exponentials"><a href="#Exponentials" class="headerlink" title="Exponentials"></a>Exponentials</h1><h2 id="Real-exponentials"><a href="#Real-exponentials" class="headerlink" title="Real exponentials"></a>Real exponentials</h2><ol>
<li><p>In the continuous-time case, $x(t)=Ce^{at}$ with $C$ and $a$ both being real numbers. When $a&gt;0$, $x(t)$ is geomatically growing. When $a&lt;0$, $x(t)$ is decaying.<br>A time shift corresponds to a scale change, vise versa. $x(t+\Delta t)=Ce^{a(t+\Delta t)}=Ce^{a\Delta t}e^{at}$</p>
</li>
<li><p>In the discrete-time case, $x[n]=Ce^{\beta n}=C\alpha^n$, with $C,\alpha$ being real number. </p>
</li>
<li><p>When $\alpha&gt;1$, $x[n]$ is growing. When $0&lt;\alpha&lt;1$, $x[n]$ is decaying.<br>When $\alpha&lt;0$, $\beta$ is an imaginary number, and the sequence is going to alternate between positive and negative. </p>
</li>
</ol>
<h2 id="Complex-exponentials"><a href="#Complex-exponentials" class="headerlink" title="Complex exponentials"></a>Complex exponentials</h2><ol>
<li><p>In continuous-time case, $C$ and $a$ are complex numbers. We write $C$ in polar form $C=|C|e^{j\theta}$ and $a$ in rectangular form $a=r+j\omega_0$.<br>$x(t)=|C|e^{j\theta}e^{(r+j\omega_0)t}=|C|e^{rt}e^{j(\omega_0t+\theta)}=|C|e^{rt}cos(\omega_0t+\theta)+j|C|e^{rt}sin(\omega_0t+\theta)$</p>
</li>
<li><p>In discrete-time case, we write $C$ and $\alpha$ both in polar form $C=|C|e^{j\theta},\alpha=|\alpha|e^{j\Omega_0}$.<br>$x[n]=|C|e^{j\theta}(|\alpha|e^{j\Omega_0})^n=|C||\alpha|^ne^{j(\Omega_0n+\theta)}=|C||\alpha|^ncos(\Omega_0n+\theta)+j|C||\alpha|^nsin(\Omega_0n+\theta)$</p>
</li>
<li><p>When $r=0$, $x(t)$ is always periodic. When $\alpha=1$, whether $x[n]$ is periodic depends on $\Omega_0$. </p>
</li>
</ol>
<h1 id="Unit-step-and-unit-impuls-signals"><a href="#Unit-step-and-unit-impuls-signals" class="headerlink" title="Unit step and unit impuls signals"></a>Unit step and unit impuls signals</h1><h2 id="Discrete-time-1"><a href="#Discrete-time-1" class="headerlink" title="Discrete-time"></a>Discrete-time</h2><ol>
<li><p>For unit step sequence, $u[n]=\left\{ \begin{matrix}0 &amp; n&lt;0\\1 &amp; n≥0  \end{matrix} \right.$</p>
</li>
<li><p>For unit impulse sequence, $\delta[n]=\left\{\begin{matrix}0 &amp; n≠0\\1 &amp; n=0 \end{matrix}\right.$</p>
</li>
<li><p>We can express the unit impulse with unit step. $\delta[n]=u[n]-u[n-1] $. Namely, the unit impulse equals to the unit step minus unit step delayed, which is the first difference. </p>
</li>
<li><p>We can also express the unit step with unit impulse. $u[n]=\displaystyle\sum^n_{m=-\infty}\delta[m] $ or $u[n]=\displaystyle\sum^\infty_{k=0}\delta[n-k] $</p>
</li>
</ol>
<h2 id="Continuous-time-1"><a href="#Continuous-time-1" class="headerlink" title="Continuous-time"></a>Continuous-time</h2><ol>
<li><p>For unit step function, $u(t)=\left\{\begin{matrix}0 &amp; t &lt; 0\\1 &amp; t &gt; 0 \end{matrix} \right. $.<br>The unit step function is discontinuous at $t=0$. So in effect, we need to think the unit step function as the limit of a continuous function, $u_\Delta(t) $ linearly increase from $0$ to $1$ at time $t$ from $0$ to $\Delta$. Thus $u(t)=u_\Delta(t) $ as $\Delta\to0$. </p>
</li>
<li><p>Unit impulse function is the derivative of unit step function, $\delta(t)=\frac{du(t)}{dt} $.<br>But $u(t)$ is discontinuous, so we would consider $\delta_\Delta(t)$ as the derivative of $u_\Delta(t)$, $\delta_\Delta(t)=\frac{du_\Delta(t)}{dt} $. So $\delta(t)=\delta_\Delta(t) $ as $\Delta\to0$.<br>$\delta_\Delta(t)=\left\{\begin{matrix}\frac{1}\Delta&amp;0&lt;t&lt;\Delta\\0&amp;otherwise \end{matrix} \right.$. As $\Delta\to0$, the width of the rectangle decreases while the height increases, and the area remains $1$.<br>We can draw $\delta(t)$ by an arrow point to the positive size of y-axis with length of $1$.<br>Since $\displaystyle\lim_{\Delta\to0}u_\Delta(t)=u(t)$, $\displaystyle\lim_{\Delta\to0}\delta_\Delta(t)=\delta(t)$</p>
</li>
<li><p>The relationship from unit impulse to unit step is derivative. And, natually, the relationship from unit step to unit impulse is integral, $u(t)=\displaystyle\int^t_{-\infty}\delta(t)dt$</p>
</li>
</ol>
<h1 id="Systems"><a href="#Systems" class="headerlink" title="Systems"></a>Systems</h1><ol>
<li><p>Systems take one signal as its input, and output another signal.<br>For continuous-time system denoted by $x(t)\to y(t)$, both signals are continuous.<br>For discrete-time system denoted by $x[n]\to y[n]$, both signals are discrete. </p>
</li>
<li><p>A cascade of systems (a series interconnection systems): taking the output of one system as the input of another system. </p>
</li>
<li><p>Parallel interconnection system: inputs are feed into two systems simultaneously. And the output of those two systems are added together to give the overall system output. </p>
</li>
<li><p>Feedback interconnection system: the output of system 1 is the output of the overall system. The output also is fed into system 2. And the sum of output of system 2 and the overall input of the system is the input of system 1. </p>
</li>
<li><p>Memoryless: the output at a certain time only depends on the input of the same time.<br>The unit delay system $y[n]=x[n-1] $ is not a memoryless system. </p>
</li>
<li><p>Invertibility: given the output, you can figure out the unique input that cause the output.<br>Identity system is the cascade of a system and its inverse. The inverse of system $A$ is denoted by $A^{-1}$<br>The inverse of an integrator is a differentiator. But a differentiator is not invertible, for the input of a given output is not unique. </p>
</li>
<li><p>Causality: output at any time depends only on input prior or equal to that time. Or system cannot anticipate future inputs.<br>For $x_1(t)\to y_1(t) $ and $x_2(t)\to y_2(t) $, if $x_1(t)$ and $x_2(t)$ are the same util some time, then $y_1(t)$ and $y_2(t)$ are alse the same util the same time. </p>
</li>
<li><p>Stability: for every bounded input, the output is bounded.<br>An unstable system can be stablized through feadback. Feadback can also destablize a system. </p>
</li>
<li><p>Time-invariable: If $x(t)\to y(t) $, then $x(t-t_0)\to y(t-t_0) $.<br>The system doesn’t care what happened at the origin. If the input is shifted by a certain time, the output is also shifted by the same time. </p>
</li>
<li><p>Linearity: For two systems $x_1(t)\to y_1(t) $ and $x_2(t)\to y_2(t) $, then $ax_1(t)+bx_2(t)\to ay_1(t)+by_2(t) $. </p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/06/11/07-Workload-driven-Perfromance-Evaluation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/11/07-Workload-driven-Perfromance-Evaluation/" class="post-title-link" itemprop="url">07. Workload-driven Perfromance Evaluation</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-11 21:39:38" itemprop="dateCreated datePublished" datetime="2022-06-11T21:39:38+08:00">2022-06-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-12 16:26:27" itemprop="dateModified" datetime="2022-06-12T16:26:27+08:00">2022-06-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>We should compare parallel program speedup to the best sequential program, instead of parallel algorithm running on one core.<br>The reason is that to allow for parallelism, we might change the algorithm, and make it slower when executed sequentially.</p>
<h1 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h1><h2 id="Why-consider-scaling"><a href="#Why-consider-scaling" class="headerlink" title="Why consider scaling?"></a>Why consider scaling?</h2><ol>
<li><p>Arithmetic intensity is determined by both problem size and the processors number. Small problem size or large processor number both yields low arithmetic intensity.</p>
</li>
<li><p>If the problem size is too small, it might already execute fast enough on a single core. Scaling the performance of small problem may not be all that important.<br>Parallelism overheads dominate parallelism benefits, and may even result in slow downs. </p>
</li>
<li><p>If the problem size is too large for a single machine, working set may not fit in memory, and causing thrashing to disk. With enough processors, the key working set fits in per-processor cache.<br>This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing. </p>
</li>
<li><p>Another situation that we might get a super-linear speedup is when we try to search for a solution. With parallelism, we are trying more different variance of search, and more likely to find the solution earlier. </p>
</li>
<li><p>So we shouldn’t only consider a fixed problem size. Instead, it is desirable to scale problem size as machine sizes grow. </p>
</li>
<li><p>In architecture, scaling up considers how does performance scale with increasing core count, and will design scale to the high end?<br>Scaling down considers how does performance scale with decreasing core count, and will desing scale to the low end? </p>
</li>
</ol>
<h2 id="Different-scalings"><a href="#Different-scalings" class="headerlink" title="Different scalings"></a>Different scalings</h2><ol>
<li><p>Strong scaling: scaling processors with a fixed problem size. Consider the ratio between the runtime of problem $X$ on $P$ processors and the runtime $X$ on $1$ processor. </p>
</li>
<li><p>The goal ratio is $P$. This kind of scaling tells us does having more processors get job done faster?</p>
</li>
<li><p>Weak scaling: scaling problem size and processors proportionally. Consider the ratio of the runtime of problem $P\times X$ on $P$ processors and the runtime of problem $X$ on $1$ processor. </p>
</li>
<li><p>The goal ratio is $1$. This kind of scaling tells us does having more procesors let me do bigger jobs? </p>
</li>
<li><p>Problem size is often determined by more than one parameter. So in weak scaling, we need to consider how should the parameter be changed. </p>
</li>
</ol>
<h2 id="Scaling-constrains"><a href="#Scaling-constrains" class="headerlink" title="Scaling constrains"></a>Scaling constrains</h2><ol>
<li><p>When scaling a probelm, we should first ask that in my situation, under what constraints should the problem be scaled? </p>
</li>
<li><p>Problem-constrained scaling focuses on using a parallel computer to solve the same problem faster<br>Speedup $=\frac{time 1 processor}{time P processors}$</p>
</li>
<li><p>Time-constrained scaling focuses on completing more work in a fixed amount of time<br>Speedup $ = \frac{work done by P processors}{work done by 1 processor}$</p>
</li>
<li><p>“Work done” may not be linear function of problem inputs. One approach of defining “work done” is by execution time of same computation on a single processor (but consider effects of thrashing if problem too big)</p>
</li>
<li><p>Ideally, a measure of work is simple to understand and scales linearly with sequential run time (So ideal speedup remains linear in $P$)</p>
</li>
<li><p>Memory-constrained scaling (weak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work or execution times are held constant.<br>Speedup $= \frac{work (P processors)}{time (P processors)}/\frac{work (1 processor)}{time (1 processor)}=\frac{work per unit time on P processors}{work per unit time on 1 processor}<br>$There are two assumptions: memory resources scale with processor count, and spilling to disk is infeasible behavior. </p>
</li>
</ol>
<h2 id="Challenges-of-scaling-down-or-up"><a href="#Challenges-of-scaling-down-or-up" class="headerlink" title="Challenges of scaling down or up"></a>Challenges of scaling down or up</h2><ol>
<li><p>Preserve ratio of time spent in different program phases. </p>
</li>
<li><p>Preserve important behavioral characteristics. </p>
</li>
<li><p>Preserve contention and communication patterns. Tough to preserve contention since contention is a function of timing and ratios. </p>
</li>
<li><p>Preserve scaling relationships between problem parameters. </p>
</li>
</ol>
<h1 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h1><ol>
<li><p>Architects evaluate architectural decisions quantitatively using<br>hardware performance simulators. </p>
</li>
<li><p>Architect runs simulations with new feature, runs simulations without new feature, compare simulated performance. Or simulate against a wide collection of benchmarks. </p>
</li>
<li><p>You can design detailed simulator to test new architectural feature. It would be very expensive to simulate a parallel machine in full detail.<br>Often cannot simulate full machine configurations or realistic problem sizes (must scale down workloads significantly). Architects need to be confident scaled down simulated results predict reality</p>
</li>
<li><p>In trace-driven simulator, we instrument real code running on real machine to record a trace of all memory accesses. Then play back trace on simulator.<br>It may lead to overfit the trace you have instead of having a better generalization. </p>
</li>
<li><p>In execution-driven simulator, we execute simulated program in software. Simulated processors generate memory references, which are processed by the simulated memory hierarchy.<br>Performance of simulator is typically inversely proportional to level of simulated detail. </p>
</li>
<li><p>When dealing with large parameter space of machines (number of processors, cache sizes, cache line sizes, memory bandwidths, etc. ), we can use the architectural simulation state space. </p>
</li>
</ol>
<h1 id="Understanding-the-performance"><a href="#Understanding-the-performance" class="headerlink" title="Understanding the performance"></a>Understanding the performance</h1><ol>
<li><p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand.</p>
</li>
<li><p>Determine if your performance is limited by computation, memory bandwidth (or memory latency), or synchronization?<br>Try and establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li><p>Roofline model: Use microbenchmarks to compute peak performance of a machine as a function of arithmetic intensity of application. Then compare application’s performance to known peak values. </p>
</li>
<li><p>The x-axis means operational intensity (like Flops/Byte), and the y-axis means attenable GFlops/s.<br>In the diagonal region, the y grows with x, which means the memory bendwidth is limited. In the horizontal region, the y stays the same as x grows, which means the compute is limited. </p>
</li>
<li><p>When compute is limited, we can make use of ILP or SIMD, or balance floating-point.<br>When memory bandwidth is limited, we can limit accesses to unit stride accesses only, develope memory affinity, or use software prefetching. </p>
</li>
</ol>
<h2 id="Establish-high-watermarks"><a href="#Establish-high-watermarks" class="headerlink" title="Establish high watermarks"></a>Establish high watermarks</h2><ol>
<li><p>Add “math” (non-memory instructions).<br>Does execution time increase linearly with operation count as math is added? If so, this is evidence that code is instruction-rate limited</p>
</li>
<li><p>Remove almost all math, but load same data.<br>How much does execution-time decrease? If not much, suspect memory bottleneck</p>
</li>
<li><p>The first two way need to avoid compiler optimization. </p>
</li>
<li><p>Change all array accesses to A[0].<br>How much faster does your code get?<br>This establishes an upper bound on benefit of improving locality of data access</p>
</li>
<li><p>Remove all atomic operations or locks.<br>How much faster does your code get? (provided it still does approximately the same amount of work)<br>This establishes an upper bound on benefit of reducing sync overhead.</p>
</li>
</ol>
<h2 id="Profilers-performance-monitoring-tools"><a href="#Profilers-performance-monitoring-tools" class="headerlink" title="Profilers/performance monitoring tools"></a>Profilers/performance monitoring tools</h2><ol>
<li><p>All modern processors have low-level event “performance counters”, which are registers that count important details such as: instructions completed, clock ticks, L2/L3 cache hits/misses, bytes read from memory controller, etc. </p>
</li>
<li><p>Intel’s Performance Counter Monitor Tool provides a C++ API for accessing these registers. </p>
</li>
<li><p>It can use <code>getIPC(begin, end)</code>, <code>getL3CacheHitRatio(begin, end)</code>, <code>getBytesReadFromMC(begin, end)</code>, etc. to get values of those information. </p>
</li>
<li><p>The <code>begin</code> and <code>end</code> is a <code>SystemCountState</code> instance acquired by <code>getSystemCounterState()</code> at the beginning and end of the code to analyze. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">PCM *m = PCM::getInstance();</span><br><span class="line">SystemCounterState begin = getSystemCounterState();</span><br><span class="line"></span><br><span class="line"><span class="comment">// code to analyze goes here</span></span><br><span class="line"></span><br><span class="line">SystemCounterState end = getSystemCounterState();</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(“Instructions per clock: %f\n”, getIPC(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“L3 cache hit ratio: %f\n”, getL3CacheHitRatio(begin, end));</span><br><span class="line"><span class="built_in">printf</span>(“Bytes read: %d\n”, getBytesReadFromMC(begin, end));</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/05/21/06-Locality-Communication-and-Contention/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/21/06-Locality-Communication-and-Contention/" class="post-title-link" itemprop="url">06. Locality, Communication, and Contention</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-21 13:42:23" itemprop="dateCreated datePublished" datetime="2022-05-21T13:42:23+08:00">2022-05-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-11 21:43:56" itemprop="dateModified" datetime="2022-06-11T21:43:56+08:00">2022-06-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Communication"><a href="#Communication" class="headerlink" title="Communication"></a>Communication</h1><h2 id="Reduce-communication-time"><a href="#Reduce-communication-time" class="headerlink" title="Reduce communication time"></a>Reduce communication time</h2><ol>
<li><p>Total communication time = overhead + occupancy + network delay. </p>
</li>
<li><p>Overhead is the time spent on the communication by a processor, occupancy is the time for data to pass through slowest component of system, and network delay is everything else. </p>
</li>
<li><p>Reduce overhead of communication to sender/receiver:<br>Reassign tasks in a better way that need to send fewer messages.<br>Make messages larger to amortize overhead.<br>Coalesce many small messages into large ones</p>
</li>
<li><p>Reduce delay:<br>Application writer can restructure code to exploit locality.<br>Hardware implementor can improve communication architecture. </p>
</li>
</ol>
<h2 id="Reduce-communication-cost"><a href="#Reduce-communication-cost" class="headerlink" title="Reduce communication cost"></a>Reduce communication cost</h2><ol>
<li><p>Total communication cost = communication time - overlap</p>
</li>
<li><p>Overlap: portion of communication performed concurrently with other work (“other work” can be computation or other communication)</p>
</li>
<li><p>Cost is the part that your cannot get over with by changing the protocol. The communication time is obviously necessary, and we can hide the overlap part by pipelining. </p>
</li>
<li><p>Increase communication/computation overlap:<br>Application writer can use asynchronous communication (e.g., async messages)<br>Hardware implementor can use pipelining, multi-threading, pre-fetching, out-of-order execution<br>Requires additional concurrency in application (more concurrency than number of execution units)</p>
</li>
<li><p>Instruction pipeline: Break execution of each instruction down into several smaller steps.<br>Enables higher clock frequency, only a simple, short operation is done by each part of pipeline each clock</p>
</li>
<li><p>Non-piplined communication: $T(n)=T_0+\frac{n}{B}$ ($T_0$ is the start-up latency, n is bytes transferred in operation, B is transfer rate or bandwidth)<br>Efficient bandwidth $=\frac{n}{T(n)}$</p>
</li>
</ol>
<h2 id="Improve-arithmetic-intensity"><a href="#Improve-arithmetic-intensity" class="headerlink" title="Improve arithmetic intensity"></a>Improve arithmetic intensity</h2><ol>
<li><p>Communication-to-computation ratio = amount of communication  / amount of computation<br>The units can be different. If the denominator is the execution time of computation, the ratio gives the average bandwidth requirement</p>
</li>
<li><p>Arithmetic intensity = 1 / communication-to-computation ratio</p>
</li>
<li><p>Change the traversal order to reduce the time between accesses to same data. We want to do all the calculations related to the accessing data now.<br>This way can improve the utilization of cache by preventing that those data get flushed out when we try to access them again. </p>
</li>
<li><p>Fuse loops to reduce the frequency of store operation. </p>
</li>
<li><p>Improve arithmetic intensity by sharing data. Co-locate tasks that operate on the same data.  Schedule threads working on the same data structure at the same time on the same processor. Reduces inherent communication</p>
</li>
</ol>
<h2 id="Reduce-artifactual-communication"><a href="#Reduce-artifactual-communication" class="headerlink" title="Reduce artifactual communication"></a>Reduce artifactual communication</h2><ol>
<li><p>Inherent communication: Communication that must occur in a parallel algorithm. The communication is fundamental to the algorithm.<br>Artifactual communication: all other communication that happens because we want to efficiently use resource.</p>
</li>
<li><p>Granularity of communication can be important because it may introduce artifactual communication.<br>Assume that communication granularity is a cache line. </p>
</li>
<li><p>If we see data as row-major layout, when the data required is in column, each communication actually only provides one needed data. </p>
</li>
<li><p>When Threads access their assigned elements (no inherent communication exists), real machine triggers (artifactual) communication due to the cache line being written to by both processors. </p>
</li>
<li><p>Reducing artifactual comm by blocked data layout. Each communication only involves  the data in the same block. If communication granularity is larger than a block row, each communication will transfer multiple rows. </p>
</li>
</ol>
<h1 id="Contention"><a href="#Contention" class="headerlink" title="Contention"></a>Contention</h1><ol>
<li><p>Contention occurs when many requests to a resource are made within a small window of time. The resource is a hot spot.<br>Contention for shared resource results in longer overall operation times. </p>
</li>
<li><p>Distributed work queues serve to reduce contention (contention in access to single shared work queue)</p>
</li>
<li><p>One way to reduce contention is to use finer-granularity locks. Instead of locking the whole data structure, we can only lock a part of that structure. </p>
</li>
<li><p>Another way is that each CUDA block computes partial results and merges them afterwards. But this requires extra work for merging, and each CUDA block need to store a partial result instead of all of them using the same result. </p>
</li>
<li><p>The best way is to stagger access to contended resources. For example, instead of calculate the final result directly, we can calculate several temporal results which has no contention, and get the final result from them. </p>
</li>
</ol>
<h1 id="Understanding-the-performance"><a href="#Understanding-the-performance" class="headerlink" title="Understanding the performance"></a>Understanding the performance</h1><ol>
<li><p>Always, always, always try the simplest parallel solution first, then measure performance to see where you stand. </p>
</li>
<li><p>We should compare parallel program speedup to the best sequential program, instead of parallel algorithm running on one core.<br>The reason is that to allow for parallelism, we might change the algorithm, and make it slower when executed sequentially. </p>
</li>
<li><p>Determine if your performance is limited by computation, memory bandwidth (or memory latency), or synchronization?<br>Try and establish “high watermarks”. What’s the best you can do in practice? How close is your implementation to a best-case scenario?</p>
</li>
<li><p>Roofline model: Use microbenchmarks to compute peak performance of a machine as a function of arithmetic intensity of application. Then compare application’s performance to known peak values</p>
</li>
</ol>
<h2 id="Establish-high-watermarks"><a href="#Establish-high-watermarks" class="headerlink" title="Establish high watermarks"></a>Establish high watermarks</h2><ol>
<li><p>Add “math” (non-memory instructions).<br>Does execution time increase linearly with operation count as math is added?<br>If so, this is evidence that code is instruction-rate limited</p>
</li>
<li><p>Remove almost all math, but load same data.<br>How much does execution-time decrease?<br>If not much, suspect memory bottleneck</p>
</li>
<li><p>Change all array accesses to A[0].<br>How much faster does your code get?<br>This establishes an upper bound on benefit of improving locality of data access</p>
</li>
<li><p>Remove all atomic operations or locks.<br>How much faster does your code get? (provided it still does approximately the same amount of work)<br>This establishes an upper bound on benefit of reducing sync overhead. </p>
</li>
</ol>
<h2 id="Scaling"><a href="#Scaling" class="headerlink" title="Scaling"></a>Scaling</h2><ol>
<li><p>Arithmetic intensity is determined by both problem size and the processors number. Small problem size or large processor number both yields low arithmetic intensity. </p>
</li>
<li><p>If the problem size is too small, it might already execute fast enough on a single core. Scaling the performance of small problem ay not be all that important. </p>
</li>
<li><p>If the problem size is too large for a single machine, working set may not fit in memory, and causing thrashing to disk. With enough processors, the key working set fits in per-processor cache.<br>This may get a super-linear speedup and make speedup on a bigger parallel machine with more memory look amazing. </p>
</li>
<li><p>Desire to scale problem size as machine sizes grow (buy a bigger machine to compute more, rather than just compute the same problem faster)</p>
</li>
<li><p>Problem-constrained scaling focuses on using a parallel computer to solve the same problem faster<br>$Speedup=\frac{time 1 processor}{time P processors}$</p>
</li>
<li><p>Time-constrained scaling focuses on completing more work in a fixed amount of time<br>$Speedup = \frac{work done by P processors}{work done by 1 processor}$<br>“Work done” may not be linear function of problem inputs. One approach of defining “work done” is by execution time of same computation on a single processor (but consider effects of thrashing if problem too big)</p>
</li>
<li><p>Memory-constrained scaling (wak scaling) focuses on running the largest problem possible without overflowing main memory. Neither work or execution times are held constant.<br>$Speedup = \frac{work (P processors)}{time (P processors)}/\frac{work (1 processor)}{time (1 processor)}=\frac{work per unit time on P processors}{work per unit time on 1 processor}$</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/05/19/05-Graphic-processing-units-and-CUDA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/19/05-Graphic-processing-units-and-CUDA/" class="post-title-link" itemprop="url">05. Graphic processing units and CUDA</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-19 09:59:09" itemprop="dateCreated datePublished" datetime="2022-05-19T09:59:09+08:00">2022-05-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-22 17:22:04" itemprop="dateModified" datetime="2022-05-22T17:22:04+08:00">2022-05-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Graphics"><a href="#Graphics" class="headerlink" title="Graphics"></a>Graphics</h1><ol>
<li>The first step to draw a graphic in screen is to describe the things (key entities) that are manipulated, whose surface is represented as a 3D triangle mesh.<br>So the input of the calculating system is a list of vertices in 3D space and their connectivity into primitives. </li>
<li>The operations of system is as following:<br>Given a scene camera position, compute where the vertices lie on screen<br>Group vertices into primitives<br>Generate one fragment for each pixel a primitive overlaps<br>Compute color of primitive for each fragment based on scene lighting and primitive material properties<br>Put color of the “closest fragment” to the camera in the output image</li>
<li>We can Abstract process of rendering a picture as a sequence of operations on vertices, primitives, fragments, and pixels.<br>GPUs are very fast processors for performing the same computation (shader programs) on large collections of data (streams of vertices, fragments, and pixels), which sounds like data-parallelism. </li>
<li>To do GPU-based scientific computation, we need to set OpenGL output image size to be output array size, and render 2 triangles that exactly cover screen. </li>
</ol>
<h1 id="CUDA"><a href="#CUDA" class="headerlink" title="CUDA"></a>CUDA</h1><h2 id="Grid-Block-and-Thread"><a href="#Grid-Block-and-Thread" class="headerlink" title="Grid, Block and Thread"></a>Grid, Block and Thread</h2><ol>
<li><p>CUDA thread IDs can be up to 3-dimensional. Multiple threads make up a block, and multiple blocks make up a grid.<br>Each kernel has a grid. All the the blocks in a grid form a 3D matrix, and all the threads in a block also form a 3D matrix.<br>We can get information about the shape of threads in a block matrix or block in a grid matrix in a block with <code>block_dim</code> and <code>grid_dim</code></p>
</li>
<li><p>When launching the CUDA threads, we need to specify the size of blocks and threads with <code>kernel_function&lt;&lt;&lt;block_dim, thread_dim&gt;&gt;&gt;(parameters)</code>.<br>Here <code>block_dim</code> and <code>thread_dim</code> can be either a <code>dim3</code> value or a number of the total number of blocks and threads per-block to make the CUDA compiler figure out how to arrange blocks and threads.</p>
</li>
<li><p>So the coordinate to declare or locate a block in a grid, or a thread in a block is 3D. In the CUDA code, we can get the information about current block or thread with <code>blockIdx</code>, <code>threadIdx</code>, and their properties <code>x</code>, <code>y</code>, <code>z</code>. </p>
</li>
<li><p>The calculation object of CUDA is usually matrices. So we would prefer to cut matrices into blocks. Each CUDA block calculate one  block in matrices, and each thread calculate one element in matrices. </p>
</li>
<li><p>In <code>pthread</code>, there is stack space for thread, and need to allocate control block so OS can schedule thread.<br>Unlike <code>pthread</code>, CUDA control those instances by thread blocks. If control by threads, there will be too many to control.</p>
</li>
<li><p>Major CUDA assumption: thread block execution can be carried out in any order (no dependencies between blocks)</p>
</li>
</ol>
<h2 id="Kernel-function"><a href="#Kernel-function" class="headerlink" title="Kernel function"></a>Kernel function</h2><ol>
<li><p>“Host” code: serial execution runs as part of normal C/C++ application on CPU<br>“CUDA device” code: a kernel function runs on GPU, which is denoted by <code>__global__</code> before the definition of the kernel function.<br>Device function: SPMD execution on GPU, which is denoted by <code>__device__</code>. These functions are called by kernels, and don’t generate new threads.<br>Kernels and device functions only reference device memory. Host code can only reference host memory, but can have pointers to device memory. </p>
</li>
<li><p>In kernel function, we need to get the indices of the element a thread calculating. For a 2D matrix, we usually take <code>x</code> as column direction, and <code>y</code> as row direction.<br>So to access <code>A[i][j]</code>, <code>i = blockIdx.y * blockDim * y + threadIdx.y</code> and <code>j = blockIdx.x * blockDim.x + threadIdx.x</code>.</p>
</li>
<li><p>Number of kernel invocations is not determined by size of data collection. So normally we want to do a test before accessing the vector values (array values). The overhead of the test can be ignored, although it does cause some threads are not used. </p>
</li>
<li><p><code>__syncthreads()</code> is a barrier that wait for all threads in the block to arrive at this point.<br>Atomic operations on both global memory and shared memory variables is also provided, but they are very expensive, should only be used as the last resort.<br>There is an implicit barrier across all threads at return of kernel. </p>
</li>
<li><p>A compiled CUDA device binary includes program text (instructions) and information about required resources (how many threads per block, how much space per thread, how much shared space per thread block) </p>
</li>
</ol>
<h2 id="Memory-Model"><a href="#Memory-Model" class="headerlink" title="Memory Model"></a>Memory Model</h2><ol>
<li><p>Host and device have distinct address spaces, so before the execution, we need to copy data into CUDA memory.<br><code>cudaMalloc(ptr, size)</code> can be used to allocate CUDA memory, and <code>cudaFree(ptr)</code> can free CUDA memory. The pointer in <code>cudaMalloc</code> can just be a host variable, but the pointer in <code>cudaFree</code> must be allocated by <code>cudaMalloc</code>.<br><code>cudaMemcpy(dest, src, size, kind)</code> can copy those data into CUDA memory. <code>kind</code> is the direction of copy, which can be <code>cudaMemcpyHostToDevice</code> or <code>curdaMemcpyDeviceToHost</code>. </p>
</li>
<li><p>There are three distinct types of memory visible to kernels, per-thread private memory, per-block shared memory, and device global memory. </p>
</li>
<li><p><code>cudaMalloc</code>, <code>cudaMemcpy</code> and <code>cudaFree</code> all operate on device global memory, and those local variables in kernel function are in perthread private memory.<br>We can declare variables in per-block shared memory with <code>__share__</code>. </p>
</li>
<li><p>If multiple threads in a block need to access a common memory, they will all read that memory once. And if that memory is in the global memory, it would be really slow. </p>
</li>
<li><p>To avoid such efficient decrease, we can copy the common memory into the per-block shared memory. So we only need to access global memory once, and later accesses only in per-block shared memory.<br>We just need to declare a <code>__share__</code> array, and assign it with values in global memory. </p>
</li>
<li><p>All threads copy data from global memory to per-block shared memory asynchronous, so we better ass a <code>_syncthread()</code> to  make sure later operations only start when all data have been copied. </p>
</li>
</ol>
<h2 id="Hardware-implementation"><a href="#Hardware-implementation" class="headerlink" title="Hardware implementation"></a>Hardware implementation</h2><ol>
<li><p>Those synchronization within a warp and scheduling are done by hardware, and programmers don’t need to worry about these things.</p>
</li>
<li><p>GPU implementation maps thread blocks (“work”) to cores using a dynamic scheduling policy that respects resource requirements. </p>
</li>
<li><p>In GPU implementation (not a CUDA abstraction), each SM has multiple groups of warps. However, the number of warp execution contexts is far more than the number of warps.<br>And there is a warp selector for each warp to choose the instruction to be executed by that warp. Each warp selector usually has two (or more) Fectch/Decoder unit. </p>
</li>
<li><p>Shared per-block memory and L1-cache are inside each SM, but L2-cache is shared by all SMs.<br>The devide global memory (GPU memory) only communicates with blocks through L2-cache. </p>
</li>
<li><p>Each clock, an SM will choose several warp contexts to fill all warps to use thread-level pallelism, and a warp will choose several instructions to fill all Fetch/Decoder units to use instruction-level pallelism. </p>
</li>
<li><p>When running a CUDA program, GPU work schedulor will map one block to multiple warps in the same SM core. CUDA threads numbered within block in row-major order.<br>Inside a single thread block is SPMD shared address space programming. </p>
</li>
<li><p>When single warp accesses consecutive memory locations, do block read or write. When single warp accesses separated memory locations, requires gather (read) or scatter(write)</p>
</li>
<li><p>One SM core may have multiple blocks, so a SMM core is capable of concurrently executing multiple CUDA thread blocks.<br>When all threads in block complete, block resources (shared memory allocations, warp execution contexts) become available for next block. </p>
</li>
<li><p>The CUDA program is not compiled to SIMD instructions like ISPC gangs. GPU hardware is dynamically checking whether 32 independent CUDA threads share an instruction, and if this is true, it executes all 32 threads in a SIMD manner, or performance can suer due to divergent execution. </p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/05/17/04-Work-Distribution-and-Scheduling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/17/04-Work-Distribution-and-Scheduling/" class="post-title-link" itemprop="url">04. Work Distribution and Scheduling</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-17 15:35:33" itemprop="dateCreated datePublished" datetime="2022-05-17T15:35:33+08:00">2022-05-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-18 17:07:58" itemprop="dateModified" datetime="2022-05-18T17:07:58+08:00">2022-05-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Balancing-the-workload"><a href="#Balancing-the-workload" class="headerlink" title="Balancing the workload"></a>Balancing the workload</h1><p>Always implement the simplest solution first, then measure performance to determine if you need to do better. </p>
<h2 id="Static-assignment"><a href="#Static-assignment" class="headerlink" title="Static assignment"></a>Static assignment</h2><ol>
<li>The assignment of work to threads is pre-determined. But not necessarily determined at compile-time, may depend on runtime parameters. </li>
<li>Benefit: simple, essentially zero runtime overhead</li>
<li>Applicable situation: When the cost (execution time) of work and the amount of work is predictable. The followings are some most common situations:<br>When it is known up front that all work has the same cost<br>When work is predictable, but not all jobs have same cost<br>When statistics about execution time are known</li>
<li>Semi-static assignment: When the cost of work is predictable for near-term future, we can periodically profile itself and re-adjust assignment.<br>Assignment is “static” for the interval between re-adjustments</li>
</ol>
<h2 id="Dynamic-assignment"><a href="#Dynamic-assignment" class="headerlink" title="Dynamic assignment"></a>Dynamic assignment</h2><ol>
<li><p>Program determines assignment dynamically at runtime to ensure a well distributed load. Often used when the execution time of tasks, or the total number of tasks, is unpredictable. </p>
</li>
<li><p>The ISPC task is implemented dynamically. </p>
</li>
</ol>
<h3 id="With-one-queue"><a href="#With-one-queue" class="headerlink" title="With one queue"></a>With one queue</h3><ol>
<li><p>The programmers divide the whole problem into sub-problems (or “tasks”, “work”). A queue shared by all threads is a collection of work to do. Whenever a thread finished its task, it will grab another task from the queue. </p>
</li>
<li><p>Fine granularity partitioning: each task is small.<br>This is likely to have a good workload balance, but potential for high synchronization cost. </p>
</li>
<li><p>Coarse granularity partitioning: each task is larger.<br>This will decrease synchronization cost and the overhead, but may have a worse workload balance. </p>
</li>
<li><p>Long tasks should be scheduled first. Thread performing long task performs fewer overall tasks, but approximately the same amount of work as the other threads. This requires some knowledge of workload. </p>
</li>
</ol>
<h3 id="With-a-set-of-queues"><a href="#With-a-set-of-queues" class="headerlink" title="With a set of queues"></a>With a set of queues</h3><ol>
<li><p>When assign with one queue, all threads have to communicate with each other about the queue. </p>
</li>
<li><p>Each thread has their own queue, and they only execute tasks in their queue. So there is no need to communication with other threads. </p>
</li>
<li><p>At the beginning, the programmer push tasks into queues arbitarily (a bit like static assignment).<br>The dynamic is that when a queue is empty, that thread can steal from other threads that is  still working.<br>It will steal from a random thread. Every time, it will steal a proportion of the tasks in the target queue, not all of them, and usually more than one task.<br>A thread is terminated when there is no thread for it to steal, namely when a steal is failed, it will try to steal from other threads, util it have tried all of them. </p>
</li>
<li><p>Stealing involves communication, but in a lower frequency then one queue method. And in this way, the local queue access is fast. </p>
</li>
<li><p>Sometimes it is hard to have fully independent task, but work in task queues need not be independent.<br>A task is not removed from queue and assigned to worker thread until all task dependencies are satisfied. Workers can submit new tasks (with optional explicit dependencies) to task system</p>
</li>
</ol>
<h1 id="Scheduling"><a href="#Scheduling" class="headerlink" title="Scheduling"></a>Scheduling</h1><p>In a divide-and-conquer algorithm, there is both dependencies and independencies. Like in quick-sort, both the divide is dependent on the partition, and those two divide is independent.<br>With Cilk Plus, we can express divide-and-conquer easier. </p>
<h2 id="cilk-spawn"><a href="#cilk-spawn" class="headerlink" title="cilk_spawn"></a>cilk_spawn</h2><ol>
<li><p><code>cilk_spawn</code> is labelled before a function call, so that the called function can run with the code after the call concurrently.<br>The call labelled <code>cilk_spawn</code> is the spawned child, and the rest of the code is the continuation.</p>
</li>
<li><p>In divide-and-conquer, there is always a time, when the problem size is small enough that overhead of spawn trumps benefits of<br>potential parallelization. And then we will solve those problems sequentially. </p>
</li>
<li><p>The main idea is to expose independent work (potential parallelism) to the system using <code>cilk_spawn</code>. </p>
</li>
<li><p><code>cilk_spawn</code> is a bit like <code>pthread_create</code>, and <code>cilk_sync</code> is similar to <code>pthread_join</code>. But <code>pthread</code> has some problems when too many threads are spawned.<br>The first is the heavyweight spawn operation. And many more concurrently running threads than cores, which will cause context switching overhead and larger working set than necessary, less cache locality. </p>
</li>
<li><p>The Cilk Plus runtime maintains pool of worker threads. All threads created at application launch.  There are exactly as many worker threads as execution contexts in the machine.<br>If we labelled everything <code>cilk_spawn</code>, the main thread has nothing to do. </p>
</li>
<li><p>Each thread in the pool will maintain a work queue to store what word needs to be done.<br>When a thread goes idle, it will look in busy thread’s queue for work, and moves work from busy thread’s queue to its own queue.</p>
</li>
<li><p>If caller thread runs the continuation first, the queue should record the child for later execution, and child is made available for stealing by other threads.<br>In this method, caller thread will spawn as many child as it could, like BFS. If no stealing, execution order is very different than<br>that of program withcilk_spawnremoved. </p>
</li>
<li><p>If caller thread runs the child first, the queue should record continuation for later execution, and continuation is made available for stealing by other threads.<br>In this method, caller thread will only create one item to steal.<br>If no stealing occurs, thread continually pops continuation from work queue, enqueues new continuation (like DFS). The order of execution is the same as for program with spawn removed.<br>If continuation is stolen, stealing thread spawns next child. </p>
</li>
<li><p>If the continuation is run first, there will be more items to steal, and thus makes a better advantage of multi-thread.<br>But if the continuation is run first, the work queue storage for system with T threads is no more than T times that of stack storage for single threaded execution, and thus saves more space. </p>
</li>
<li><p>Work queue is implemented as a dequeue (double ended queue).<br>Local thread pushes/pops from the “tail” (bottom), while remote threads steal from “head” (top).<br>Reduces contention with local thread: local thread is not accessing same part of dequeue that stealing threads do.<br>Do larger work first: in divide-and-conquer, the top of the queue is usually at the beginning of call tree, and is a larger piece of work.<br>Maximizes locality: in conjunction with run-child-first policy, local thread works on local part of call tree</p>
</li>
</ol>
<h2 id="cilk-sync"><a href="#cilk-sync" class="headerlink" title="cilk_sync"></a>cilk_sync</h2><ol>
<li><p><code>cilk_sync</code> is used after those <code>cilk_spawn</code> call code. It will return when all calls spawned by current function have completed. </p>
</li>
<li><p>There is an implicitcilk_syncat the end of every function that contains a cilk_spawn, so when a Cilk function returns, all work associated with that function is complete. </p>
</li>
<li><p>If no work has been stolen by other threads, then there’s nothing to do at the sync point, <code>cilk_sync</code> is a no-op. But this is not a common situation. </p>
</li>
<li><p>One way to implement sync is with stalling joint.<br>Thread that initiates the fork must perform the sync. Therefore it waits for all spawned work to be complete.<br>descriptor for block A created<br>When a stealing from the initial thread happens, a descriptor for that stolen work is created to track the number of outstanding spawns for the block, and the number of those spawns that have completed.<br>When all the child spowned for the block is done, this block is considered done, and the descriptor is free. When all the blocks are done, sync is fullfilled. </p>
</li>
<li><p>Another way to implement sync is the greedy policy.<br>When thread that initiates the fork goes idle, it can look to steal new work.  Last thread to reach the join point continues execution after sync. This will also create a decriptor for those stolen work.<br>Worker thread that initiated spawn may not be thread that executes logic after <code>cilk_sync</code>. </p>
</li>
<li><p>In greedy policy, All threads always attempt to steal if there is nothing to do, and thread only goes idle if no work to steal is present in system. But in stalling policy, the initial thread doesn’t steal, and only waits when its work is done. </p>
</li>
<li><p>Overhead of bookkeeping steals and managing sync points only occurs when steals occur. If large pieces of work are stolen, this should occur infrequently. Most of the time, threads are pushing/popping local work from their local dequeue. </p>
</li>
<li><p>Cilk uses greedy join scheduling. </p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/05/12/08-Metaprogramming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/12/08-Metaprogramming/" class="post-title-link" itemprop="url">08. Metaprogramming</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-05-12 19:13:47 / Modified: 19:51:22" itemprop="dateCreated datePublished" datetime="2022-05-12T19:13:47+08:00">2022-05-12</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/MIT-The-Missing-Semester-of-You-CS-Education/" itemprop="url" rel="index">
                    <span itemprop="name">MIT The Missing Semester of You CS Education</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li><p><code>Makefile</code> contains the dependencies and the rules. The rules are commands that described how to go from a complete list of dependencies to the given target.  </p>
</li>
<li><p>The depencies can be declared with <code>:</code>. The file in the left if the dependencies of the file in the right. </p>
</li>
<li><p>Only if any of the dependencies are changed, the file should be rebuild. And <code>make</code> will only rebuild those changed dependencies. </p>
</li>
<li><p>Sematic versioning: It marks the versions with three numbers, <code>major version</code>, <code>minor version</code>, <code>patch version</code>. If a backwards compatible changes is made, only <code>patch version</code> should be changed. If something is added, the <code>minor version</code> will increase, and <code>patch version</code> is set to 0. If a change is not backwards compatible, then <code>major version</code> will be increased. </p>
</li>
<li></li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/05/11/06-Version-Control-git/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/11/06-Version-Control-git/" class="post-title-link" itemprop="url">06.Version Control (git)</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-11 13:35:13" itemprop="dateCreated datePublished" datetime="2022-05-11T13:35:13+08:00">2022-05-11</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-12 14:14:50" itemprop="dateModified" datetime="2022-05-12T14:14:50+08:00">2022-05-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/MIT-The-Missing-Semester-of-You-CS-Education/" itemprop="url" rel="index">
                    <span itemprop="name">MIT The Missing Semester of You CS Education</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Git-Model"><a href="#Git-Model" class="headerlink" title="Git Model"></a>Git Model</h1><ol>
<li><p>Folders are called <code>trees</code>, and files are called <code>blobs</code>. Trees can have other trees and blobs, while blobs cannot have trees. </p>
</li>
<li><p>Git keeps snapshots of the file system, and arrange them in a form of tree according to commits. </p>
</li>
<li><p>One snapshot has only one parant snapshot, but may have multiple children. Those children contains different commits, and don’t share their new changes.<br>We can merge snapshots together to get all changes in all snapshots. Sometimes this process is automatically, but there are some cases when it trigues the merge conflict. </p>
</li>
<li><p>Actually, git stores versions not by snapshots, but by commits. Each commit contains not only the snapshot of the file system snapshot, but also some metadata, like the author, message and parent of this commit. </p>
</li>
<li><p>Git calculates the hash name of a commit through Sha-1 algorithm, which is a 40 character hex code.<br>So references is provided to refer a human-readable name to a hash name. While the human-readable name is changeable, the hash name cannot be changed. </p>
</li>
</ol>
<h1 id="Git-Commands"><a href="#Git-Commands" class="headerlink" title="Git Commands"></a>Git Commands</h1><ol>
<li><p><code>git init</code> will create a <code>.git</code> folder, and make the current folder into a git folder</p>
</li>
<li><p><code>git help &lt;git command&gt;</code> can show the document of that command. Don’t need <code>git</code> in git command. </p>
</li>
<li><p><code>git status</code> will show the status of changes compared the current file system with its parent. It will only show blobs or trees, not the details.<br><code>git log</code> will show all the commits in history in a flat form, including author, date and message. <code>git log --all --graph --decorate</code> can show the history more beautiful, and the relations more clear. <code>--oneline</code> will show some more simplified information. </p>
</li>
<li><p><code>git add &lt;file path&gt;</code> can add a changed file to the next commit.<br><code>git commit</code> will commit every changes that have been added. </p>
</li>
<li><p><code>git cat-file -p &lt;hash name&gt;</code> will show what is in the object with that hash name. The object might be a commit, a snapshot, a tree or a blob. </p>
</li>
<li><p><code>HEAD</code> is the current commit you are looking, and <code>master</code> is the up-to-date commit.<br><code>git checkout &lt;hash name&gt;</code> will change the current working space  to that commit, and the <code>HEAD</code> will point to that commit. This will change all the files into the snapshot of the commit.<br>If we want to go to master, we can use <code>master</code> instead of <code>&lt;hash name&gt;</code>. </p>
</li>
<li><p><code>git diff</code> will show the differet between the current file system and <code>HEAD</code> commit. It will not only show the file-names, by also what is changed in those files.<br>It can takes some arguments, like <code>git &lt;hash name1&gt; &lt;hash name2&gt; &lt;file name&gt;</code>. In this case, it will only compare the state of that file between that two commits. </p>
</li>
<li><p>When the <code>HEAD</code> is pointing to the end of a branch, and we make a commit, <code>HEAD</code> that that branch mark will both point to the new commit.<br><code>git branch</code> will list all the braches we have. <code>Master</code> is a special brach that will be created from the beginning.<br><code>git brach &lt;name&gt;</code> will name the commit <code>HEAD</code> is pointing to that name.<br><code>git checkout &lt;branch name&gt;</code> will point the <code>HEAD</code> to that brach mark. <code>-b</code> flag will create and check out that branck. </p>
</li>
<li><p><code>git merge &lt;branch name&gt;</code> will merge the commit that branch mark points to into the <code>HEAD</code> commit.<br>If the <code>HEAD</code> commit is a predecessor of that branch, a fast-forward merging is used, which simply move the <code>HEAD</code> to that branch.<br>Or Git will do its best to auto-merge the changes, but sometimes it might get confused, so a merge conflict is raised. We can manually fix those conflicts, and use <code>git add</code> to add the changed files again, use <code>git merge --continue</code> to finish the merge. </p>
</li>
</ol>
<h1 id="Git-Remote"><a href="#Git-Remote" class="headerlink" title="Git Remote"></a>Git Remote</h1><ol>
<li><p><code>git remote</code> will print all the remotes that git is aware of for the current repository</p>
</li>
<li><p><code>git remote add &lt;name&gt; &lt;url&gt;</code>. The name is <code>origin</code> by convention. </p>
</li>
<li><p><code>git push &lt;remote name&gt; &lt;local branch&gt;:&lt;remote branch&gt;</code> will set the branch in the remote to the local branch. If that remote branch doesn’t exist, it will be created.<br>We can use <code>git branch --set-upstream-to=&lt;remote name&gt;/&lt;remote branch&gt;</code> so that we will push the current branch to the set remote branch by default. </p>
</li>
<li><p><code>git clone &lt;url&gt; &lt;foler name&gt;</code> will clone the repo into the local folder.<br><code>--shallow</code> will only get the latest snapshot, not the whole history. </p>
</li>
<li><p><code>git fetch &lt;remote name&gt;</code> will retrieve the changes in the remote. But it will not change anything we have locally, and the local master branch is still pointing to where it was. We can use <code>git merge</code> to move master branch.<br><code>git pull &lt;remote name&gt;</code> will fetch and merge. </p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/05/10/05-Commandline-Environment/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/05/10/05-Commandline-Environment/" class="post-title-link" itemprop="url">05. Commandline Environment</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-05-10 15:43:00" itemprop="dateCreated datePublished" datetime="2022-05-10T15:43:00+08:00">2022-05-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-11 13:35:38" itemprop="dateModified" datetime="2022-05-11T13:35:38+08:00">2022-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/MIT-The-Missing-Semester-of-You-CS-Education/" itemprop="url" rel="index">
                    <span itemprop="name">MIT The Missing Semester of You CS Education</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Job-Control"><a href="#Job-Control" class="headerlink" title="Job Control"></a>Job Control</h1><ol>
<li><p>The unit of argument of <code>sleep</code> is second. </p>
</li>
<li><p><code>Ctrl + C</code> sends a <code>SIGINT</code> (interrupt program, can be captured by programs and do something else)<br><code>Ctrl + \</code> sends a <code>SIGQUIT</code> (quit program, cannot be captured by programs)<br><code>Ctrl + Z</code> sends a <code>SIGSTOP</code> (stop or suspend program,cannot be caught or ignored)</p>
</li>
<li><p>Add a <code>&amp;</code> at the end of a command, it will run on background. </p>
</li>
<li><p><code>jobs</code> can show all the jobs in the shell.<br><code>bg %[number]</code> can run a job on background. The number is the same as what showed in <code>jobs</code>.<br><code>fg %[number]</code> can bring a job to the frontground.<br><code>kill %[number]</code> can do more than kill a program. With flags, it can send any signal to that program. </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bg %1</span><br><span class="line">kill -STOP %1 # only suspend, don't kill</span><br></pre></td></tr></table></figure>
</li>
<li><p>When a program get a <code>SIGHUP</code>, it will hung up and close. But we can prevent that by add a <code>nohup</code> at the begining of the command. </p>
</li>
</ol>
<h1 id="Terminal-multiplexer-tmux"><a href="#Terminal-multiplexer-tmux" class="headerlink" title="Terminal multiplexer (tmux)"></a>Terminal multiplexer (tmux)</h1><ol>
<li><p>Normally, when the ssh is closed, a SIGHUP will be sent to the session. But tmux wrapped the session to ignore that SIGUP. </p>
</li>
<li><p>Sessions have windows, and windows have panes. Windows like tabs in web browser. </p>
</li>
<li><p>We can create a new session with different commands.<br><code>tmux</code> will create with a default number of a number that starts from 0.<br><code>tmux new -t [name]</code> will create with a specific name.<br>We can attach to a session with <code>tmux attach -t [name]</code> or <code>tmux a -t [name]</code></p>
</li>
<li><p>When in a tmux session, <code>Ctrl + B</code> will enter the command mode. The way to use it is to press <code>Ctrl + B</code> and release it to press other command keys. </p>
</li>
<li><p><code>d</code> will detach session, but won’t kill it. <code>Ctrl + D</code> (without <code>Ctrl + B</code>) will kill the session.<br><code>$</code> can rename a session. </p>
</li>
<li><p><code>c</code> will create a new window.<br>When there are multiple windows, <code>Ctrl + D</code> will close the current window without closing the session.<br><code>[number]</code> will jump to the window indexed by that number, and it can have only one digit. However, <code>&#39;</code> can enter any number and jump in.<br><code>,</code> can rename the current window, but the indices remain unchanfed. </p>
</li>
<li><p><code>&quot;</code> will split the current pane horizontally, while <code>%</code> will split vertically.<br>When there are multiple panes, <code>Ctrl + D</code> will now close the current pane without closing the window.<br>The arrow keys can switch between panes.<br><code>space</code> will change through different layouts.<br><code>z</code> will zoom the current pane to take all the space or go back to multiple panes at the same time. </p>
</li>
<li><p><code>s</code> will allow us to see and select all the sessions, widowns and panes.<br><code>Right arraw</code> can expand sessions with multiple widows or windows with multiple panes. <code>Left arraw</code> can collapse them.<br><code>Enter</code> can select a session, window, or a pane. </p>
</li>
<li><p><code>f</code> can find windows according to their name, not index. When none is matching, all will show up. </p>
</li>
</ol>
<h1 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h1><ol>
<li><p>Log in ssh with <code>ssh [username]@[IP address]</code>. The <code>IP address</code> can be either an IP or a DNS. Log out with <code>logout</code> in the shell in server. </p>
</li>
<li><p><code>ssh</code> can have a second argument which is the command we want to perform on the server. It will return to our local shell immediately, and print the contents in the server. </p>
</li>
<li><p>SSH Key can allow us to log in a server without enter the password.<br>First, we need to generate the SSH Key locally with <code>ssh-keygen</code>. That command will create two files in <code>~/.ssh/</code>, and one of them ends with <code>.pub</code>. The <code>.pub</code> file contains the public key we just generated, and copy it into the <code>~./ssh/authorized_keys</code> in server.<br>If we opened passphrase in SSH Key, it can protect our private key, but we still need to type it when we try to use the private key. <code>ssh-copy-id [user]@[IP address]</code> can skip passphrase in this computer. </p>
</li>
<li><p><code>scp [from address] [to address]</code></p>
</li>
<li><p>We can set configurations of hosts in <code>~/.ssh/config</code>. Then we can log in with <code>ssh [host alias]</code></p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Host [host alias]</span><br><span class="line"> User [<span class="built_in">log</span> in user name]</span><br><span class="line"> HostName [<span class="built_in">log</span> in IP address]</span><br><span class="line"> IdentityFile [private key address]</span><br><span class="line"> RemoteForward []</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiyunZhang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">221</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liyun Zhang</span>
</div>
<div class="BbeiAn-info">
       浙ICP备 -
    <a target="_blank" href="http://www.miitbeian.gov.cn/" style="color:#ffffff"  rel="nofollow">19047088号-1</a> <!--a标签中增加nofollow属性，避免爬虫出站。-->|
    <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001835" style="color:#ffffff;text-decoration:none;padding-left:30px;background:url(https://s1.ax1x.com/2018/09/29/ilmwIH.png) no-repeat left center" rel="nofollow">浙公网安备 33011802001835号</a>      <!--这里将图标作为了背景，以使得能和后面的文字在同一行-->

</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
