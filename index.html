<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://http://www.laughingtree.cn').hostname,
    root: '/',
    scheme: 'Mist',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":true,"show_result":true,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="LaughingTree">
<meta property="og:url" content="http://http//www.laughingtree.cn/index.html">
<meta property="og:site_name" content="LaughingTree">
<meta property="article:author" content="LiyunZhang">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://http//www.laughingtree.cn/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>LaughingTree</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">LaughingTree</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags<span class="badge">29</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories<span class="badge">33</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="Searching..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/17/14-Fine-grained-Synchronization-and-Lock-free-Programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/17/14-Fine-grained-Synchronization-and-Lock-free-Programming/" class="post-title-link" itemprop="url">14. Fine-grained Synchronization and Lock-free Programming</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-17 11:22:54" itemprop="dateCreated datePublished" datetime="2022-07-17T11:22:54+08:00">2022-07-17</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-18 17:00:51" itemprop="dateModified" datetime="2022-07-18T17:00:51+08:00">2022-07-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Fine-grained-synchronization-and-fine-grained-lock"><a href="#Fine-grained-synchronization-and-fine-grained-lock" class="headerlink" title="Fine-grained synchronization and fine-grained lock"></a>Fine-grained synchronization and fine-grained lock</h1><ol>
<li>Data structures are often larger than a single memory location. One solution is to protect the structure with a single lock.<br>It is relatively simple to implement correct mutual exclusion for data structure operations. But the operations on the data structure are serialized, which may limit parallel application performance. </li>
<li>Another solution is “hand-over-hand” locking. We set a lock for each elements in the data structure. Each threads only keeps as less as lock they need, and every time a thread move forward, it unlock the locks they don’t need any more and lock the locks they need now. </li>
<li>The goal of fine-grained lock is to enable parallelism in data structure operations by Reducing contention for global data structure lock.<br>It is tricky to ensure correctness (how to determine when mutual exclusion is required, how to avoid deadlock or livelock). </li>
<li>It has an overhead of taking a lock each traversal step. There are extra instructions and traversal now involves memory writes. Also, it has extra storage cost, namely a lock per node. </li>
<li>C++11 has an <code>atomic&lt;T&gt;</code>. It provides atomic read, write, read-modify-write of entire objects. Atomicity may be implemented by mutex or efficiently by processor-supported atomic instructions if <code>T</code> is a basic type.<br>It also provides memory ordering semantics for operations before and after atomic operations. </li>
</ol>
<h1 id="Lock-free-programming"><a href="#Lock-free-programming" class="headerlink" title="Lock-free programming"></a>Lock-free programming</h1><ol>
<li>Single reader, single writer bounded queue: Only two threads (one producer, one consumer) accessing queue at the same time<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Queue</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> data[N];</span><br><span class="line">  <span class="keyword">unsigned</span> head; <span class="comment">// head of queue</span></span><br><span class="line">  <span class="keyword">unsigned</span> tail; <span class="comment">// next free element</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(Queue* q)</span> </span>&#123;</span><br><span class="line">  q-&gt;head = q-&gt;tail = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// return false if queue is full</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">push</span><span class="params">(Queue* q, <span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// queue is full if tail is element before head</span></span><br><span class="line">  <span class="keyword">if</span> (q-&gt;tail == MOD_N(q-&gt;head - <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">  q.data[q-&gt;tail] = value;</span><br><span class="line">  q-&gt;tail = MOD_N(q-&gt;tail + <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// returns false if queue is empty</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">pop</span><span class="params">(Queue* q, <span class="keyword">int</span>* value)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// if not empty</span></span><br><span class="line">  <span class="keyword">if</span> (q-&gt;head != q-&gt;tail) &#123;</span><br><span class="line">    *value = q-&gt;data[q-&gt;head];</span><br><span class="line">    q-&gt;head = MOD_N(q-&gt;head + <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p>Single reader, single writer unbounded queue: Only push modifies <code>tail</code> and <code>reclaim</code>; only pop modifies <code>head</code>. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  Node* next;</span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Queue</span> &#123;</span></span><br><span class="line">  Node* head;    <span class="comment">// the element before head of queue</span></span><br><span class="line">  Node* tail;    <span class="comment">// the last element added</span></span><br><span class="line">  Node* reclaim; <span class="comment">// the head of undeleted nodes</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(Queue* q)</span> </span>&#123;</span><br><span class="line">  q-&gt;head = q-&gt;tail = q-&gt;reclaim = <span class="keyword">new</span> Node;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(Queue* q, <span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line">  Node* n = <span class="keyword">new</span> Node;</span><br><span class="line">  n-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">  n-&gt;value = value;</span><br><span class="line">  q-&gt;tail-&gt;next = n;</span><br><span class="line">  q-&gt;tail = q-&gt;tail-&gt;next;</span><br><span class="line">  <span class="comment">// delete all nodes between reclaim and head</span></span><br><span class="line">  <span class="keyword">while</span> (q-&gt;reclaim != q-&gt;head) &#123;</span><br><span class="line">    Node* tmp = q-&gt;reclaim;</span><br><span class="line">    q-&gt;reclaim = q-&gt;reclaim-&gt;next;</span><br><span class="line">    <span class="keyword">delete</span> tmp;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// returns false if queue is empty</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">pop</span><span class="params">(Queue* q, <span class="keyword">int</span>* value)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (q-&gt;head != q-&gt;tail) &#123;</span><br><span class="line">    *value = q-&gt;head-&gt;next-&gt;value;</span><br><span class="line">    q-&gt;head = q-&gt;head-&gt;next;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Lock-free stack: the main idea is that as long as no other thread has modified the stack, a thread’s modification can proceed. The <code>compare_and_swap</code> operation is atomic, but doesn’t need to hold any other lock on data structure. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  Node* next;</span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Stack</span> &#123;</span></span><br><span class="line">  Node* top;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(Stack* s)</span> </span>&#123;</span><br><span class="line">  s-&gt;top = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(Stack* s, Node* n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    Node* old_top = s-&gt;top;</span><br><span class="line">    n-&gt;next = old_top;</span><br><span class="line">    <span class="comment">// Check whether the current top is the old top, </span></span><br><span class="line">    <span class="comment">// if true, set the current top to n; or get a new top</span></span><br><span class="line">    <span class="keyword">if</span> (compare_and_swap(&amp;s-&gt;top, old_top, n) == old_top)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Node* <span class="title">pop</span><span class="params">(Stack* s)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    Node* old_top = s-&gt;top;</span><br><span class="line">    <span class="keyword">if</span> (old_top == <span class="literal">NULL</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    Node* new_top = old_top-&gt;next;</span><br><span class="line">    <span class="comment">// if the top is still the old top, return old top and set to new top</span></span><br><span class="line">    <span class="comment">// or get a new top</span></span><br><span class="line">    <span class="keyword">if</span> (compare_and_swap(&amp;s-&gt;top, old_top, new_top) == old_top)</span><br><span class="line">      <span class="keyword">return</span> old_top;  <span class="comment">// Assume that consumer then recycles old_top</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>The above push operation is fine, but the pop operation may have some error.<br>After thread0 stored the <code>old_top</code>, if thread1 pops the <code>old_top</code> out, modifies the stack, and pushes the <code>old_top</code> into stack again, then the <code>compare_and_swap</code> operation of thread0 will pass. Namely, thread0 cannot realize that the stack has already been modified. </p>
</li>
<li><p>One solution is adding a <code>pop_counter</code> to check whether other pop operations happened. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  Node* next;</span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Stack</span> &#123;</span></span><br><span class="line">  Node* top;</span><br><span class="line">  <span class="keyword">int</span> pop_count;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(Stack* s)</span> </span>&#123;</span><br><span class="line">  s-&gt;top = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(Stack* s, Node* n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    Node* old_top = s-&gt;top;</span><br><span class="line">    n-&gt;next = old_top;</span><br><span class="line">    <span class="keyword">if</span> (compare_and_swap(&amp;s-&gt;top, old_top, n) == old_top)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Node* <span class="title">pop</span><span class="params">(Stack* s)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">int</span> pop_count = s-&gt;pop_count;</span><br><span class="line">    Node* top = s-&gt;top;</span><br><span class="line">    <span class="keyword">if</span> (top == <span class="literal">NULL</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    Node* new_top = top-&gt;next;</span><br><span class="line">    <span class="keyword">if</span> (double_compare_and_swap(&amp;s-&gt;top, top, new_top,</span><br><span class="line">                                &amp;s-&gt;pop_count, pop_count, pop_count+<span class="number">1</span>))</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">return</span> top;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Another solution of ABA problem is hazard pointers. The ABA problem is caused by the reuse of the <code>old_top</code>. We can use the hazard pointers to track all <code>old_top</code>s, and ode cannot be recycled or reused if matches any hazard pointer. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  Node* next;</span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Stack</span> &#123;</span></span><br><span class="line">  Node* top;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Node *hazard[NUM_THREADS];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">init</span><span class="params">(Stack* s)</span> </span>&#123;</span><br><span class="line">  s-&gt;top = <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(Stack* s, Node* n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    Node* old_top = s-&gt;top;</span><br><span class="line">    n-&gt;next = old_top;</span><br><span class="line">    <span class="keyword">if</span> (compare_and_swap(&amp;s-&gt;top, old_top, n) == old_top)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Node* <span class="title">pop</span><span class="params">(Stack* s)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    hazard[t] = s-&gt;top;</span><br><span class="line">    Node* top = hazard[t];</span><br><span class="line">    <span class="keyword">if</span> (top == <span class="literal">NULL</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    Node* new_top = top-&gt;next;</span><br><span class="line">    <span class="keyword">if</span> (compare_and_swap(&amp;s-&gt;top, top, new_top))</span><br><span class="line">      <span class="keyword">return</span> top;  <span class="comment">// Caller must clear hazard[t] when it’s done with top</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>Lock-free linked list insertion: assumes the only operation on the list is insert. Supporting lock-free deletion significantly complicates data-structure. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> value;</span><br><span class="line">  Node* next;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">List</span> &#123;</span></span><br><span class="line">  Node* head;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// insert new node after specified node</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert_after</span><span class="params">(List* <span class="built_in">list</span>, Node* after, <span class="keyword">int</span> value)</span> </span>&#123;</span><br><span class="line">  Node* n = <span class="keyword">new</span> Node;</span><br><span class="line">  n-&gt;value = value;</span><br><span class="line">  <span class="comment">// assume case of insert into empty list handled</span></span><br><span class="line">  <span class="comment">// here (keep code on slide simple for class discussion)</span></span><br><span class="line">  Node* prev = <span class="built_in">list</span>-&gt;head;</span><br><span class="line">  <span class="keyword">while</span> (prev-&gt;next) &#123;</span><br><span class="line">    <span class="keyword">if</span> (prev == after) &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">        Node* old_next = prev-&gt;next;</span><br><span class="line">        n-&gt;next = old_next;</span><br><span class="line">        <span class="keyword">if</span> (compare_and_swap(&amp;prev-&gt;next, old_next, n) == old_next)</span><br><span class="line">          <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    prev = prev-&gt;next;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>If only your program is using the machine, well written code with locks can be as fast (or faster) than lock-free code.<br>But there are situations where code with locks can suffer from tricky performance problems. Like multi-programmed situations where page faults, pre-emption, etc. can occur while thread is in a critical section</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/16/13-Implementing-Synchronization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/16/13-Implementing-Synchronization/" class="post-title-link" itemprop="url">13. Implementing Synchronization</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-16 11:05:13" itemprop="dateCreated datePublished" datetime="2022-07-16T11:05:13+08:00">2022-07-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-17 11:21:32" itemprop="dateModified" datetime="2022-07-17T11:21:32+08:00">2022-07-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Implementing-locks"><a href="#Implementing-locks" class="headerlink" title="Implementing locks"></a>Implementing locks</h1><ol>
<li>Three phases of a synchronization event：<br>Acquire method: How a thread attempts to gain access to protected resource?<br>Waiting algorithm: How a thread waits for access to be granted to shared resource?<br>Release method: How thread enables other threads to gain resource when its work in the synchronized region is complete? </li>
<li>Busy waiting (spinning): <code>while (condition X not true) ;</code></li>
<li>Blocking synchronization: <code>if (condition X not true) block until true;</code><br>If progress cannot be made because a resource cannot be acquired, it is desirable to free up execution resources for another thread and preempt the running thread.</li>
<li>Busy-waiting can be preferable to blocking if scheduling overhead is larger than expected wait time or processor’s resources are not needed for other tasks.<br>The later situation is often the case in a parallel program since we usually don’t oversubscribe a system when running a performance-critical parallel app</li>
<li>Desirable lock performance characteristics:<br><strong>Low latency</strong>: If lock is free and no other processors are trying to acquire it, a processor should be able to acquire the lock quickly<br><strong>Low interconnect traffic</strong>: If all processors are trying to acquire lock at once, they should acquire the lock in succession with as little traffic as possible<br><strong>Scalability</strong>: Latency / traffic should scale reasonably with number of processors<br><strong>Low storage cost</strong><br><strong>Fairness</strong>: Avoid starvation or substantial unfairness. One ideal is that processors should acquire lock in the order they request access to it</li>
</ol>
<h2 id="Test-and-set-lock"><a href="#Test-and-set-lock" class="headerlink" title="Test-and-set lock"></a>Test-and-set lock</h2><h3 id="Simple-test-and-set-lock"><a href="#Simple-test-and-set-lock" class="headerlink" title="Simple test-and-set lock"></a>Simple test-and-set lock</h3><ol>
<li>The following spin lock has data race because LOAD-TEST-STORE is not atomic. <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Lock</span><br><span class="line">ld   R0, mem[addr]  &#x2F;&#x2F; load word into R0</span><br><span class="line">cmp  R0, #0          &#x2F;&#x2F; compare R0 to 0</span><br><span class="line">bnz  lock            &#x2F;&#x2F; if nonzero jump to top</span><br><span class="line">st   mem[addr], #1  &#x2F;&#x2F; set lock to 1</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Unlock</span><br><span class="line">st   mem[addr], #0  &#x2F;&#x2F; set lock to 0</span><br></pre></td></tr></table></figure></li>
<li>So we need an atomic test-and-set instruction<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ts R0, mem[addr]  &#x2F;&#x2F; load mem[addr] into R0</span><br><span class="line">                  &#x2F;&#x2F; if mem[addr] is 0, set mem[addr] to 1</span><br><span class="line">                  </span><br><span class="line">&#x2F;&#x2F; Lock</span><br><span class="line">ts   R0, mem[addr]  &#x2F;&#x2F; load word into R0</span><br><span class="line">bnz  R0, lock        &#x2F;&#x2F; if nonzero jump to top</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Unlock</span><br><span class="line">st   mem[addr], #0  &#x2F;&#x2F; store 0 to address</span><br></pre></td></tr></table></figure></li>
<li>Everytime a processor execute the <code>ts</code> instruction will send a BusRdX signal and invalidate the lock in all other processor’s cache. When there are many processors trying to execute on the same lock, the coherence traffic may be heavy.<br>This lock generates one invalidation per waiting processor per test. </li>
<li>Bus contention increases amount of time to transfer lock since lock holder must wait to acquire bus to release. Bus contention also slows down execution of critical section. </li>
<li>In x86, we can do the atomic compare and exchange by <code>lock cmpxchg src, dst</code> instruction. The <code>lock</code> prefix makes the operation atomic. The logic of the instruction is as followed: <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (dst == %eax)  <span class="comment">// eax is the x86 accumulator register</span></span><br><span class="line">  ZF = <span class="number">1</span>          <span class="comment">// ZF is a flag registor</span></span><br><span class="line">dst = src <span class="keyword">else</span></span><br><span class="line">  ZF = <span class="number">0</span></span><br><span class="line">  %eax = dst</span><br></pre></td></tr></table></figure></li>
<li>Simple test-and-set lock has low latency (under low contention), high traffic, poor scaling, low storage cost (one int), no provisions for fairness. </li>
</ol>
<h3 id="Test-and-set-lock-with-back-off"><a href="#Test-and-set-lock-with-back-off" class="headerlink" title="Test-and-set lock with back off"></a>Test-and-set lock with back off</h3><ol>
<li><p>Upon failure to acquire lock, delay for awhile before retrying. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Lock</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">int</span>* lock)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> amount = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (test_and_set(lock) == <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    delay(amount);</span><br><span class="line">    amount *= <span class="number">2</span>;</span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>It has the same uncontended latency as test-and-set, but potentially higher latency under contention due to that the waiting processor may be still in delaying even when the lock is available. </p>
</li>
<li><p>It Generates less traffic than test-and-set since it is not continually attempting to acquire lock. It improves scalability due to less traffic. </p>
</li>
<li><p>Storage cost unchanged (still one int for lock)</p>
</li>
<li><p>Exponential back-off can cause severe unfairness. Newer requesters back off for shorter intervals</p>
</li>
</ol>
<h3 id="Test-and-test-and-set-lock"><a href="#Test-and-test-and-set-lock" class="headerlink" title="Test-and-test-and-set lock"></a>Test-and-test-and-set lock</h3><ol>
<li><p>To prevent the coherence traffic problem, we can test first and do the test-and-set only if the earlier test has passed. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Lock</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">int</span>* lock)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (<span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">while</span> (*lock != <span class="number">0</span>) ;</span><br><span class="line">    <span class="keyword">if</span> (test_and_set(lock) == <span class="number">0</span>)</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Unlock</span><span class="params">(<span class="keyword">volatile</span> <span class="keyword">int</span>* lock)</span> </span>&#123;</span><br><span class="line">  *lock = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>This lock has slightly higher latency than test-and-set in uncontended case, but generates much less interconnect traffic. Only One invalidation is generated per waiting processor per lock release.<br>Namely, only $O(P)$ invalidation and $O(P^2)$ interconnect traffic. </p>
</li>
<li><p>It is more scalable due to less traffic, storage cost unchanged (still one int), and still no provisions for fairness. </p>
</li>
</ol>
<h2 id="Ticket-lock"><a href="#Ticket-lock" class="headerlink" title="Ticket lock"></a>Ticket lock</h2><ol>
<li>The test-and-set style locks cannot provide for fairness because all waiting processors attempt to acquire lock using test-and-set upon release. </li>
<li>We can assign each thread a number when they try to acquire the lock. And everytime, we give the lock to the waiting thread with smallest number. <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">lock</span> &#123;</span></span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">int</span> next_ticket;</span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">int</span> now_serving;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Lock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> my_ticket = atomic_increment(&amp;lock-&gt;next_ticket);  <span class="comment">// take a “ticket”</span></span><br><span class="line">  <span class="keyword">while</span> (my_ticket != lock-&gt;now_serving);                <span class="comment">//wait for number</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">unlock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  lock-&gt;now_serving++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>There is no atomic operation needed to acquire the lock. Only a read is needed when acquiring the lock, and write only happens when the lock is released. So only one invalidation is generated per lock release, namely $O(P)$ interconnect traffic. </li>
</ol>
<h2 id="Array-based-lock"><a href="#Array-based-lock" class="headerlink" title="Array-based lock"></a>Array-based lock</h2><ol>
<li><p>Each processor spins on a different memory address. And utilizes atomic operation to assign address on attempt to acquire.<br>If there are two barriers, after some threads passed the first barrier, they might set the <code>flag</code> to 0 even if some other threads haven’t passed the first barrier yet causing those slower threads waiting. </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">lock</span> &#123;</span></span><br><span class="line">  <span class="keyword">volatile</span> padded_int status[P];  <span class="comment">// padded to keep off same cache line</span></span><br><span class="line">  <span class="keyword">volatile</span> <span class="keyword">int</span> head;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> my_element;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Lock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  my_element = atomic_circ_increment(&amp;lock-&gt;head);  <span class="comment">// assume modular increment </span></span><br><span class="line">  <span class="keyword">while</span> (lock-&gt;status[my_element] == <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">unlock</span><span class="params">(lock* lock)</span> </span>&#123;</span><br><span class="line">  lock-&gt;status[my_element] = <span class="number">1</span>;</span><br><span class="line">  lock-&gt;status[circ_next(my_element)] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>The lock only requires $O(1)$ interconnect traffic per release, but requires space linear in $O(P)$. </p>
</li>
<li>The atomic circular increment is a more complex operation. So the lock has a higher overhead. </li>
<li>Queue-based Lock (MCS lock): Create a queue of waiters. Each thread allocates a local space on which to wait. </li>
</ol>
<h1 id="Implementing-barrier"><a href="#Implementing-barrier" class="headerlink" title="Implementing barrier"></a>Implementing barrier</h1><ol>
<li>The following code uses <code>counter</code> to count how many threads have hit the barrier. The last thread hit the barrier wil release all of them. <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Barrier_t</span> &#123;</span></span><br><span class="line">   LOCK lock;</span><br><span class="line">  <span class="keyword">int</span> counter;  <span class="comment">// initialize to 0</span></span><br><span class="line">  <span class="keyword">int</span> flag;      <span class="comment">// the flag field should probably be padded to </span></span><br><span class="line">                <span class="comment">// sit on its own cache line. </span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// barrier for p processors</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Barrier</span><span class="params">(Barrier_t* b, <span class="keyword">int</span> p)</span> </span>&#123;</span><br><span class="line">  lock(b-&gt;lock);</span><br><span class="line">  <span class="keyword">if</span> (b-&gt;counter == <span class="number">0</span>) &#123;</span><br><span class="line">    b-&gt;flag = <span class="number">0</span>; <span class="comment">// first thread arriving at barrier clears flag </span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> num_arrived = ++(b-&gt;counter);</span><br><span class="line">  unlock(b-&gt;lock);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (num_arrived == p) &#123; <span class="comment">// last arriver sets flag b-&gt;counter = 0;</span></span><br><span class="line">    b-&gt;flag = <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (b-&gt;flag == <span class="number">0</span>); <span class="comment">// wait for flag</span></span><br><span class="line">  &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>To solve the problem, we should wait for all processes to leave first barrier, before clearing flag for entry into the second<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Centralized barrier</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Barrier_t</span> &#123;</span></span><br><span class="line">  LOCK lock;</span><br><span class="line">  <span class="keyword">int</span> arrive_counter;  <span class="comment">// initialize to 0 (number of threads that have arrived)</span></span><br><span class="line">  <span class="keyword">int</span> leave_counter;  <span class="comment">// initialize to P (number of threads that have left barrier)</span></span><br><span class="line">  <span class="keyword">int</span> flag;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// barrier for p processors</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Barrier</span><span class="params">(Barrier_t* b, <span class="keyword">int</span> p)</span> </span>&#123;</span><br><span class="line">  lock(b-&gt;lock);</span><br><span class="line">  <span class="keyword">if</span> (b-&gt;arrive_counter == <span class="number">0</span>) &#123;   <span class="comment">// if first to arrive...</span></span><br><span class="line">    <span class="keyword">if</span> (b-&gt;leave_counter == P) &#123;  <span class="comment">// check to make sure no other threads “still in barrier”</span></span><br><span class="line">      b-&gt;flag = <span class="number">0</span>;               <span class="comment">// first arriving thread clears flag</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      unlock(lock);</span><br><span class="line">      <span class="keyword">while</span> (b-&gt;leave_counter != P);  <span class="comment">// wait for all threads to leave before clearing</span></span><br><span class="line">      lock(lock);</span><br><span class="line">      b-&gt;flag = <span class="number">0</span>;                <span class="comment">// first arriving thread clears flag</span></span><br><span class="line">    &#125; </span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> num_arrived = ++(b-&gt;arrive_counter);</span><br><span class="line">  unlock(b-&gt;lock);</span><br><span class="line">  <span class="keyword">if</span> (num_arrived == p) &#123;  <span class="comment">// last arriver sets flag</span></span><br><span class="line">    b-&gt;arrive_counter = <span class="number">0</span>;</span><br><span class="line">    b-&gt;leave_counter = <span class="number">1</span>;</span><br><span class="line">    b-&gt;flag = <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (b-&gt;flag == <span class="number">0</span>);  <span class="comment">// wait for flag</span></span><br><span class="line">    lock(b-&gt;lock);</span><br><span class="line">    b-&gt;leave_counter++;</span><br><span class="line">    unlock(b-&gt;lock);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>We can save one variable by sense reversal. Processors wait for flag to be equal to local sense. <figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Barrier_t</span> &#123;</span></span><br><span class="line">  LOCK lock;</span><br><span class="line">  <span class="keyword">int</span> counter; <span class="comment">// initialize to 0</span></span><br><span class="line">  <span class="keyword">int</span> flag; <span class="comment">// initialize to 0</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> local_sense = <span class="number">0</span>;  <span class="comment">// private per processor</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// barrier for p processors</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Barrier</span><span class="params">(Barrier_t* b, <span class="keyword">int</span> p)</span> </span>&#123;</span><br><span class="line">  local_sense = (local_sense == <span class="number">0</span>) ? <span class="number">1</span> : <span class="number">0</span>; lock(b-&gt;lock);</span><br><span class="line">  <span class="keyword">int</span> num_arrived = ++(b-&gt;counter);</span><br><span class="line">  <span class="keyword">if</span> (b-&gt;counter == p) &#123; <span class="comment">// last arriver sets flag</span></span><br><span class="line">    unlock(b-&gt;lock);</span><br><span class="line">    b-&gt;counter = <span class="number">0</span>;</span><br><span class="line">    b-&gt;flag = local_sense;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    unlock(b-&gt;lock);</span><br><span class="line">    <span class="keyword">while</span> (b-&gt;flag != local_sense); <span class="comment">// wait for flag</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li>There are $O(P)$ traffic on interconnect per barrier.<br>All threads have $2P$ write transactions to obtain barrier lock and update counter. $O(P)$ traffic assuming lock acquisition is implemented in $O(1)$ manner<br>Last thread has $2$ write transactions to write to the flag and reset the counter. $O(P)$ traffic since there are many sharers of the flag<br>$P-1$ transactions to read updated flag</li>
<li>In centralized barrier, all threads share single barrier lock and counter, which causes high contention. </li>
<li>Combining trees make better use of parallelism in interconnect topologies, which has $logP$ span (latency). This strategy makes less sense on a bus where all traffic still serialized on single shared bus.<br>Barrier acquire: when processor arrives at barrier, performs increment of parent counter. Process recurses to root.<br>Barrier release: beginning from root, notify children of release</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/13/12-Interconnection-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/13/12-Interconnection-Network/" class="post-title-link" itemprop="url">12. Interconnection Network</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-13 10:50:56" itemprop="dateCreated datePublished" datetime="2022-07-13T10:50:56+08:00">2022-07-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-16 11:06:00" itemprop="dateModified" datetime="2022-07-16T11:06:00+08:00">2022-07-16</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Interconnection"><a href="#Interconnection" class="headerlink" title="Interconnection"></a>Interconnection</h1><ol>
<li>All parallel processors are connected and form an interconnection network. </li>
<li>The interconnection network are used to connect processor cores with other cores, processors and memories, processor cores and caches, caches and caches, I/O devices. </li>
<li>The design of the interconnection network has an important impact on system scalability (How large of a system can be built? How easy is it to add more nodes?), system performance and energy efficiency (How fast can cores, caches, memory communicate? How long is latency to memory? How much energy is spent on communication?)</li>
</ol>
<h2 id="Terminology"><a href="#Terminology" class="headerlink" title="Terminology"></a>Terminology</h2><ol>
<li><strong>Network node</strong>: a network endpoint connected to a router/switch, like the processor, the cache controller, or the memory controller</li>
<li><strong>Network interface</strong>: Connects nodes to the network</li>
<li><strong>Switch/router</strong>: Connects a fixed number of input links to a fixed number of output links</li>
<li><strong>Link</strong>: A bundle of wires carrying a signal<br><img src="/img/12-Interconnection-Network/1.png" width="40%"></li>
</ol>
<h2 id="Design-issues"><a href="#Design-issues" class="headerlink" title="Design issues"></a>Design issues</h2><ol>
<li><strong>Topology</strong>: How switches are connected via links. Affects routing, throughput, latency, complexity/cost of implementation. </li>
<li><strong>Routing</strong>: How a message gets from its source to its destination in the network. Can be static (messages take a predetermined path) or adaptive based on load. </li>
<li><strong>Buffering and flow control</strong>: What data are stored in the network? packets, partial packets? etc. How does the network manage buffer space?</li>
</ol>
<h2 id="Properties-of-interconnect-topology"><a href="#Properties-of-interconnect-topology" class="headerlink" title="Properties of interconnect topology"></a>Properties of interconnect topology</h2><ol>
<li><strong>Routing distance</strong>: Number of links (“hops”) along a route between two nodes</li>
<li><strong>Diameter</strong>: the maximum routing distance</li>
<li><strong>Average distance</strong>: average routing distance over all valid routes</li>
<li><strong>Direct network</strong>: The switches and nodes are one in the same. The logic of the switch is built into the node itself. </li>
<li><strong>Indirect network</strong>: The switches are distinct from the nodes that form a chain from one node to the other. </li>
<li><strong>Blocking or non-blocking</strong>: If connecting any pairing of node simultaneously won’t cause conflict (using the same link or the same switch, assuming that one switch can only handle on message at one time), the network is non-blocking. Otherwise, it is blocking. </li>
<li><strong>Bisection bandwidth</strong>: A measure of how much connectivity in the network. If we cut the network in half, it is the connection between those two halves, namely the sum bandwidth of all severed links.<br>The low bisection bandwidth will be the choke point of the network. </li>
<li>Latency increases with load (throughput).<br>The topology, routing algorithm and flow control each has their own min latency. The zero load or idle latency is the sum of the three min latencies.<br>Also, the topology, routing algorithm and flow control each has their own throughput limit. The overall throughput limit is the min of the three limits. </li>
</ol>
<h1 id="Interconnect-topologies"><a href="#Interconnect-topologies" class="headerlink" title="Interconnect topologies"></a>Interconnect topologies</h1><h2 id="Bus"><a href="#Bus" class="headerlink" title="Bus"></a>Bus</h2><ol>
<li><p>Physically, a bus is a wire. But from graph point of view, a bus is a switch. </p>
</li>
<li><p>It is simple to design. It costs effective for a small number of nodes. It is easy to implement coherence via snooping</p>
</li>
<li><p>Contention: all nodes contend for shared bus</p>
</li>
<li><p>Limited bandwidth: all nodes communicate over same wires, and only one communication is allowed at a time. </p>
</li>
<li><p>There has a scalibility problem. It is expensive to drive wires across the whole chips. There is quite a lot of power in driving signals on the bus. </p>
<p><img src="/img/12-Interconnection-Network/2.png" width="40%"></p>
</li>
</ol>
<h2 id="Crossbar"><a href="#Crossbar" class="headerlink" title="Crossbar"></a>Crossbar</h2><ol>
<li><p>Every node is connected to every other node by a direct connection. The network is non-blocking and indirect (the network is indirect, but the nodes are connected directly). </p>
</li>
<li><p>It has $O(1)$ latency and high bandwidth. </p>
</li>
<li><p>It also has a scalability problem since $O(N^2)$ switches are required. Also it has high cost and is difficult to arbitrate at scale. </p>
</li>
<li><p>Crossbars were used in recent multi-core processing from Oracle. In a multi-core chip, the crossbar (CCX) occupies about the same chip area as a core. </p>
<p><img src="/img/12-Interconnection-Network/3.png" width="40%"></p>
</li>
</ol>
<h2 id="Ring"><a href="#Ring" class="headerlink" title="Ring"></a>Ring</h2><ol>
<li><p>It lets the message to circle around. </p>
</li>
<li><p>It is simple enough and quite cheap with only $O(N)$ cost. </p>
</li>
<li><p>But the latency is $O(N)$, which is very high. And the bisection bandwidth remains constant as nodes are added, which will cause the scalibility problem. </p>
</li>
<li><p>It is used in recent Intel architectures, Core i7, and in IBM CELL Broadband Engine. This is usually used on the scale of 4 or 8 elements on the ring. </p>
</li>
<li><p>The Intel’s ring interconnect has four rings (request, snoop, ack, 32 bytes of data) and six interconnect nodes (four “slices” of L3 cache, system agent, graphics). Each bank of L3 connected to ring bus twice. </p>
<p><img src="/img/12-Interconnection-Network/4.png" width="40%"></p>
</li>
</ol>
<h2 id="Mesh"><a href="#Mesh" class="headerlink" title="Mesh"></a>Mesh</h2><ol>
<li><p>This is a direct network. It echoes locality in grid-based applications. </p>
</li>
<li><p>The cost is $O(N)$, and the average latency is $O(\sqrt{N})$. </p>
</li>
<li><p>It is easy to lay out on chip since all links have a fixed-length. </p>
</li>
<li><p>Path diversity: many ways for message to travel from one node to another</p>
<p><img src="/img/12-Interconnection-Network/5.png" width="40%"></p>
</li>
</ol>
<h2 id="Torus"><a href="#Torus" class="headerlink" title="Torus"></a>Torus</h2><ol>
<li><p>Characteristics of node in mesh topology are different based on whether node is near edge or middle of network. Torus topology introduces new links to avoid this problem. </p>
</li>
<li><p>In torus topology, each row or each column forms a ring by adding a new link to connect the two nodes on the edge. </p>
</li>
<li><p>Its cost is still $O(N)$, but higher than 2D mesh. It also has higher path diversity and bisection bandwidth than mesh. </p>
</li>
<li><p>However, it has higher complexity and is difficult to layout on chip since its unequal link lengths. </p>
</li>
<li><p>Folded torus interleaving rows and columns to eliminate need for long connections. All connections are doubled in length. </p>
<p><img src="/img/12-Interconnection-Network/6.png" width="40%"></p>
</li>
</ol>
<h2 id="Tree"><a href="#Tree" class="headerlink" title="Tree"></a>Tree</h2><ol>
<li><p>This is a planar, hierarchical topology. Like mesh/torus, it performs good when traffic has locality. </p>
</li>
<li><p>The latency is $O(logN)$</p>
</li>
<li><p>In tree routing, if the signal need to go upward, there is only one option. If the signal need to go downward, we can use $0$ to mark go left while $1$ to mark go right.<br>Everytime, the signal will go upward first until have a common ancestor, then it will go downward.<br>If the tree is a complete tree and we assign numbers to the nodes accroding to their inorder traversal starting from zero, then the way to each node from root node is the binary code of the number. </p>
</li>
<li><p>Fat tree increases bandwidth between nodes as move upward. It can alleviate root bandwidth problem with higher bandwidth links near root. </p>
</li>
<li><p>The number of wires is the same to the height of the subtree. So the bisection bandwidth of fat tree is $O(N)$. </p>
</li>
<li><p>The fat tree routing is similar to tree routing, but randomly choose when multiple links possible. </p>
</li>
<li><p>Constant-width fat tree (folded clos network): All nodes fixed degree, which makes the hardware design simpler. This is used in Infiniband networks. </p>
<p><img src="/img/12-Interconnection-Network/7.png" width="30%"><img src="/img/12-Interconnection-Network/8.png" width="30%"><img src="/img/12-Interconnection-Network/9.png" width="30%"></p>
</li>
</ol>
<h2 id="Hypercube"><a href="#Hypercube" class="headerlink" title="Hypercube"></a>Hypercube</h2><ol>
<li><p>It has a low latency of $O(logN)$. Its number of links is $O(NlogN)$. </p>
</li>
<li><p>6D hypercube used in 64-core Cosmic Cube computer developed at Caltech in the 80s. </p>
</li>
<li><p>If the address of two nodes only differs by one bit, then there is a link connecting them.<br>So is we want to go from address A to address B, we just do it one bit a time. </p>
</li>
<li><p>The problem is that we cannot pack more dimensions to 3D that exists. To implement a higher dimension cube, we need to put enough wires in to make it work. But as we scale up more and more, the wires becomes so much that doesn’t layout well. </p>
<p><img src="/img/12-Interconnection-Network/10.png" width="40%"></p>
</li>
</ol>
<h2 id="Multi-stage-logarithmic"><a href="#Multi-stage-logarithmic" class="headerlink" title="Multi-stage logarithmic"></a>Multi-stage logarithmic</h2><ol>
<li><p>It is an indirect network with multiple switches between terminals. </p>
</li>
<li><p>The cost is $O(NlogN)$ while the latency is $O(logN)$. </p>
</li>
<li><p>It has many variations: Omega, butterfly, Clos networks, etc. </p>
</li>
<li><p>In the topology shown below, each switch has two output wires. In the routing, if 0 stands for up and 1 for down, we can also routing according to the binary code of the number of targeting node.<br>Here up means that we will take the upper wire, and down means that we will take the lower wire. They don’t always means going up or going down. </p>
<p><img src="/img/12-Interconnection-Network/11.png" width="40%"></p>
</li>
</ol>
<h1 id="Buffering-and-flow-control"><a href="#Buffering-and-flow-control" class="headerlink" title="Buffering and flow control"></a>Buffering and flow control</h1><ol>
<li>Circuit switching sets up a full path (acquires all resources) between sender and receiver prior to sending a message. It has higher bandwidth transmission.<br>It has no per-packet link management overhead, but does incur overhead to set up/tear down path. And reserving links can result in low utilization. </li>
<li>Packet switching makes routing decisions per packet. It has an opportunity to use link for a packet whenever link is idle.<br>It has overhead due to dynamic switching logic during transmission, but no setup/tear down overhead. </li>
<li>The granularity of communication from larger to miner is message, packet and flit<br>Message is the unit of transfer between network clients, and can be transmitted using many packets.<br>Packet is the unit of transfer for network, and can be transmitted using multiple flits.<br>Flit (flow control digit) is a unit of flow control in the network. Packets broken into flits. </li>
<li>A packet consists of header, payload/body and tail.<br>Header contains routing and control information, and is at start of packet to router can start forwarding early.<br>Payload/body contains the data to be sent.<br>Tail contains control information, like error code, and is generally located at end of packet so it can be generated “on the way out”. </li>
<li>When two packets need to be routed onto the same outbound link at the same time, contention is occurred. There are three options buffer one packet and send it over link later, drop one packet, or reroute one packet (deflection). </li>
</ol>
<h2 id="Circuit-switched-routing"><a href="#Circuit-switched-routing" class="headerlink" title="Circuit-switched routing"></a>Circuit-switched routing</h2><ol>
<li>Main idea: pre-allocate all resources (links across multiple switches) along entire network path for a message (“setup a flow”)</li>
<li>Costs:<br>Needs setup phase (“probe”) to set up the path (and to tear it down and release<br>the resources when message complete)<br>Lower link utilization. Transmission of two messages cannot share same link (even if some resources on a preallocated path are no longer utilized during a transmission)</li>
<li>Benefits:<br>No contention during transmission due to preallocation, so no need for buffering<br>Arbitrary message sizes (once path is set up, send data until done)</li>
</ol>
<h2 id="Packet-based-flow-control"><a href="#Packet-based-flow-control" class="headerlink" title="Packet-based flow control"></a>Packet-based flow control</h2><ol>
<li>Store-and-forward: Packet copied entirely into network switch before moving to next node, which requires buffering for entire packet in each router</li>
<li>Flow control unit is an entire packet. Different packets from the same message can take different routes, but all data in a packet is transmitted over the same route</li>
<li>Store-and-forward has high per-packet latency. Its latency $=$ packet transmission time on link $\times$ network distance)</li>
<li>Cut-through flow control: Switch starts forwarding data on next link as soon as packet header is received, since header determines how much link bandwidth packet requires and where to route. </li>
<li>Cut-through flow control reduces transmission latency and reduces to store-and-forward under high contention. </li>
<li>The latency of cut-through flow control is header transmission time $\times$ network distance $+$ rest packet transmission time on one link $\simeq$ network distance $+$ number of packets. </li>
<li>The difference between store-and-forward and cut-through is whether we parallel the transmission of header and the rest of the packet. </li>
<li>In cut-through flow control, if output link is blocked (cannot transmit head), transmission of tail can continue. So the switch need to store more data before it can transmit and delete them, which requires switches to have buffering for entire packet, just like store-and-forward.<br>The worst case is that entire message is absorbed into a buffer in a switch, namely cut-through flow control degenerates to store-and-forward in this case. </li>
</ol>
<h2 id="Wormhole-flow-contorl"><a href="#Wormhole-flow-contorl" class="headerlink" title="Wormhole flow contorl"></a>Wormhole flow contorl</h2><ol>
<li>Routing information only in head flit, body flits follows head, and tail flit flows body. It looks like that all flits moves to their next switch simultaneously. If head flit blocks, rest of packet stops. </li>
<li>Its latency $=$ header transmission time $\times$</li>
<li>Head-of-line blocking problem: The route of the head flit of one packet is free, but blocked behind flit of another packet in buffer while that packet is blocked waiting for a busy link. </li>
<li>Virtual channel flow control: Multiplex multiple operations over single physical channel, and divide switch’s input buffer into multiple buffers sharing a single physical channel. </li>
<li>Virtual channel reduces head-of-line blocking.<br>It can break cyclic dependency of resources by ensuring requests and responses use different virtual channels to avoid deadlock.<br>Also, it provided quality-of-service guarantees. Some virtual channels have higher priority than others</li>
<li>“Escape” virtual channels: retain at least one virtual channel that uses deadlock-free routing</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/10/11-Memory-Consistency/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/10/11-Memory-Consistency/" class="post-title-link" itemprop="url">11. Memory Consistency</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-10 14:07:29" itemprop="dateCreated datePublished" datetime="2022-07-10T14:07:29+08:00">2022-07-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-11 15:24:08" itemprop="dateModified" datetime="2022-07-11T15:24:08+08:00">2022-07-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Consistency"><a href="#Consistency" class="headerlink" title="Consistency"></a>Consistency</h1><h2 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h2><ol>
<li>In a correct behaviored parallel memory hierachy, reading a location should return the latest value written by any thread.<br>Side-effects of writes are only observable when reads occur, so we will focus on the values returned by reads. </li>
<li>Within a thread, “latest” can be defined by program order. But when it comes across threads, we don’t want it be physical time, because there is no way that the hardware can pull that off. If it takes &gt;10 cycles to communicate between processors, there is no way that processor 0 can know what processor 1 did 2 clock ticks ago. </li>
<li>Writes from any particular thread must be consistent with program order. And writes across threads must be consistent with a valid interleaving of threads.<br>We define the memory model that each thread proceeds in program order, and memory accesses interleaved (one at a time) to a single-ported memory while rate of progress of each thread is unpredictable.<br>“Latest” means consistent with some interleaving that matches this model</li>
</ol>
<h2 id="Hide-memory-latency"><a href="#Hide-memory-latency" class="headerlink" title="Hide memory latency"></a>Hide memory latency</h2><ol>
<li>Idea: overlap memory accesses with other accesses and computation</li>
<li>“Out of order” pipelining: When an instruction is stuck, perhaps there are subsequent instructions that can be executed. </li>
<li>We don’t need to wait for a conditional branch to be resolved before proceeding. Just predict the branch outcome and continue executing speculatively. If prediction is wrong, squash any side-effects and restart down correct path. </li>
<li>Modern processors fetch and graduate instructions in-order, but issue out-of-order. So intra-thread dependences are preserved, but memory accesses get reordered. </li>
<li>Hiding write latency is simple in uniprocessors, adding a write buffer. But this affects correctness in multiprocessors.<br>In multiprocessor, write buffer or write-back cache might cause that later writes write earlier to memory, namely accesses issued in order may be observed out of order by other processors. </li>
</ol>
<h1 id="Sequential-consistency-SC-model"><a href="#Sequential-consistency-SC-model" class="headerlink" title="Sequential consistency (SC) model"></a>Sequential consistency (SC) model</h1><ol>
<li>Accesses of each processor in program order, all accesses appear in sequential order. Any order implicitly assumed by programmer is maintained. Any order implicitly assumed by programmer is maintained. </li>
<li>How to implement sequential consistency:<br>Implement cache coherence: writes to the same location are observed in same order by all processors<br>For each processor, delay start of memory access until previous one completes, namely each processor has only one outstanding memory access at a time</li>
<li>A read completes when its return value is bound.<br>A write completes when the new value is “visible” to other processors. “Visible” does not mean that other processors have necessarily seen the value yet. It means the new value is committed to the hypothetical serializable order (HSO). </li>
<li>The strict requirements of SC model severely restrict common hardware and compiler optimizations.<br>Processor issues accesses one-at-a-time and stalls for completion, which results the Low processor utilization even with caching. </li>
<li>Total store ordering (TSO) model: Comparing to SC model, a read operation doesn’t need to stall for waiting an earlier write operation. This is similar to the architecture with a FIFO write buffer. </li>
<li>Partial store ordering (PSO) model: Comparing to TSO model, even a write operation doesn’t need to stall for waiting an earlier write operation. This is an architecture with a write buffer that doesn’t have to be FIFO. </li>
</ol>
<h1 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h1><ol>
<li>Most programs don’t require strict ordering (all of the time) for correctness. Here correctness means same results as sequential consistency. </li>
<li>Two accesses conflict if they access same location, and at least one is a write. </li>
<li>We can order accesses by program order (po) and dependence order (do). Operation2 dependents on operation1 if operation2 reads operation1. </li>
<li>Data Race is that two conflicting accesses on different processors, not ordered by intervening accesses. </li>
<li>Properly Synchronized Programs is that all synchronizations are explicitly identified and all data accesses are ordered through synchronization. </li>
<li>Many parallel programs have mixtures of “private” and “public” parts.  The “private” parts must be protected by synchronization,  like locks and unlocks.<br>Between synchronization operations, we can allow reordering of memory operations as long as intra-thread dependences are preserved.<br>Just before and just after synchronization operations, thread must wait for all prior operations to complete</li>
<li>MFENCE does not begin until all prior reads &amp; writes from that thread have completed, and no subsequent read or write from that thread can start until after it finishes. Xchg does this implicitly.<br>MFENCE operation does not push values out to other threads. It simply stalls the thread that performs the MFENCE until write buffer empty.<br>MFENCE operations create partial orderings that are observable across threads. </li>
<li>In weak ordering model, we put MFENCEs before lock operation and after unlock operation. </li>
<li>Lock operation: only gains (“acquires”) permission to access data. Unlock operation: only gives away (“releases”) permission to access data.<br>Release Consistency (RC) model make sure writes before the lock or in the critical section completed before exit critical section, and make sure reads/writes in the critical section or after exit critical section don’t access shared state until lock acquired. </li>
<li>LFENCE serializes only with respect to load operations, and SFENCE serializes only with respect to store operations. In practice MFENCE and xchg are the most likely used ones. </li>
<li>Don’t use only normal memory operations for synchronization, like Peterson’s algorithm. Do use either explicit synchronization operations, like xchg (atomic), or fences. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/07/10-Snooping-Implementation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/07/10-Snooping-Implementation/" class="post-title-link" itemprop="url">10. Snooping Implementation</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-07-07 10:24:14 / Modified: 23:53:49" itemprop="dateCreated datePublished" datetime="2022-07-07T10:24:14+08:00">2022-07-07</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Building-with-an-atomic-bus"><a href="#Building-with-an-atomic-bus" class="headerlink" title="Building with an atomic bus"></a>Building with an atomic bus</h1><h2 id="Transaction"><a href="#Transaction" class="headerlink" title="Transaction"></a>Transaction</h2><ol>
<li>There is a bus controller to do arbitration. If a processor wants to communicate on the bus, it has to make a request. If there are simultaneous requests from multiple processors, the arbiter will only grant one of them. </li>
<li>A transaction on an atomic bus generally need four steps.<br>Client is granted bus access (result of arbitration). The client places command on bus (may also place data on bus).<br>Response to command by another bus client placed on bus. Next client obtains bus access (arbitration)</li>
<li>In a multi-processor with atomic bus scenario, no other bus transactions is allowed between issuing address and receiving data when one processor want to read. Also, when flush is occurred, address and data are sent simultaneously, received by memory before any other transaction is allowed. </li>
<li>Both requests from processor and bus require to look the tag on cache.<br>If bus receives priority, during bus transaction, processor is locked out from its own cache.<br>If processor receives priority, during processor cache accesses, cache cannot respond with its snoop result. So it delays other processors even if no sharing of any form is present. </li>
<li>We can alleviate contention to allow simultaneous access by processor-side and snoop controllers through cache duplicate tags or multi-ported tag memory. In either case cost of the additional performance is additional hardware resources.<br>Tags must stay in sync for correctness, so tag update by one controller will still need to block the other controller, but modifying tags is infrequent compared to checking them. </li>
</ol>
<h2 id="Read-miss"><a href="#Read-miss" class="headerlink" title="Read miss"></a>Read miss</h2><ol>
<li>When a cache read miss occurred, memory needs to know what to do. If the line is dirty, memory should not respond. And the loading cache needs to know what to do. If the line is shared, cache should load into S state, not E. </li>
<li>If one cache controller find that the line is shared in its cache, the controller will send a message through the “shared” wire on bus. If that line is dirty, the controller will send a message through the “dirty” wire on bus.<br>Everytime a processor has responded a snoop, the value in “snoop-pending” wire will be lower and $0$ value indicates that all processors have responded. </li>
<li>Memory controller could immediately start accessing DRAM, but not respond (squelch response). If a snoop result from another cache indicates it has copy of most recent data, then cache should provide data, not memory. Memory could assume one of the caches will service request until snoop results are valid. If snoop indicates no cache has data, then memory must respond</li>
</ol>
<h2 id="Write-back"><a href="#Write-back" class="headerlink" title="Write back"></a>Write back</h2><ol>
<li>Write backs involve two bus transactions, incoming line (line requested by processor) and outgoing line (evicted dirty line in cache that must be flushed).<br>Ideally would like the processor to continue as soon as possible, namely it shouldn’t have to wait for the flush to complete. </li>
<li>The solution is write-back buffer.<br>Stick line to be flushed in a write-back buffer. Immediately load requested line to allow processor to continue. Flush contents of write-back buffer at a later time. </li>
<li>If a request of another processor for the address of the data in the write-back buffer appears on the bus, snoop controller must check the write-back buffer addresses in addition to cache tags.<br>If there is a write-back buffer match, the controller will respond with data from write- back buffer rather than cache and cancel outstanding bus access request. </li>
<li>A write commits when a read-exclusive transaction appears on bus and is acknowledged by all other caches. All future reads will reflect the value of this write, even if data from P has not yet been written to P’s dirty cache line, or to memory.<br>Order of transactions on the bus defines the global order of writes in the parallel program. </li>
<li>“Commit” is not “complete”. A write completes when the updated value is in the cache line</li>
</ol>
<h2 id="Race-conditions"><a href="#Race-conditions" class="headerlink" title="Race conditions"></a>Race conditions</h2><ol>
<li>Coherence protocol state transition diagrams assumed that transitions between states were atomic. However, in practice state transitions are not atomic. </li>
<li>We’ve assumed the bus transaction itself is atomic, but all the operations the system performs as a result of a memory operation are not. </li>
<li>Processor, cache, and bus all are resources operating in parallel. They often contend for shared resources: processor and bus contend for cache while caches contend for bus access. </li>
<li>Cache must be able to handle requests while waiting to acquire bus AND be able to modify its own outstanding requests. </li>
<li>To avoid deadlock, processor must be able to service incoming transactions while waiting to issue requests. </li>
<li>To avoid livelock, a write that obtains exclusive ownership must be allowed to complete before exclusive ownership is relinquished. </li>
<li>Multiple processors competing for bus access must be careful to avoid (or minimize likelihood of) starvation. </li>
<li>Performance optimization often entails splitting operations into several, smaller transactions. Splitting costs in more hardware needed to exploit additional parallelism, and care needed to ensure abstractions still hold. </li>
</ol>
<h1 id="Building-with-non-atomic-bus"><a href="#Building-with-non-atomic-bus" class="headerlink" title="Building with non-atomic bus"></a>Building with non-atomic bus</h1><ol>
<li>Problem with atomic bus: bus is idle while response is pending, which decreases effective bus bandwidth. The interconnect is a limited, shared resource in a multi-processor system. So it is important to use it as efficiently as possible. </li>
<li>Bus transactions are split into two transactions, the request and the response. Other transactions can intervene between a transaction’s request and response. </li>
<li>Basic design:<br>Up to eight outstanding requests at a time (system wide)<br>Responses need not occur in the same order as requests. But request order establishes the total order for the system<br>Flow control via negative acknowledgements (NACKs). When a buffer is full, client can NACK a transaction, causing a retry</li>
<li>We can think of a split-transaction bus as two separate buses, a request bus and a response bus.<br>The request bus has lines for command and address. The response bus has lines for data and response tag. Response tag has 3 bits to represent 8 requests. </li>
</ol>
<h2 id="Read-miss-1"><a href="#Read-miss-1" class="headerlink" title="Read miss"></a>Read miss</h2><h3 id="Phase-1"><a href="#Phase-1" class="headerlink" title="Phase 1"></a>Phase 1</h3><ol>
<li>Request arbitration: cache controllers present request for address to bus (many caches may be doing so in the same cycle)</li>
<li>Request resolution: address bus arbiter grants access to one of the requestors. Request table entry allocated for request. Special arbitration lines indicate tag assigned to request. </li>
<li>Bus “winner” places command/address on the bus. </li>
<li>Caches perform snoop: look up tags, update cache state, etc. Memory operation commits here. (no bus traffic)</li>
<li>Caches acknowledge this snoop result is ready, or signal they could not complete snoop in time here. </li>
</ol>
<h3 id="Phase-2"><a href="#Phase-2" class="headerlink" title="Phase 2"></a>Phase 2</h3><ol>
<li>Data response arbitration: responder presents intent to respond to request with tag T. (many caches or memory may be doing so in the same cycle)</li>
<li>Data bus arbiter grants one responder bus access. </li>
<li>Original requestor signals readiness to receive response (or lack thereof: requestor may be busy at this time)</li>
<li>Then in phase 3, Responder places response data on data bus. Caches present snoop result for request with the data. Request table entry is freed. Those 3 actions can happen parallel. </li>
</ol>
<h2 id="Pipelined-transactions"><a href="#Pipelined-transactions" class="headerlink" title="Pipelined transactions"></a>Pipelined transactions</h2><ol>
<li>Request bus and response bus can run parallel. So the response of the last transaction and the request of the next transaction can happen simultaneously. Pipelining may cause out-of-order completion. </li>
<li>Write backs and BusUpg transactions do not have a response component. Write backs acquire access to both request address bus and data bus as part of “request” phase BusUpg does not need any acknowledgement or data. </li>
<li>Avoid conflicting requests by disallowing them. Each cache has a copy of the request table. Caches do not make requests that conflict with requests in the request table. </li>
<li>Caches/memory have buffers for receiving data off the bus. If the buffer fills, client NACKs relevant requests or responses. </li>
<li>In parallel system, we use queues to accommodate variable (unpredictable) rates of production and consumption. As long as workers, on average, produce and consume at the same rate, all workers can run at full rate. Otherwise, some will stall waiting for others to accept or produce new input. </li>
<li>We have queues to track requests and responses between L1 and L2 caches and between L2 cache and bus. One queue is for requests and responses from closer (to processor) to farther (L1 to L2 or L2 to bus), and the other is from farther to closer (L2 to L1 or bus to L2). </li>
<li>This may rise deadlock due to full queue. Outgoing read request (initiated by processor) and incoming read request (due to another cache) both requests generate responses that require space in the other queue (circular dependency)</li>
<li>Sizing all buffers to accommodate the maximum number of outstanding requests on bus is one solution to avoiding deadlock. But a costly one. </li>
<li>Avoiding buffer deadlock with separate request/response queues. Namely, we distinguish whether it is a request or a response.<br>Responses can be completed without generating further transactions. Requests increase queue length But responses reduce queue length. While stalled attempting to send a request, cache must be able to service responses. Responses will make progress, eventually freeing up resources for requests. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/06/09-Directory-Based-Cache-Cohurence/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/06/09-Directory-Based-Cache-Cohurence/" class="post-title-link" itemprop="url">09. Directory-Based Cache Cohurence</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-07-06 17:10:17 / Modified: 20:09:21" itemprop="dateCreated datePublished" datetime="2022-07-06T17:10:17+08:00">2022-07-06</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97-CMU-15-418-Stanford-CS149/" itemprop="url" rel="index">
                    <span itemprop="name">并行计算 (CMU 15-418 / Stanford CS149)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Problems-to-solve"><a href="#Problems-to-solve" class="headerlink" title="Problems to solve"></a>Problems to solve</h1><ol>
<li>The snooping cache coherence protocols relied on broadcasting coherence information to all processors over the chip interconnect. Every time a cache miss occurred, the triggering cache communicated with all other caches, so the interconnect has a heavy traffic. </li>
<li>The efficiency of NUMA system does little good if the coherence protocol can’t also be scaled. Processor accesses nearby memory, but to ensure coherence still must broadcast to all other processors. </li>
<li>One possible solution is hierarchical snooping which arranges nodes in a tree and uses snooping coherence at each level. The interconnects involved in a communication is as low as possible and kept as local as possible. </li>
<li>The structure of hierarchical snooping is relatively simple to build.<br>It uses a tree to reduce the conjunction at the center part, but if the workload is not nicely partitioned, then the root of the network can become a bottleneck.<br>It also has larger latencies than direct communication and does not apply to more general network topologies (meshes, cubes)</li>
</ol>
<h1 id="Directory"><a href="#Directory" class="headerlink" title="Directory"></a>Directory</h1><ol>
<li>Snooping schemes broadcast coherence messages to determine the state of a line in the other caches. An alternative idea is to avoid broadcast by storing information about the status of the line in one place, namely a “directory”. </li>
<li>A line is a region of memory that would be cached as a single block. One directory entry corresponds to one line of memory.<br>In a directory entry, there are a dirty bit that indicates line is dirty in one of the processors’ caches and P presence bits that indicate whether processor P has line in its cache. </li>
<li>The directory is used in NUMA system, each processor has a “local” memory and a directory.<br>Home node of a line is the node with memory holding the corresponding data for the line.<br>Requesting node is the node containing processor requesting line</li>
</ol>
<h2 id="Read-and-write"><a href="#Read-and-write" class="headerlink" title="Read and write"></a>Read and write</h2><ol>
<li>When a read miss happens:<br>The requesting node will send a read miss message to the home node of the requested line. Then home directory checks entry for line.<br>If dirty bit for cache line is OFF, the home node will respond with contents from memory, set presence bit of the requesting node to true to indicate line is cached by the requesting processor.<br>If dirty bit for cache line is ON, the home node will respond with message providing identity of line owner. Requesting node requests data from owner and owner changes state in cache to SHARED (read only), responds to requesting node. Owner also responds to home node, home clears dirty, updates presence bits (line is cached by both the requesting node and owner), updates memory. </li>
<li>When a write miss happens:<br>The requesting node will send a write miss message to the home node of the requested line.<br>The home node will response the sharer ids and data to the requesting node.<br>The requesting node will send invalidation signal to all sharers. After receiving invalidation acks from all sharers, the requesting node can perform write.<br>The home node will update presence bits (line is cached by only the requesting node) and dirty bit. </li>
<li>On reads, directory tells requesting node exactly where to get the line from, either from home node (if the line is clean) or from the owning node (if the line is dirty). Either way, retrieving data involves only point-to-point communication. </li>
<li>On writes, the advantage of directories depends on the number of sharers. In the limit, if all caches are sharing data, all caches must be communicated with, just like broadcast in a snooping protocol. </li>
<li>In general only a few processors share the line, namely only a few processors must be told of writes.  And the expected number of sharers typically increases slowly with P. </li>
</ol>
<h2 id="Reduce-storage-overhead"><a href="#Reduce-storage-overhead" class="headerlink" title="Reduce storage overhead"></a>Reduce storage overhead</h2><ol>
<li>Full bit vector directory storage is proportional to $P\times M$ where $P$ is the number of nodes and $M$ is the number of lines in memory. The storage overhead of directory is too much, and we do not want it to be DRAM since we need it to run fast.</li>
<li>One way to reduce storage overhead is to optimize on full-bit vector.<br> Increase cache line size to reduce $M$ term.<br> Group multiple processors into a single directory node to reduce $P$ term. We could use snooping protocol to maintain coherence among processors in a node, directory across nodes. </li>
<li>Another way is to limit sharer pointer. Since data is expected to only be in a few caches at once, storage for a limited number of pointers per directory entry should be sufficient. Only need a list of the nodes holding a valid copy of the line. </li>
<li>When an overflow in limited pointer schemes occurs, we can revert to broadcast if broadcast mechanism exists. If no broadcast mechanism present on machine, newest sharer replaces an existing one (must invalidate line in the old sharer’s cache)</li>
<li>One more way is through sparse directory.<br> The majority of memory is not resident in cache. And to carry out coherence protocol the system only needs sharing information for lines that are currently in some cache. So most directory entries are empty most of the time.<br> We can add a tag for each directory line to indicate the memory held in some cache. The overhead is now $P\times C$ where $C$ is the number of lines in each cache. </li>
</ol>
<h2 id="Reduce-number-of-message-sent"><a href="#Reduce-number-of-message-sent" class="headerlink" title="Reduce number of message sent"></a>Reduce number of message sent</h2><ol>
<li>In a read miss to dirty line, there are five network transactions in total. But only four of the transactions are on the critical path. </li>
<li>In intervention forward, home node requests data from owner node (intervention read). After the owner has responded, home node updates directory, and responds to requesting node with data.<br>Four network transactions in total (less traffic). But all four of the transactions are on the critical path. </li>
<li>In request forwarding, home node requests the owner to sent data to the requesting node. Then the owner will send data to requesting node and home node at the same time.<br>Four network transactions in total, with only three of the transactions are on the critical path. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/03/12-Feedback/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/03/12-Feedback/" class="post-title-link" itemprop="url">12. Feedback</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-07-03 15:37:32" itemprop="dateCreated datePublished" datetime="2022-07-03T15:37:32+08:00">2022-07-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-04 11:47:02" itemprop="dateModified" datetime="2022-07-04T11:47:02+08:00">2022-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F-MIT-6-007/" itemprop="url" rel="index">
                    <span itemprop="name">信号与系统 (MIT 6.007)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li>A continuous-loop system with a discrete-time feedback control loop is as followed<br><img src="/img/12-Feedback/1.png" width="40%"></li>
<li>For a discrete-time feedback loop, we can move the continuous to discrete-time converter right after $H(s)$ and the interpolating system after the summer.<br><img src="/img/12-Feedback/2.png" width="40%"></li>
<li>In a basic feedback system, we may ignore the converter and interpolating system.<br><img src="/img/12-Feedback/3.png" width="40%"><br>The system is usually a negative feedback, namely signal fed back is subtracted from the input.<br>The output of the adder is error signal, indicating that it’s the difference between the signal fed back and the input to the overall system. </li>
<li>We can label the Laplace transform of the output $y(t)$ to be $Y(s)$ which is also the input of $G(s)$. Hence, the Laplace transform of $r(t)$ is $Y(s)G(s)$. The Laplace transform of $e(t)=x(t)-r(t)$ is $X(s)-Y(s)G(s)$. The output of $H(s)$ is $[X(s)-Y(s)G(s)]H(s)$ which is also $Y(s)$.<br>Namely, we have $[X(s)-Y(s)G(s)]H(s)=Y(s)\Leftrightarrow Q(s)=\frac{Y(s)}{X(s)}=\frac{H(s)}{1+G(s)H(s)}$, which is the system function in the open loop forward path divided by $1$ plus the loop gain.<br>In the discrete-time case, the equation is similar $Q(z)=\frac{H(z)}{1+G(z)H(z)}$. </li>
<li>In the amplifier design, we use $G(s)=k$, namely $Q(s)=\frac{H(s)}{1+kH(s)}$. If $kH(s)&gt;&gt;1$, then $Q(s)\simeq\frac1k$. </li>
<li>In the inverse design, we use $H(s)=k$ and $G(s)$ being the system $P(s)$ which we want to inverse. $Q(s)=\frac{k}{1+kP(s)}$. If $kP(s)&gt;&gt;1$, then $Q(s)\simeq\frac{1}{P(s)}$. </li>
<li>To stabilize an unstable continuous-time system $H(s)$, we need to choose $G(s)$ so that $Re\{s_i\}&lt;0$ where $s_i$ are the roots of $1+G(s)H(s)=0$.<br>In the discrete-time case, we need to choose $G(z)$ so that $|z_i|&lt;1$ where $z_i$ are the roots of $1+G(z)H(z)=0$.<br>A feedback system can also unstabilize a stable system. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/07/02/11-Mapping-of-Continuous-Time-Filter-To-Discrete-Time-Filter/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/07/02/11-Mapping-of-Continuous-Time-Filter-To-Discrete-Time-Filter/" class="post-title-link" itemprop="url">11. Mapping of Continuous-Time Filter To Discrete-Time Filter</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-07-02 16:52:21 / Modified: 18:03:00" itemprop="dateCreated datePublished" datetime="2022-07-02T16:52:21+08:00">2022-07-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F-MIT-6-007/" itemprop="url" rel="index">
                    <span itemprop="name">信号与系统 (MIT 6.007)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li>We want to map a continuou-time function $H_c(s)$, $\sum^N_{k=0}a_k\frac{d^ky(t)}{dt^k}=\sum^M_{k=0}b_k\frac{d^kx(t)}{dt^k}$, to a discrete-time function $H_d(z)$, $\sum^N_{k=0}c_ky[n-k]=\sum^N_{k=0}d_kx[n-k]$. </li>
<li>When mapping, we want the mapping function can work correctly in frequency domain $H_c(s)\to H_d(z)$, in time domain $h_c(t)\to h_d[n]$,  and map the $j\omega$-axis in s-plane to the unit circle in the z-plane. We also want the mapped system to be stable if the continuous-time system is stable. </li>
<li>One way of mapping is through difference.<br>In the backward difference, we map $\frac{dy(t)}{dt}$ to $\frac{y[n]-y[n-1]}{T}$. In the frequency domain, the mapping function maps $sY(s)$ to $\frac{1-z^{-1}}{T}Y(z)$. Compare the two functions to find that $H_d(z)=H_c(s)|_{s=\frac{1-z^{-1}}{T}}$.<br>In the forward difference, we map $\frac{dy(t)}{dt}$ to $\frac{y[n+1]-y[n]}{T}$. In the frequency domain, the mapping function maps $sY(s)$ to $\frac{z-1}{T}Y(z)$. Compare the two functions, $H_d(z)=H_c(s)|_{z=\frac{z-1}{T}}$. </li>
<li>In the backward difference, we are not mapping the $j\omega$-axis to the unit circle but a little circle inside the unit circle. If the continuous-time filter has a very sharp resonance, the discrete-time filter might have that resonance broadened considerably because the poles are so far away from the unit circle.<br>In backward difference, stability is guarenteed. Forward difference even cannot guarentee stability. </li>
<li>Another way of mapping is impulse invariance.<br>We map the discrete-time as samples of the continuous-time signal, namely $h_d[n]=h_c(nT)$. In the frequency domain, $H_d(e^{j\Omega})=\frac1T\sum^{+\infty}_{k=-\infty}H_c[j(\frac\Omega T-\frac{2\pi k}{T})]$. </li>
<li>The impulse invariance keeps the same basic shape but linearly scale the frequency axis. And superposition of these added together at multiples of $2\pi$ in discrete time frequency.<br>But if the filter is not an ideal filter, it may cause aliasing. </li>
<li>A last way to map is bilinear transform. We map the frequency function by $s=\frac2T\frac{1-z^{-1}}{1+z^{-1}}$, namely $z=\frac{1+\frac{T}{2}s}{1-\frac{T}{2}s}$. </li>
<li>Impulse invariance corresponds to a linear mapping between the two frequency axes, except for the issue of aliasing, which limits its usefulness to filter designs or for mapping continuous time filters that are band limited.<br>Bilinear transformation avoids aliasing, but has the disadvantage that it represents a nonlinear mapping from the continuous-time filter to the discrete-time filter. This nonlinear distortion is perfectly acceptable if we are designing filters that have flat frequency characteristics. It is not acceptable if we had a linear frequency characteristic that we wanted to map to a discrete-time filter and end up with a linear frequency characteristic. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/06/29/10-The-Z-transform/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/29/10-The-Z-transform/" class="post-title-link" itemprop="url">10. The Z-transform</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-29 12:18:17" itemprop="dateCreated datePublished" datetime="2022-06-29T12:18:17+08:00">2022-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-02 17:50:40" itemprop="dateModified" datetime="2022-07-02T17:50:40+08:00">2022-07-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F-MIT-6-007/" itemprop="url" rel="index">
                    <span itemprop="name">信号与系统 (MIT 6.007)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Z-transform"><a href="#Z-transform" class="headerlink" title="Z-transform"></a>Z-transform</h1><ol>
<li>In discrete-time Fourier transform, we choose $e^{j\Omega n}$ to be the basic signal. In z-transform, we want to use a more general signal $z^n$ as the basic signal, where $z=re^{j\Omega}$.<br>The response of $z^n$ is $\sum^{+\infty}_{k=-\infty}h[k]z^{n-k}=z^n\sum^{+\infty}_{k=-\infty}h[k]z^{-k}=H(z)z^n$, where $H(z)=\sum^{+\infty}_{n=-\infty}h[n]z^{-n}$. </li>
<li>We can define that the z-transform of $x[n]$ is $X(z)=\sum^{+\infty}_{n=-\infty}x[n]e^{-n}$ and call $x[n]$ and $X(z)$ as a z-transform pair. </li>
<li>Compare the definition of z-transform and Fourier transform, we can know that if $z=e^{j\Omega}$, then z-transform is identical to Fourier transform. Namely, $X(z)|_{z=e^{j\Omega}}=F\{x[n]\}$.<br>We use the notation of $F\{x[n]\}=X(e^{j\Omega})$ to denote the Fourier transform of $x[n]$. </li>
<li>$X(re^{j\Omega})=\sum^{+\infty}_{n=-\infty}x<a href="re^{j\Omega}">n</a>^n=\sum^{+\infty}_{n=-\infty}x[n]r^{-n}e^{-j\Omega n}$. Compare this equation with the definition of Fourier transform, we know that $X(z)=F\{x[n]r^{-n}\}$. </li>
<li>The ROC does not contain poles. </li>
<li>Unlike Laplace transform, the ROC of $X(z)$ consists of a ring in the z-plane centered about the origin. </li>
<li>$F\{x[n]\}$ converges $\Leftrightarrow$ ROC includes the unit circle in the z-plane. </li>
<li>If $x[n]$ is finite duration, then ROC is entire z-plane with the possible exception of $z=0$ or $z=\infty$.<br>If $x[n]$ is right-sided and $|z|=r_0$ is in ROC, then all finite values of $z$ for which $|z|&gt;r_0$ are in ROC.<br>If $x[n]$ is right-sided and $X(z)$ is rational, then ROC is outside the outermost pole.<br>If $x[n]$ is left-sided and $|z|=r_0$ is in ROC, then all finite values of $z$ for which $0&lt;|z|&lt;r_0$ are in ROC.<br>If $x[n]$ is left-sided and $X(z)$ is rational, then ROC is inside the innermost pole.<br>If $x[n]$ is two-sided and $|z|=r_0$ is in ROC, then ROC is a ring in the z-plane which includes the circle $|z|=r_0$. </li>
<li>One way to calculate the inverse z-transform is by Fourier transform.<br>Since $X(z)=F\{x[n]r^{-n}\}$, the inverse of $x[n]r^{-n}=\frac1{2\pi}\int_{2\pi}X(z)e^{j\Omega n}d\Omega$. Thus $x[n]=\frac1{2\pi}\int_{2\pi}X(z)(re^{j\Omega})^nd\Omega=\frac1{2\pi}\int_{2\pi}X(z)z^nd\Omega$. $dz=jre^{j\Omega}d\Omega=jzd\Omega$, which make the equation $x[n]=\frac1{2\pi j}\oint X(z)z^{n-1}dz$. </li>
<li>Another way is to decompose $X(z)$ into partial fractions which we know the algebraic expression and ROC.<br>  One last way is to decompose $X(z)$ into power series sum of $z^{-n}$. Then the $x[n]$ is the coefficients of $z^{-n}$. </li>
</ol>
<h1 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h1><ol>
<li>Linearity: $ax_1[n]+bx_2[n]$ and $aX_1[n]+bX_2[n]$ are a z-transform pair, and its ROC is at least $R_1\cap R_2$. </li>
<li>Time shifting: $x[n-n_0]$ and $z^{-n_0}X(z)$ are a z-transform pair, and its ROC remains the same as $x[n]$. </li>
<li>Convolution property: $x_1[n]\ast x_2[n]$ and $X_1(z)X_2(z)$ are a z-transform pair, and its ROC is at least $R_1\cap R_2$. </li>
<li>The system is stable $\Leftrightarrow$ $\sum^{+\infty}_{n=-\infty}|h[n]|&lt;\infty$. And the Fourier transform $F\{h[n]\}$ converges $\Leftrightarrow$ $\sum^{+\infty}_{n=-\infty}|h[n]|&lt;\infty$. A system is stable $\Leftrightarrow$ its Fourier transform $F\{h[n]\}$ converges.<br>Hence, a system is stable $\Leftrightarrow$ ROC of $H(z)$ includes unit circle in z-plane. </li>
<li>If a system is causal, then $h[n]$ is right-sided and thus ROC of $H(z)$ is outside of the outmost pole.<br>A system is causal and linear $\Leftrightarrow$ all poles are inside the unit circle. </li>
<li>Similar to Laplace transform, the same $X(z)$ expression may have different ROC and thus different $x[n]$. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="http://http//www.laughingtree.cn/2022/06/28/09-The-Laplace-Transform/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="LiyunZhang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="LaughingTree">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2022/06/28/09-The-Laplace-Transform/" class="post-title-link" itemprop="url">09. The Laplace Transform</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-28 16:48:48" itemprop="dateCreated datePublished" datetime="2022-06-28T16:48:48+08:00">2022-06-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-06-29 12:18:36" itemprop="dateModified" datetime="2022-06-29T12:18:36+08:00">2022-06-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index">
                    <span itemprop="name">计算机科学与技术</span>
                  </a>
                </span>
                  , 
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%A7%91%E5%AD%A6%E4%B8%8E%E6%8A%80%E6%9C%AF/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E7%B3%BB%E7%BB%9F-MIT-6-007/" itemprop="url" rel="index">
                    <span itemprop="name">信号与系统 (MIT 6.007)</span>
                  </a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Laplace-transform"><a href="#Laplace-transform" class="headerlink" title="Laplace transform"></a>Laplace transform</h1><ol>
<li>In Fourier transform, we choose $e^{j\omega t}$ to be the basic signal. In Laplace transform, we would like to use a more general signal $e^{st}$ with $s=\sigma+j\omega$.<br>The response of $e^{st}$ is $e^{st}\ast h(t)=\int^{+\infty}_{-\infty}h(\tau)e^{s(t-\tau)}d\tau=e^{st}\int^{+\infty}_{-\infty}h(\tau)e^{-s\tau}d\tau=H(s)e^{st}$. </li>
<li>Similar to Fourier transform, we can define the Laplace transform as $X(s)=\int^{+\infty}_{-\infty}x(t)e^{-st}dt$ and call $X(s)$ and $x(t)$ as a Laplace transform pair. </li>
<li>If we set $\sigma=0$, namely $s=j\omega$, then Laplace transform is actually Fourier transform. Here we denote the Fourier transform as $F\{x(t)\}$ since it can be determined by $x(t)$. So $X(s)|_{s=j\omega}=X(j\omega)=F\{x(t)\}$</li>
<li>The Laplace transform is $X(\sigma+j\omega)=\int^{+\infty}_{-\infty}x(t)e^{-(\sigma+j\omega)t}dt=\int^{+\infty}_{-\infty}x(t)e^{-\sigma t}e^{-j\omega t}dt$. Compare this equation with the definition of Fourier transform, we can easily find that $X(s)=F\{x(t)e^{-\sigma t}\}$.<br>For some $x(t)$ that is not absolutely integrable, we can choose a reasonable $\sigma$ and multiply it by the exponential factor $e^{-\sigma t}$ to make it absolutely integrable.<br>So Laplace transform may converge when Fourier transform does not. </li>
<li>If we can decompose $X(s)$ into $X(s)=\frac{N(s)}{D(s)}$, then $X(s)$ is rational.<br>In a complex s-plane, poles of $X(s)$ is $D(s)=0$ while zeros of $X(s)$ is $N(s)=0$. </li>
<li>If a certain $X(s)$ is convergent for a certain $s=\sigma_0+j\omega_0$, then $X(s)$ is convergent for any other $\omega$.<br>In the s-plane, region of convergence (ROC) of $X(s)$ consists a strip parallel to the $j\omega$-axis.<br>$F\{x(t)\}$ converges $\Leftrightarrow$ ROC includes the $j\omega$-axis in the s-plane. </li>
<li>The ROC is a connected region. </li>
<li>If $x(t)$ is a finite duration (zero except some time interval), then ROC is the entire s-plane<br>If $x(t)$ is right-sided and $Re\{s\}=\sigma_0$ is in ROC, then all values for which $Re\{s\}&gt;\sigma_0$ are in the ROC.<br>If $x(t)$ is right-sided and $X(s)$ is rational, then ROC is to the right of the rightmost pole.<br>If $x(t)$ is left-sided and $Re\{s\}=\sigma_0$ is in ROC, then all values for which $Re\{s\}&lt;\sigma_0$ are in the ROC.<br>If $x(t)$ is left-sided and $X(s)$ is rational, then ROC is to the left of the leftmost pole.<br>If $x(t)$ is both-sided and $Re\{s\}=\sigma_0$ is in ROC, then ROC is a strip in the s-plane. </li>
</ol>
<h1 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h1><ol>
<li>Linearity: $ax_1(t)+bx_2(t)$ and $aX_1(t)+bX_2(t)$ are a Laplace transform. </li>
<li>Differential: $\frac{dx(t)}{dt}$ and $sH(s)$ are a Laplace transform. </li>
<li>Convolution property: $x_1(t)\ast x_2(t)$ and $X_1(t)X_2(t)$ are a Laplace transform. </li>
<li>If a system is stable, its ROC must contain the $j\omega$-axis.<br>If a system is causal, then its impulse response is right-sided, thus its ROC must be right to the right most pole.<br>If a system is linear and causal, its all poles must in left-half s-plane. </li>
<li>In some system, we can determine its $H(s)$, but that $H(s)$ may correspond to several algebraic expression. Then we should combine the stability and causality to determine the ROC and hence determine the expression. </li>
<li>If we cascade $N$ systems together, the Laplace transform of the overall system is $H(s)=H_1(s)H_2(s)\dots H_N(s)$.<br>If we parallel $N$ systems and add their outputs together, the Laplace transform of the overall system is $H(s)=H_1(s)+H_2(s)+\dots+H_N(s)$. </li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/23/">23</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="LiyunZhang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">LiyunZhang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">227</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">29</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liyun Zhang</span>
</div>
<div class="BbeiAn-info">
       浙ICP备 -
    <a target="_blank" href="http://www.miitbeian.gov.cn/" style="color:#ffffff"  rel="nofollow">19047088号-1</a> <!--a标签中增加nofollow属性，避免爬虫出站。-->|
    <a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001835" style="color:#ffffff;text-decoration:none;padding-left:30px;background:url(https://s1.ax1x.com/2018/09/29/ilmwIH.png) no-repeat left center" rel="nofollow">浙公网安备 33011802001835号</a>      <!--这里将图标作为了背景，以使得能和后面的文字在同一行-->

</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    

  

</body>
</html>
